[
  {
    "objectID": "In-class_Ex/In-class_Ex02/In-class_Ex02.html",
    "href": "In-class_Ex/In-class_Ex02/In-class_Ex02.html",
    "title": "In-class Exercise 2:Fundamental of Geospatial Data Visualisation and tmap Methods",
    "section": "",
    "text": "sf for handling geospatial data.\ntidyverse for manipulating and wrangling data.\n\n\npacman::p_load(sf, tidyverse)\n\n\n\n\nImporting shape-file of Master Plan 2024 Sub-zone Boundary.\n\nmpsz_shp &lt;- st_read(dsn = \"data/geospatial/MasterPlan2014SubzoneBoundaryWebSHP\", layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `/Users/mingwei/Desktop/SMU/Y3S1/IS415/xXxPMWxXx/IS415-GAA/In-class_Ex/In-class_Ex02/data/geospatial/MasterPlan2014SubzoneBoundaryWebSHP' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\nTo check the object class\n\nclass(mpsz_shp)\n\n[1] \"sf\"         \"data.frame\"\n\n\n\nmpsz_shp\n\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\nFirst 10 features:\n   OBJECTID SUBZONE_NO       SUBZONE_N SUBZONE_C CA_IND      PLN_AREA_N\n1         1          1    MARINA SOUTH    MSSZ01      Y    MARINA SOUTH\n2         2          1    PEARL'S HILL    OTSZ01      Y          OUTRAM\n3         3          3       BOAT QUAY    SRSZ03      Y SINGAPORE RIVER\n4         4          8  HENDERSON HILL    BMSZ08      N     BUKIT MERAH\n5         5          3         REDHILL    BMSZ03      N     BUKIT MERAH\n6         6          7  ALEXANDRA HILL    BMSZ07      N     BUKIT MERAH\n7         7          9   BUKIT HO SWEE    BMSZ09      N     BUKIT MERAH\n8         8          2     CLARKE QUAY    SRSZ02      Y SINGAPORE RIVER\n9         9         13 PASIR PANJANG 1    QTSZ13      N      QUEENSTOWN\n10       10          7       QUEENSWAY    QTSZ07      N      QUEENSTOWN\n   PLN_AREA_C       REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR\n1          MS CENTRAL REGION       CR 5ED7EB253F99252E 2014-12-05 31595.84\n2          OT CENTRAL REGION       CR 8C7149B9EB32EEFC 2014-12-05 28679.06\n3          SR CENTRAL REGION       CR C35FEFF02B13E0E5 2014-12-05 29654.96\n4          BM CENTRAL REGION       CR 3775D82C5DDBEFBD 2014-12-05 26782.83\n5          BM CENTRAL REGION       CR 85D9ABEF0A40678F 2014-12-05 26201.96\n6          BM CENTRAL REGION       CR 9D286521EF5E3B59 2014-12-05 25358.82\n7          BM CENTRAL REGION       CR 7839A8577144EFE2 2014-12-05 27680.06\n8          SR CENTRAL REGION       CR 48661DC0FBA09F7A 2014-12-05 29253.21\n9          QT CENTRAL REGION       CR 1F721290C421BFAB 2014-12-05 22077.34\n10         QT CENTRAL REGION       CR 3580D2AFFBEE914C 2014-12-05 24168.31\n     Y_ADDR SHAPE_Leng SHAPE_Area                       geometry\n1  29220.19   5267.381  1630379.3 MULTIPOLYGON (((31495.56 30...\n2  29782.05   3506.107   559816.2 MULTIPOLYGON (((29092.28 30...\n3  29974.66   1740.926   160807.5 MULTIPOLYGON (((29932.33 29...\n4  29933.77   3313.625   595428.9 MULTIPOLYGON (((27131.28 30...\n5  30005.70   2825.594   387429.4 MULTIPOLYGON (((26451.03 30...\n6  29991.38   4428.913  1030378.8 MULTIPOLYGON (((25899.7 297...\n7  30230.86   3275.312   551732.0 MULTIPOLYGON (((27746.95 30...\n8  30222.86   2208.619   290184.7 MULTIPOLYGON (((29351.26 29...\n9  29893.78   6571.323  1084792.3 MULTIPOLYGON (((20996.49 30...\n10 30104.18   3454.239   631644.3 MULTIPOLYGON (((24472.11 29...\n\n\nImporting KML file.\n\n# mpsz_kml = st_read(\"data/geospatial/MasterPlan2014SubzoneBoundaryWebKML.kml\")\n\n\n\n\n\n\n\nWarning\n\n\n\nThe above code cannot run, as the KML file was corrupted.\n\n\n\n\n\nThe code chunk below export the data frame into KML file.\n\nst_write(mpsz_shp,\n         \"data/geospatial/mpsz.kml\",\n         delete_dsn = TRUE)\n\nDeleting source `data/geospatial/mpsz.kml' using driver `KML'\nWriting layer `mpsz' to data source `data/geospatial/mpsz.kml' using driver `KML'\nWriting 323 features with 15 fields and geometry type Multi Polygon.\n\n\n\n\n\n\n\n\nNote\n\n\n\ndelete_dsn is is used to specify whether the existing data source (the file you’re writing to) should be deleted before writing new data into it. Useful when we want to overwrite the existing file."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#import-packages",
    "href": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#import-packages",
    "title": "In-class Exercise 2:Fundamental of Geospatial Data Visualisation and tmap Methods",
    "section": "",
    "text": "sf for handling geospatial data.\ntidyverse for manipulating and wrangling data.\n\n\npacman::p_load(sf, tidyverse)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#import-data",
    "href": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#import-data",
    "title": "In-class Exercise 2:Fundamental of Geospatial Data Visualisation and tmap Methods",
    "section": "",
    "text": "Importing shape-file of Master Plan 2024 Sub-zone Boundary.\n\nmpsz_shp &lt;- st_read(dsn = \"data/geospatial/MasterPlan2014SubzoneBoundaryWebSHP\", layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `/Users/mingwei/Desktop/SMU/Y3S1/IS415/xXxPMWxXx/IS415-GAA/In-class_Ex/In-class_Ex02/data/geospatial/MasterPlan2014SubzoneBoundaryWebSHP' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\nTo check the object class\n\nclass(mpsz_shp)\n\n[1] \"sf\"         \"data.frame\"\n\n\n\nmpsz_shp\n\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\nFirst 10 features:\n   OBJECTID SUBZONE_NO       SUBZONE_N SUBZONE_C CA_IND      PLN_AREA_N\n1         1          1    MARINA SOUTH    MSSZ01      Y    MARINA SOUTH\n2         2          1    PEARL'S HILL    OTSZ01      Y          OUTRAM\n3         3          3       BOAT QUAY    SRSZ03      Y SINGAPORE RIVER\n4         4          8  HENDERSON HILL    BMSZ08      N     BUKIT MERAH\n5         5          3         REDHILL    BMSZ03      N     BUKIT MERAH\n6         6          7  ALEXANDRA HILL    BMSZ07      N     BUKIT MERAH\n7         7          9   BUKIT HO SWEE    BMSZ09      N     BUKIT MERAH\n8         8          2     CLARKE QUAY    SRSZ02      Y SINGAPORE RIVER\n9         9         13 PASIR PANJANG 1    QTSZ13      N      QUEENSTOWN\n10       10          7       QUEENSWAY    QTSZ07      N      QUEENSTOWN\n   PLN_AREA_C       REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR\n1          MS CENTRAL REGION       CR 5ED7EB253F99252E 2014-12-05 31595.84\n2          OT CENTRAL REGION       CR 8C7149B9EB32EEFC 2014-12-05 28679.06\n3          SR CENTRAL REGION       CR C35FEFF02B13E0E5 2014-12-05 29654.96\n4          BM CENTRAL REGION       CR 3775D82C5DDBEFBD 2014-12-05 26782.83\n5          BM CENTRAL REGION       CR 85D9ABEF0A40678F 2014-12-05 26201.96\n6          BM CENTRAL REGION       CR 9D286521EF5E3B59 2014-12-05 25358.82\n7          BM CENTRAL REGION       CR 7839A8577144EFE2 2014-12-05 27680.06\n8          SR CENTRAL REGION       CR 48661DC0FBA09F7A 2014-12-05 29253.21\n9          QT CENTRAL REGION       CR 1F721290C421BFAB 2014-12-05 22077.34\n10         QT CENTRAL REGION       CR 3580D2AFFBEE914C 2014-12-05 24168.31\n     Y_ADDR SHAPE_Leng SHAPE_Area                       geometry\n1  29220.19   5267.381  1630379.3 MULTIPOLYGON (((31495.56 30...\n2  29782.05   3506.107   559816.2 MULTIPOLYGON (((29092.28 30...\n3  29974.66   1740.926   160807.5 MULTIPOLYGON (((29932.33 29...\n4  29933.77   3313.625   595428.9 MULTIPOLYGON (((27131.28 30...\n5  30005.70   2825.594   387429.4 MULTIPOLYGON (((26451.03 30...\n6  29991.38   4428.913  1030378.8 MULTIPOLYGON (((25899.7 297...\n7  30230.86   3275.312   551732.0 MULTIPOLYGON (((27746.95 30...\n8  30222.86   2208.619   290184.7 MULTIPOLYGON (((29351.26 29...\n9  29893.78   6571.323  1084792.3 MULTIPOLYGON (((20996.49 30...\n10 30104.18   3454.239   631644.3 MULTIPOLYGON (((24472.11 29...\n\n\nImporting KML file.\n\n# mpsz_kml = st_read(\"data/geospatial/MasterPlan2014SubzoneBoundaryWebKML.kml\")\n\n\n\n\n\n\n\nWarning\n\n\n\nThe above code cannot run, as the KML file was corrupted."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#export-data-frame-into-kml-file",
    "href": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#export-data-frame-into-kml-file",
    "title": "In-class Exercise 2:Fundamental of Geospatial Data Visualisation and tmap Methods",
    "section": "",
    "text": "The code chunk below export the data frame into KML file.\n\nst_write(mpsz_shp,\n         \"data/geospatial/mpsz.kml\",\n         delete_dsn = TRUE)\n\nDeleting source `data/geospatial/mpsz.kml' using driver `KML'\nWriting layer `mpsz' to data source `data/geospatial/mpsz.kml' using driver `KML'\nWriting 323 features with 15 fields and geometry type Multi Polygon.\n\n\n\n\n\n\n\n\nNote\n\n\n\ndelete_dsn is is used to specify whether the existing data source (the file you’re writing to) should be deleted before writing new data into it. Useful when we want to overwrite the existing file."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#transforming-coordinate-system",
    "href": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#transforming-coordinate-system",
    "title": "In-class Exercise 2:Fundamental of Geospatial Data Visualisation and tmap Methods",
    "section": "2.1 Transforming coordinate system",
    "text": "2.1 Transforming coordinate system\n\nmpsz2019_shp &lt;- st_read(dsn = \"data/geospatial/MPSZNoSeaSHP\", \n                        layer = \"MPSZ-2019\") %&gt;%\n                st_transform(crs = 3414)\n\nReading layer `MPSZ-2019' from data source \n  `/Users/mingwei/Desktop/SMU/Y3S1/IS415/xXxPMWxXx/IS415-GAA/In-class_Ex/In-class_Ex02/data/geospatial/MPSZNoSeaSHP' \n  using driver `ESRI Shapefile'\nSimple feature collection with 332 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 103.6057 ymin: 1.158699 xmax: 104.0885 ymax: 1.470775\nGeodetic CRS:  WGS 84"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#checking-coordinate-system",
    "href": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#checking-coordinate-system",
    "title": "In-class Exercise 2:Fundamental of Geospatial Data Visualisation and tmap Methods",
    "section": "2.2 Checking coordinate system",
    "text": "2.2 Checking coordinate system\nTo check what CRS the data frame is using\n\nst_crs(mpsz2019_shp)\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#data-wrangling",
    "href": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#data-wrangling",
    "title": "In-class Exercise 2:Fundamental of Geospatial Data Visualisation and tmap Methods",
    "section": "3.1 Data Wrangling",
    "text": "3.1 Data Wrangling\n\npopdata2023 &lt;- popdata %&gt;%\n  group_by(PA, SZ, AG) %&gt;%\n  summarise(`POP` = sum(as.numeric(`Pop`))) %&gt;%\n  ungroup()%&gt;%\n  pivot_wider(names_from=AG, \n              values_from=POP)\n\nTo check the col order\n\ncolnames(popdata2023)\n\n [1] \"PA\"          \"SZ\"          \"0_to_4\"      \"10_to_14\"    \"15_to_19\"   \n [6] \"20_to_24\"    \"25_to_29\"    \"30_to_34\"    \"35_to_39\"    \"40_to_44\"   \n[11] \"45_to_49\"    \"50_to_54\"    \"55_to_59\"    \"5_to_9\"      \"60_to_64\"   \n[16] \"65_to_69\"    \"70_to_74\"    \"75_to_79\"    \"80_to_84\"    \"85_to_89\"   \n[21] \"90_and_Over\""
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#data-processing",
    "href": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#data-processing",
    "title": "In-class Exercise 2:Fundamental of Geospatial Data Visualisation and tmap Methods",
    "section": "3.2 Data Processing",
    "text": "3.2 Data Processing\n\npopdata2023 &lt;- popdata2023 %&gt;%\n  mutate(YOUNG = rowSums(.[3:6])\n         +rowSums(.[14])) %&gt;%\nmutate(`ECONOMY ACTIVE` = rowSums(.[7:13])+\nrowSums(.[15]))%&gt;%\nmutate(`AGED`=rowSums(.[16:21])) %&gt;%\nmutate(`TOTAL`=rowSums(.[3:21])) %&gt;%  \nmutate(`DEPENDENCY` = (`YOUNG` + `AGED`)\n/`ECONOMY ACTIVE`) %&gt;%\n  select(`PA`, `SZ`, `YOUNG`, \n       `ECONOMY ACTIVE`, `AGED`, \n       `TOTAL`, `DEPENDENCY`)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#joining-popdata2023-and-mpsz19_shp",
    "href": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#joining-popdata2023-and-mpsz19_shp",
    "title": "In-class Exercise 2:Fundamental of Geospatial Data Visualisation and tmap Methods",
    "section": "3.3 Joining popdata2023 and mpsz19_shp",
    "text": "3.3 Joining popdata2023 and mpsz19_shp\n\npopdata2023 &lt;- popdata2023 %&gt;% mutate_at(.vars = vars(PA,SZ), .funs = list(toupper))\n\n\nmpsz_pop2023 &lt;- left_join(mpsz2019_shp, popdata2023, by = c(\"SUBZONE_N\" = \"SZ\"))\n\n\npop2023_mpsz &lt;- left_join(popdata2023, mpsz2019_shp,\n                          by = c(\"SZ\" = \"SUBZONE_N\"))"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04/data/rawdata/Kepulauan_Bangka_Belitung.html",
    "href": "In-class_Ex/In-class_Ex04/data/rawdata/Kepulauan_Bangka_Belitung.html",
    "title": "IS415-GAA",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     \n\n\n       GEOGCRS[“WGS 84”,ENSEMBLE[“World Geodetic System 1984 ensemble”,MEMBER[“World Geodetic System 1984 (Transit)”],MEMBER[“World Geodetic System 1984 (G730)”],MEMBER[“World Geodetic System 1984 (G873)”],MEMBER[“World Geodetic System 1984 (G1150)”],MEMBER[“World Geodetic System 1984 (G1674)”],MEMBER[“World Geodetic System 1984 (G1762)”],MEMBER[“World Geodetic System 1984 (G2139)”],ELLIPSOID[“WGS 84”,6378137,298.257223563,LENGTHUNIT[“metre”,1]],ENSEMBLEACCURACY[2.0]],PRIMEM[“Greenwich”,0,ANGLEUNIT[“degree”,0.0174532925199433]],CS[ellipsoidal,2],AXIS[“geodetic latitude (Lat)”,north,ORDER[1],ANGLEUNIT[“degree”,0.0174532925199433]],AXIS[“geodetic longitude (Lon)”,east,ORDER[2],ANGLEUNIT[“degree”,0.0174532925199433]],USAGE[SCOPE[“Horizontal component of 3D system.”],AREA[“World.”],BBOX[-90,-180,90,180]],ID[“EPSG”,4326]] +proj=longlat +datum=WGS84 +no_defs 3452 4326 EPSG:4326 WGS 84 longlat EPSG:7030 true"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03/In-class_Ex03.html",
    "href": "In-class_Ex/In-class_Ex03/In-class_Ex03.html",
    "title": "In-class Exercise 3: Spatial Point Patterns Analysis",
    "section": "",
    "text": "maptools is retired and binary is remove from CRAN. However, we can download from Posit Public Package Manager snapshot using the code chunk below:\n\ninstall.packages(\"maptools\",\n                 repos = \"https://packagemanager.posit.co/cran/2023-10-13\")\n\n\n\n\n\n\n\nNote\n\n\n\nTo avoid install maptools every time the page being render, add “#| eval: false” in the code chunk."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#working-with-st_union",
    "href": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#working-with-st_union",
    "title": "In-class Exercise 3: Spatial Point Patterns Analysis",
    "section": "Working with st_union()",
    "text": "Working with st_union()\n\npacman::p_load(sf,tidyverse,tmap)\n\n\nmpsz_sf &lt;- st_read(dsn = \"data\", \n                layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `/Users/mingwei/Desktop/SMU/Y3S1/IS415/xXxPMWxXx/IS415-GAA/In-class_Ex/In-class_Ex03/data' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\n\nsg_sf &lt;- mpsz_sf %&gt;%\n  st_union()\n\n\nplot(sg_sf)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nowin is a window specifies the region of space within which spatial points are observed or analyzed. Represented a domain or boundary of the spatial data."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html",
    "title": "Take-home Exercise 1: Geospatial Analytics for Social Good: Application of Spatial and Spatio-temporal Point Patterns Analysis to discover the geographical distribution of Armed Conflict in Myanmar",
    "section": "",
    "text": "Millions of people have their lives shattered by armed conflict – wars – every year.\nArmed conflict has been on the rise since about 2012, after a decline in the 1990s and early 2000s. First came conflicts in Libya, Syria and Yemen, triggered by the 2011 Arab uprisings. Libya’s instability spilled south, helping set off a protracted crisis in the Sahel region. A fresh wave of major combat followed: the 2020 Azerbaijani-Armenian war over the Nagorno-Karabakh enclave, horrific fighting in Ethiopia’s northern Tigray region that began weeks later, the conflict prompted by the Myanmar army’s 2021 power grab and Russia’s 2022 assault on Ukraine. Add to those 2023’s devastation in Sudan and Gaza. Around the globe, more people are dying in fighting, being forced from their homes or in need of life-saving aid than in decades.\n\n\nGeospatial analytics hold tremendous potential to address complex problems facing society. In this study, I will apply spatial point patterns analysis methods to discover the spatial and spatio-temporal distribution of armed conflict in Myanmar.\n\n\n\n\n\nFor the purpose of this assignment, armed conflict data of Myanmar between 2021-2024 from Armed Conflict Location & Event Data (ACLED), an independent, impartial, international non-profit organization collecting data on violent conflict and protest in all countries and territories in the world, should be used.\nIn terms of event types, I will focus on at least four main event types, namely: Battles, Explosion/Remote violence, Strategic developments, and Violence against civilians.\nIn terms of study period, I will focus on quarterly armed conflict events from January 2021 until June 2024.\n\n\n\nI will be using Myanmar State and Region Boundaries with Sub-regions MIMU v9.4.\n\n\n\n\nThe following R packages are used for this assignment:\n\nsf, for importing, managing and processing geospatial data.\ntidyverse, for importing, wrangling and visualising data.\ntmap, to create thematic maps.\nspatstat, for point pattern analysis.\nraster,to reads, writes, manipulates, analyses and model of gridded spatial data (i.e. raster).\nsparr, provides functions for kernel density estimation.\nmagick, modern and simple toolkit for image processing in R.\ngifski, to create animated GIFs.\nstpp, for spatio-temporal analysis.\n\n\npacman::p_load(sf,tidyverse,tmap,spatstat,sparr,raster,magick,gifski,stpp)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#objective",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#objective",
    "title": "Take-home Exercise 1: Geospatial Analytics for Social Good: Application of Spatial and Spatio-temporal Point Patterns Analysis to discover the geographical distribution of Armed Conflict in Myanmar",
    "section": "",
    "text": "Geospatial analytics hold tremendous potential to address complex problems facing society. In this study, I will apply spatial point patterns analysis methods to discover the spatial and spatio-temporal distribution of armed conflict in Myanmar."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#data",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#data",
    "title": "Take-home Exercise 1: Geospatial Analytics for Social Good: Application of Spatial and Spatio-temporal Point Patterns Analysis to discover the geographical distribution of Armed Conflict in Myanmar",
    "section": "",
    "text": "For the purpose of this assignment, armed conflict data of Myanmar between 2021-2024 from Armed Conflict Location & Event Data (ACLED), an independent, impartial, international non-profit organization collecting data on violent conflict and protest in all countries and territories in the world, should be used.\nIn terms of event types, I will focus on at least four main event types, namely: Battles, Explosion/Remote violence, Strategic developments, and Violence against civilians.\nIn terms of study period, I will focus on quarterly armed conflict events from January 2021 until June 2024.\n\n\n\nI will be using Myanmar State and Region Boundaries with Sub-regions MIMU v9.4."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#importing-packages",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#importing-packages",
    "title": "Take-home Exercise 1: Geospatial Analytics for Social Good: Application of Spatial and Spatio-temporal Point Patterns Analysis to discover the geographical distribution of Armed Conflict in Myanmar",
    "section": "",
    "text": "The following R packages are used for this assignment:\n\nsf, for importing, managing and processing geospatial data.\ntidyverse, for importing, wrangling and visualising data.\ntmap, to create thematic maps.\nspatstat, for point pattern analysis.\nraster,to reads, writes, manipulates, analyses and model of gridded spatial data (i.e. raster).\nsparr, provides functions for kernel density estimation.\nmagick, modern and simple toolkit for image processing in R.\ngifski, to create animated GIFs.\nstpp, for spatio-temporal analysis.\n\n\npacman::p_load(sf,tidyverse,tmap,spatstat,sparr,raster,magick,gifski,stpp)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#aspatial-data",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#aspatial-data",
    "title": "Take-home Exercise 1: Geospatial Analytics for Social Good: Application of Spatial and Spatio-temporal Point Patterns Analysis to discover the geographical distribution of Armed Conflict in Myanmar",
    "section": "2.1 Aspatial Data",
    "text": "2.1 Aspatial Data\n\n2.1.1 Importing Data\n\nacled_df &lt;- read_csv(\"data/aspatial/2021-01-01-2024-06-30-Myanmar.csv\")\n\n\n\n2.1.2 CRS Adjustment\n\nst_crs(acled_df)\n\nCoordinate Reference System: NA\n\n\nAs the current acled_df do not have CRS, which means there is no geometry column in the object. Therefore, before we change the CRS, we need to convert the longitude and latitude columns into a spatial format. Since Myanmar is UTM zone 47(EPSG:32647), lets create the geometry column and transform the CRS to UTM Zone 47N.\n\nacled_df &lt;- acled_df %&gt;%\n  st_as_sf(coords = c(\"longitude\", \"latitude\"), crs=4326) %&gt;%\n  st_transform(crs = 32647)\n\n\nst_crs(acled_df)\n\nCoordinate Reference System:\n  User input: EPSG:32647 \n  wkt:\nPROJCRS[\"WGS 84 / UTM zone 47N\",\n    BASEGEOGCRS[\"WGS 84\",\n        ENSEMBLE[\"World Geodetic System 1984 ensemble\",\n            MEMBER[\"World Geodetic System 1984 (Transit)\"],\n            MEMBER[\"World Geodetic System 1984 (G730)\"],\n            MEMBER[\"World Geodetic System 1984 (G873)\"],\n            MEMBER[\"World Geodetic System 1984 (G1150)\"],\n            MEMBER[\"World Geodetic System 1984 (G1674)\"],\n            MEMBER[\"World Geodetic System 1984 (G1762)\"],\n            MEMBER[\"World Geodetic System 1984 (G2139)\"],\n            MEMBER[\"World Geodetic System 1984 (G2296)\"],\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]],\n            ENSEMBLEACCURACY[2.0]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4326]],\n    CONVERSION[\"UTM zone 47N\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",99,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",0.9996,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",500000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Navigation and medium accuracy spatial referencing.\"],\n        AREA[\"Between 96°E and 102°E, northern hemisphere between equator and 84°N, onshore and offshore. China. Indonesia. Laos. Malaysia - West Malaysia. Mongolia. Myanmar (Burma). Russian Federation. Thailand.\"],\n        BBOX[0,96,84,102]],\n    ID[\"EPSG\",32647]]\n\n\nWe can see that now the CRS is UTM Zone 47 now.\n\n\n2.1.3 Check and Clean Aspatial Data\n\nhead(acled_df, n =1)\n\nSimple feature collection with 1 feature and 29 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 214961 ymin: 2452068 xmax: 214961 ymax: 2452068\nProjected CRS: WGS 84 / UTM zone 47N\n# A tibble: 1 × 30\n  event_id_cnty event_date    year time_precision disorder_type      event_type\n  &lt;chr&gt;         &lt;chr&gt;        &lt;dbl&gt;          &lt;dbl&gt; &lt;chr&gt;              &lt;chr&gt;     \n1 MMR64313      30 June 2024  2024              1 Political violence Battles   \n# ℹ 24 more variables: sub_event_type &lt;chr&gt;, actor1 &lt;chr&gt;, assoc_actor_1 &lt;chr&gt;,\n#   inter1 &lt;dbl&gt;, actor2 &lt;chr&gt;, assoc_actor_2 &lt;chr&gt;, inter2 &lt;dbl&gt;,\n#   interaction &lt;dbl&gt;, civilian_targeting &lt;chr&gt;, iso &lt;dbl&gt;, region &lt;chr&gt;,\n#   country &lt;chr&gt;, admin1 &lt;chr&gt;, admin2 &lt;chr&gt;, admin3 &lt;chr&gt;, location &lt;chr&gt;,\n#   geo_precision &lt;dbl&gt;, source &lt;chr&gt;, source_scale &lt;chr&gt;, notes &lt;chr&gt;,\n#   fatalities &lt;dbl&gt;, tags &lt;chr&gt;, timestamp &lt;dbl&gt;, geometry &lt;POINT [m]&gt;\n\n\nBy checking the data, we can see that the data type for event_date was in Character instead of Date. And there is a timestamp field, in order to figure out what is this field for, we need to convert it to human readable as well.\nLets convert the data type for event_date to Date and timestamp to normal date time format, so we handle the field easier at later stage.\n\nacled_df &lt;- acled_df %&gt;%\n  mutate(event_date = dmy(event_date)) %&gt;%\n  mutate(timestamp = as.POSIXct(timestamp, origin = \"1970-01-01\", tz = \"Asia/Yangon\"))\n\nBy looking at the data, it seems that the timestamp field was the date time when the data was recorded. Therefore, this field would not be useful for us.\n\n\n2.1.4 Data Transformation\nSince we will be focus on quarterly, lets extract year and quarter from event_date field.\n\nacled_df &lt;- acled_df %&gt;%\n  mutate(year = year(event_date),\n         quarter = quarter(event_date))\n\n\nsummary(acled_df)\n\n event_id_cnty        event_date              year      time_precision \n Length:51553       Min.   :2021-01-01   Min.   :2021   Min.   :1.000  \n Class :character   1st Qu.:2021-10-23   1st Qu.:2021   1st Qu.:1.000  \n Mode  :character   Median :2022-07-28   Median :2022   Median :1.000  \n                    Mean   :2022-08-25   Mean   :2022   Mean   :1.045  \n                    3rd Qu.:2023-07-05   3rd Qu.:2023   3rd Qu.:1.000  \n                    Max.   :2024-06-30   Max.   :2024   Max.   :3.000  \n disorder_type       event_type        sub_event_type        actor1         \n Length:51553       Length:51553       Length:51553       Length:51553      \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n assoc_actor_1          inter1         actor2          assoc_actor_2     \n Length:51553       Min.   :1.000   Length:51553       Length:51553      \n Class :character   1st Qu.:1.000   Class :character   Class :character  \n Mode  :character   Median :2.000   Mode  :character   Mode  :character  \n                    Mean   :2.648                                        \n                    3rd Qu.:3.000                                        \n                    Max.   :8.000                                        \n     inter2       interaction    civilian_targeting      iso     \n Min.   :0.000   Min.   :10.00   Length:51553       Min.   :104  \n 1st Qu.:0.000   1st Qu.:13.00   Class :character   1st Qu.:104  \n Median :1.000   Median :17.00   Mode  :character   Median :104  \n Mean   :2.992   Mean   :25.33                      Mean   :104  \n 3rd Qu.:7.000   3rd Qu.:33.00                      3rd Qu.:104  \n Max.   :8.000   Max.   :80.00                      Max.   :104  \n    region            country             admin1             admin2         \n Length:51553       Length:51553       Length:51553       Length:51553      \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n    admin3            location         geo_precision      source         \n Length:51553       Length:51553       Min.   :1.000   Length:51553      \n Class :character   Class :character   1st Qu.:1.000   Class :character  \n Mode  :character   Mode  :character   Median :2.000   Mode  :character  \n                                       Mean   :1.508                     \n                                       3rd Qu.:2.000                     \n                                       Max.   :3.000                     \n source_scale          notes             fatalities          tags          \n Length:51553       Length:51553       Min.   :  0.000   Length:51553      \n Class :character   Class :character   1st Qu.:  0.000   Class :character  \n Mode  :character   Mode  :character   Median :  0.000   Mode  :character  \n                                       Mean   :  1.062                     \n                                       3rd Qu.:  1.000                     \n                                       Max.   :201.000                     \n   timestamp                               geometry        quarter     \n Min.   :2021-01-19 23:11:26.00   POINT        :51553   Min.   :1.000  \n 1st Qu.:2023-12-12 07:49:00.00   epsg:32647   :    0   1st Qu.:1.000  \n Median :2024-04-17 00:47:50.00   +proj=utm ...:    0   Median :2.000  \n Mean   :2023-12-09 18:55:27.39                         Mean   :2.382  \n 3rd Qu.:2024-06-26 04:30:12.00                         3rd Qu.:3.000  \n Max.   :2024-09-04 08:18:12.00                         Max.   :4.000  \n\n\n\n2.1.4.1 Handling Categorical Data\nSince there are fields in the dataset are categorical related fields and they are in character vectors. To make sure consistency of the data, we can use as.factor function to convert the variable to categorical variable. These fields are: disorder_type, event_type, sub_event_type, actor1, actor2, admin1 , source_scale etc.\nBy doing so, it makes easier to group and summarize data by categories and will provide more consistent behavior.\n\nacled_df &lt;- acled_df %&gt;%\n  mutate( event_type = as.factor(event_type),\n          admin1 = as.factor(admin1)\n         )\n\n\n\n\n\n\n\nNote\n\n\n\nAs there are other categorical fields, but since not all the field will be used and useful(as some of the field have too many level), hence I only factor fields that will be useful for the next stage.\n\n\nTo conserve memory, we should select only the fields relevant to the analysis and save them in RDS format, making it more convenient for future reference.\n\nacled_df &lt;- acled_df %&gt;%\n  dplyr::select(event_date,year,quarter,event_type,actor1,admin1,fatalities)\n\n\nwrite_rds(acled_df, \"data/rds/acled_df.rds\")\n\n\nacled_df &lt;- read_rds( \"data/rds/acled_df.rds\")\n\nTo check the levels.\n\nlevels(acled_df$event_type)\n\n[1] \"Battles\"                    \"Explosions/Remote violence\"\n[3] \"Protests\"                   \"Riots\"                     \n[5] \"Strategic developments\"     \"Violence against civilians\""
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#geospatial-data",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#geospatial-data",
    "title": "Take-home Exercise 1: Geospatial Analytics for Social Good: Application of Spatial and Spatio-temporal Point Patterns Analysis to discover the geographical distribution of Armed Conflict in Myanmar",
    "section": "2.2 Geospatial Data",
    "text": "2.2 Geospatial Data\n\n2.2.1 Importing Data\n\nmsrb_sub_reg &lt;- st_read(dsn = \"data/geospatial/mmr_polbnda2_adm1_250k_mimu_1\", layer = \"mmr_polbnda2_adm1_250k_mimu_1\")\n\nReading layer `mmr_polbnda2_adm1_250k_mimu_1' from data source \n  `/Users/mingwei/Desktop/SMU/Y3S1/IS415/xXxPMWxXx/IS415-GAA/Take-home_Ex/Take-home_Ex01/data/geospatial/mmr_polbnda2_adm1_250k_mimu_1' \n  using driver `ESRI Shapefile'\nSimple feature collection with 18 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 92.1721 ymin: 9.696844 xmax: 101.17 ymax: 28.54554\nGeodetic CRS:  WGS 84\n\n\n\n\n2.2.2 CRS Adjustments\nLets check what is the CRS for msrb_sub_reg.\n\nst_crs(msrb_sub_reg)\n\nCoordinate Reference System:\n  User input: WGS 84 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    DATUM[\"World Geodetic System 1984\",\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"latitude\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"longitude\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    ID[\"EPSG\",4326]]\n\n\nAs from above, we can see that the original data is geographic coordinate system. We need to convert it to projected coordinate system. Since Myanmar is UTM zone 47(EPSG:32647), lets transform the CRS to UTM Zone 47N.\n\nmsrb_sub_reg &lt;- st_transform(msrb_sub_reg, crs = 32647)\nst_crs(msrb_sub_reg)\n\nCoordinate Reference System:\n  User input: EPSG:32647 \n  wkt:\nPROJCRS[\"WGS 84 / UTM zone 47N\",\n    BASEGEOGCRS[\"WGS 84\",\n        ENSEMBLE[\"World Geodetic System 1984 ensemble\",\n            MEMBER[\"World Geodetic System 1984 (Transit)\"],\n            MEMBER[\"World Geodetic System 1984 (G730)\"],\n            MEMBER[\"World Geodetic System 1984 (G873)\"],\n            MEMBER[\"World Geodetic System 1984 (G1150)\"],\n            MEMBER[\"World Geodetic System 1984 (G1674)\"],\n            MEMBER[\"World Geodetic System 1984 (G1762)\"],\n            MEMBER[\"World Geodetic System 1984 (G2139)\"],\n            MEMBER[\"World Geodetic System 1984 (G2296)\"],\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]],\n            ENSEMBLEACCURACY[2.0]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4326]],\n    CONVERSION[\"UTM zone 47N\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",99,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",0.9996,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",500000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Navigation and medium accuracy spatial referencing.\"],\n        AREA[\"Between 96°E and 102°E, northern hemisphere between equator and 84°N, onshore and offshore. China. Indonesia. Laos. Malaysia - West Malaysia. Mongolia. Myanmar (Burma). Russian Federation. Thailand.\"],\n        BBOX[0,96,84,102]],\n    ID[\"EPSG\",32647]]\n\n\n\n\n2.2.3 Check and Clean Geospatial Data\nTo ensure that the geometries in the shapefile are valid and correct them if any invalid geometries found.\n\n# Check if the data are valid\nst_is_valid(msrb_sub_reg)\n\n [1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[16] TRUE TRUE TRUE\n\n# Fix invalid data\nmsrb_sub_reg &lt;- st_make_valid(msrb_sub_reg)\n\n\n\n2.2.4 Standardize Column Names\nWe can see that the column name are not descriptive.\n\nmsrb_sub_reg\n\nSimple feature collection with 18 features and 6 fields\nGeometry type: GEOMETRY\nDimension:     XY\nBounding box:  xmin: -210008.6 ymin: 1072026 xmax: 724647.6 ymax: 3158467\nProjected CRS: WGS 84 / UTM zone 47N\nFirst 10 features:\n   OBJECTID          ST ST_PCODE  ST_RG               ST_MMR PCode_V\n1         1  Ayeyarwady   MMR017 Region       ဧရာဝတီတိုင်းဒေသကြီး     9.4\n2         2 Bago (East)   MMR007 Region   ပဲခူးတိုင်းဒေသကြီး (အရှေ့)     9.4\n3         3 Bago (West)   MMR008 Region ပဲခူးတိုင်းဒေသကြီး (အနောက်)     9.4\n4         4        Chin   MMR004  State            ချင်းပြည်နယ်     9.4\n5         5      Kachin   MMR001  State            ကချင်ပြည်နယ်     9.4\n6         6       Kayah   MMR002  State            ကယားပြည်နယ်     9.4\n7         7       Kayin   MMR003  State             ကရင်ပြည်နယ်     9.4\n8         8      Magway   MMR009 Region        မကွေးတိုင်းဒေသကြီး     9.4\n9         9    Mandalay   MMR010 Region      မန္တလေးတိုင်းဒေသကြီး     9.4\n10       10         Mon   MMR011  State              မွန်ပြည်နယ်     9.4\n                         geometry\n1  MULTIPOLYGON (((-5792.088 1...\n2  POLYGON ((203949.9 2157841,...\n3  POLYGON ((153405.1 2125288,...\n4  POLYGON ((-72918.03 2675831...\n5  POLYGON ((362696.3 3156293,...\n6  POLYGON ((309155.7 2211716,...\n7  MULTIPOLYGON (((373550.1 18...\n8  POLYGON ((-1717.607 2525523...\n9  POLYGON ((208184.3 2614332,...\n10 MULTIPOLYGON (((350576.4 17...\n\n\nRename the column name to more descriptive name and remove not useful column .\n\nmsrb_sub_reg &lt;- msrb_sub_reg %&gt;%\n  rename(state_name = ST,\n         state_pcode = ST_PCODE,\n         state_region = ST_RG) %&gt;%\n  dplyr::select(-ST_MMR)\n\n\nsummary(msrb_sub_reg)\n\n    OBJECTID      state_name        state_pcode        state_region      \n Min.   : 1.00   Length:18          Length:18          Length:18         \n 1st Qu.: 5.25   Class :character   Class :character   Class :character  \n Median : 9.50   Mode  :character   Mode  :character   Mode  :character  \n Mean   : 9.50                                                           \n 3rd Qu.:13.75                                                           \n Max.   :18.00                                                           \n    PCode_V             geometry \n Min.   :9.4   MULTIPOLYGON : 6  \n 1st Qu.:9.4   POLYGON      :12  \n Median :9.4   epsg:32647   : 0  \n Mean   :9.4   +proj=utm ...: 0  \n 3rd Qu.:9.4                     \n Max.   :9.4                     \n\n\n\n\n2.2.5 Myanmar Boundaries\n\nplot(st_geometry(msrb_sub_reg))\n\n\n\n\n\n\n\n\nSave into RDS, for future use.\n\nwrite_rds(msrb_sub_reg,\"data/rds/msrb_sub_reg.rds\")\n\n\n\n2.2.6 Myanmar Boundaries With State Name\n\nggplot(data = msrb_sub_reg) +\n  geom_sf(aes(fill = state_name), color = \"black\") + \n  geom_sf_label(aes(label = state_name), size = 4, color = \"black\",fill = \"white\", label.size = 0.5) + \n  scale_fill_hue(n = length(unique(msrb_sub_reg$state_name))) +\n  theme(legend.position = \"right\") +\n  ggtitle(\"Myanmar Administrative Boundaries\")"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#remove-island",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#remove-island",
    "title": "Take-home Exercise 1: Geospatial Analytics for Social Good: Application of Spatial and Spatio-temporal Point Patterns Analysis to discover the geographical distribution of Armed Conflict in Myanmar",
    "section": "3.1 Remove island",
    "text": "3.1 Remove island\nAs we can see from above, the Myanmar boundaries have many islands, which will affect us as when plot the points. Therefore, by removing smaller island,it will help us in visualization.\n\n# Merge adj polygons\nmerged_msrb &lt;- st_union(msrb_sub_reg) %&gt;%\n  st_cast(\"POLYGON\")\n# Convert back to sf object\nmerged_msrb &lt;- st_as_sf(merged_msrb)\n# set threshold\nmin_area_threshold &lt;- units::set_units(100, \"km^2\")\n\nareas &lt;- st_area(merged_msrb)\n\n# Filter out polygons smaller than the threshold\nfiltered_msrb &lt;- merged_msrb %&gt;% \n  filter(areas &gt; min_area_threshold)\n\nplot(st_geometry(filtered_msrb), main = \"Without Small Islands\")\n\n\n\n\n\n\n\n\n\nwrite_rds(filtered_msrb, \"data/rds/filtered_msrb.rds\")"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#create-owin-object",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#create-owin-object",
    "title": "Take-home Exercise 1: Geospatial Analytics for Social Good: Application of Spatial and Spatio-temporal Point Patterns Analysis to discover the geographical distribution of Armed Conflict in Myanmar",
    "section": "3.2 Create owin Object",
    "text": "3.2 Create owin Object\nAfter removing the small island, we can now create an owin (observed window) object, which we will use when creating the KDE layer.\n\nmsrb_owin &lt;- as.owin(filtered_msrb)\nplot(msrb_owin)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#determine-best-sigma-and-kernel",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#determine-best-sigma-and-kernel",
    "title": "Take-home Exercise 1: Geospatial Analytics for Social Good: Application of Spatial and Spatio-temporal Point Patterns Analysis to discover the geographical distribution of Armed Conflict in Myanmar",
    "section": "3.3 Determine Best Sigma And Kernel",
    "text": "3.3 Determine Best Sigma And Kernel\nTo find out which sigma and kernel to use for KDE, we will try various type and see which give us the best visualization. We will use data that is battles event in 2021 Q1 as the sample data.\n\nbattles_2021_Q1 &lt;- acled_df %&gt;%\n  filter(event_type == 'Battles' & year == 2021 & quarter == 1)\n\nbattles_2021_Q1_ppp &lt;- as.ppp(st_coordinates(battles_2021_Q1), st_bbox(battles_2021_Q1))\nany(duplicated(battles_2021_Q1_ppp))\n\n[1] TRUE\n\n\nUsing jittering, to add a small perturbation to the duplicate points so that they do not occupy the exact same space.\n\nbattles_2021_Q1_ppp &lt;- rjitter(battles_2021_Q1_ppp, \n                             retry=TRUE, \n                             nsim=1, \n                             drop=TRUE)\nany(duplicated(battles_2021_Q1_ppp))\n\n[1] FALSE\n\n\nTo apply the owin object to the ppp object.”\n\nbattles_2021_Q1_ppp = battles_2021_Q1_ppp[msrb_owin]\nplot(battles_2021_Q1_ppp, pch = 20, cex = 1)\n\n\n\n\n\n\n\n\n\n3.3.1 Comparing Different Sigma\n\npar(mfrow = c(2, 2))\npar(mar = c(3, 3, 2, 1))  # adjust the margin values\n\nplot(density(battles_2021_Q1_ppp, sigma = bw.ppl, edge = TRUE, kernel = \"gaussian\"), main = \"PPL\")\nplot(density(battles_2021_Q1_ppp, sigma = bw.CvL, edge = TRUE, kernel = \"gaussian\"), main = \"CvL\")\nplot(density(battles_2021_Q1_ppp, sigma = bw.scott, edge = TRUE, kernel = \"gaussian\"), main = \"Scott\")\nplot(density(battles_2021_Q1_ppp, sigma = bw.diggle, edge = TRUE, kernel = \"gaussian\"), main = \"Diggle\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nAs can be seen above, among the four types of sigma, PPL provides the best visualization. Both Scott and CvL methods result in too much spread, while Diggle makes it difficult to see the points in the bottom part of the plot.\n\n\n\n\n3.3.2 Comparing Different Kernel\nCheck the bandwidth for bw.ppl.\n\nbw &lt;- bw.ppl(battles_2021_Q1_ppp)\nbw\n\n   sigma \n43203.34 \n\n\n\n# Rescale the measurement unit\nbattles_2021_Q1_ppp.km &lt;- rescale(battles_2021_Q1_ppp, 55000, \"km\")\n\n\npar(mfrow = c(2, 2))\npar(mar = c(3, 3, 2, 1))\n\nplot(density(battles_2021_Q1_ppp.km, sigma = bw.ppl, edge = TRUE, kernel = \"gaussian\"), main = \"Gaussian\")\nplot(density(battles_2021_Q1_ppp.km, sigma = bw.ppl, edge = TRUE, kernel = \"epanechnikov\"), main = \"Epanechnikov\")\nplot(density(battles_2021_Q1_ppp.km, sigma = bw.ppl, edge = TRUE, kernel = \"quartic\"), main = \"Quartic\")\nplot(density(battles_2021_Q1_ppp.km, sigma = bw.ppl, edge = TRUE, kernel = \"disc\"), main = \"Disc\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nAmong the four kernels, the Gaussian kernel provides the smoothest visualization. Therefore, we will use Gaussian kernel for our analysis."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#ay2021",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#ay2021",
    "title": "Take-home Exercise 1: Geospatial Analytics for Social Good: Application of Spatial and Spatio-temporal Point Patterns Analysis to discover the geographical distribution of Armed Conflict in Myanmar",
    "section": "3.4 AY2021",
    "text": "3.4 AY2021\nAs I am interested in understanding an overall picture of the conflict situation each year, including how different types of conflicts happen concurrently. Therefore, I will analysis it by year and event type. By doing this way it will give me the following pros and cons:\n\nPros:\n\nHolistic View: I can compare multiple event types within the same time frame, identifying relationships or patterns between them.\nComprehensive Spatial Understanding: Provide a snapshot of all event types within a year, is there any interactions between event type. (e.g. Battles occur near Violence against civilians)\n\nCons:\n\nTemporal Trends Less Visible: This way does not provide insight into how a specific event type evolves over the years.\n\n\n\n3.4.1 Q1\n\n\nClick to expand/collapse code\nevent_types &lt;- c(\"Battles\", \"Explosions/Remote violence\", \"Strategic developments\", \"Violence against civilians\")\n\npar(mfrow = c(2, 2))\npar(mar = c(3, 3, 2, 1))\n# Create a plot for each event type\nfor (event in event_types) {\n  # Filter data by event type\n  filtered_data &lt;- acled_df %&gt;%\n    filter(event_type == event & year == 2021 & quarter == 1)\n  \n  # Convert to ppp format\n  ppp_data &lt;- as.ppp(st_coordinates(filtered_data), st_bbox(filtered_data))\n  \n  # Handle duplicated points\n  ppp_data &lt;- rjitter(ppp_data, retry = TRUE, nsim = 1, drop = TRUE)\n  \n  # Extract conflict Event located within Myanmar\n  ppp_data &lt;- ppp_data[msrb_owin]\n  \n  # Re-scale\n  ppp_data.km &lt;- rescale(ppp_data, 55000, \"km\")\n  \n  # Plot KDE\n  plot(density(ppp_data.km, \n               sigma = bw.ppl, \n               edge = TRUE, \n               kernel = \"gaussian\"), \n       main = paste(\"2021 Q1 -\", event))\n}\n\n\n\n\n\n\n\n\n\n\n\n3.4.2 Q2\n\n\nClick to expand/collapse code\npar(mfrow = c(2, 2))\npar(mar = c(3, 3, 2, 1))\n# Create a plot for each event type\nfor (event in event_types) {\n  # Filter data by event type\n  filtered_data &lt;- acled_df %&gt;%\n    filter(event_type == event & year == 2021 & quarter == 2)\n  \n  # Convert to ppp format\n  ppp_data &lt;- as.ppp(st_coordinates(filtered_data), st_bbox(filtered_data))\n  \n  # Handle duplicated points\n  ppp_data &lt;- rjitter(ppp_data, retry = TRUE, nsim = 1, drop = TRUE)\n  \n  # Extract conflict Event located within Myanmar\n  ppp_data &lt;- ppp_data[msrb_owin]\n  \n  # Re-scale\n  ppp_data.km &lt;- rescale(ppp_data, 55000, \"km\")\n  \n  # Plot density\n  plot(density(ppp_data.km, \n               sigma = bw.ppl, \n               edge = TRUE, \n               kernel = \"gaussian\"), \n       main = paste(\"2021 Q2 -\", event))\n}\n\n\n\n\n\n\n\n\n\n\n\n3.4.3 Q3\n\n\nClick to expand/collapse code\npar(mfrow = c(2, 2))\npar(mar = c(3, 3, 2, 1))\n# Create a plot for each event type\nfor (event in event_types) {\n  # Filter data by event type\n  filtered_data &lt;- acled_df %&gt;%\n    filter(event_type == event & year == 2021 & quarter == 3)\n  \n  # Convert to ppp format\n  ppp_data &lt;- as.ppp(st_coordinates(filtered_data), st_bbox(filtered_data))\n  \n  # Handle duplicated points\n  ppp_data &lt;- rjitter(ppp_data, retry = TRUE, nsim = 1, drop = TRUE)\n  \n  # Extract conflict Event located within Myanmar\n  ppp_data &lt;- ppp_data[msrb_owin]\n  \n  # Re-scale\n  ppp_data.km &lt;- rescale(ppp_data, 55000, \"km\")\n  \n  # Plot density\n  plot(density(ppp_data.km, \n               sigma = bw.ppl, \n               edge = TRUE, \n               kernel = \"gaussian\"), \n       main = paste(\"2021 Q3 -\", event))\n}\n\n\n\n\n\n\n\n\n\n\n\n3.4.4 Q4\n\n\nClick to expand/collapse code\npar(mfrow = c(2, 2))\npar(mar = c(3, 3, 2, 1))\n# Create a plot for each event type\nfor (event in event_types) {\n  # Filter data by event type\n  filtered_data &lt;- acled_df %&gt;%\n    filter(event_type == event & year == 2021 & quarter == 4)\n  \n  # Convert to ppp format\n  ppp_data &lt;- as.ppp(st_coordinates(filtered_data), st_bbox(filtered_data))\n  \n  # Handle duplicated points\n  ppp_data &lt;- rjitter(ppp_data, retry = TRUE, nsim = 1, drop = TRUE)\n  \n  # Extract conflict Event located within Myanmar\n  ppp_data &lt;- ppp_data[msrb_owin]\n  \n  # Re-scale\n  ppp_data.km &lt;- rescale(ppp_data, 55000, \"km\")\n  \n  # Plot density\n  plot(density(ppp_data.km, \n               sigma = bw.ppl, \n               edge = TRUE, \n               kernel = \"gaussian\"), \n       main = paste(\"2021 Q4 -\", event))\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2021 Insight\n\n\n\n\nIn Q1 and Q2 of 2021, we can observe that Battles and Explosions/Remote Violence tend to occur in similar locations(the boundary between Kachin state and Shan state; Rakhine State and the boundary between Kayin State, Mon State and Yangon Division), suggesting a closer relationship between these two event type.\nOn the other hand, Strategic Developments and Violence Against Civilians appear to be spatially correlated with each other, mostly at the boundary between Magway Division and Mandalay Division , but less so with Battles and Explosions/Remote Violence.\nBy Q3 and Q4, as the frequency of conflicts increase, the spatial distribution of all four event type begins to converge mostly at the boundary between Magway Division and Mandalay Division. This suggests that these events are increasingly co-occurring in the similar geographic areas."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#ay2022",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#ay2022",
    "title": "Take-home Exercise 1: Geospatial Analytics for Social Good: Application of Spatial and Spatio-temporal Point Patterns Analysis to discover the geographical distribution of Armed Conflict in Myanmar",
    "section": "3.5 AY2022",
    "text": "3.5 AY2022\n\n3.5.1 Q1\n\n\nClick to expand/collapse code\npar(mfrow = c(2, 2))\npar(mar = c(3, 3, 2, 1))\n# Create a plot for each event type\nfor (event in event_types) {\n  # Filter data by event type\n  filtered_data &lt;- acled_df %&gt;%\n    filter(event_type == event & year == 2022 & quarter == 1)\n  \n  # Convert to ppp format\n  ppp_data &lt;- as.ppp(st_coordinates(filtered_data), st_bbox(filtered_data))\n  \n  # Handle duplicated points\n  ppp_data &lt;- rjitter(ppp_data, retry = TRUE, nsim = 1, drop = TRUE)\n  \n  # Extract conflict Event located within Myanmar\n  ppp_data &lt;- ppp_data[msrb_owin]\n  \n  # Re-scale\n  ppp_data.km &lt;- rescale(ppp_data, 55000, \"km\")\n  \n  # Plot density\n  plot(density(ppp_data.km, \n               sigma = bw.ppl, \n               edge = TRUE, \n               kernel = \"gaussian\"), \n       main = paste(\"2022 Q1 -\", event))\n}\n\n\n\n\n\n\n\n\n\n\n\n3.5.2 Q2\n\n\nClick to expand/collapse code\npar(mfrow = c(2, 2))\npar(mar = c(3, 3, 2, 1))\n# Create a plot for each event type\nfor (event in event_types) {\n  # Filter data by event type\n  filtered_data &lt;- acled_df %&gt;%\n    filter(event_type == event & year == 2022 & quarter == 2)\n  \n  # Convert to ppp format\n  ppp_data &lt;- as.ppp(st_coordinates(filtered_data), st_bbox(filtered_data))\n  \n  # Handle duplicated points\n  ppp_data &lt;- rjitter(ppp_data, retry = TRUE, nsim = 1, drop = TRUE)\n  \n  # Extract conflict Event located within Myanmar\n  ppp_data &lt;- ppp_data[msrb_owin]\n  \n  # Re-scale\n  ppp_data.km &lt;- rescale(ppp_data, 55000, \"km\")\n  \n  # Plot density\n  plot(density(ppp_data.km, \n               sigma = bw.ppl, \n               edge = TRUE, \n               kernel = \"gaussian\"), \n       main = paste(\"2022 Q2 -\", event))\n}\n\n\n\n\n\n\n\n\n\n\n\n3.5.3 Q3\n\npar(mfrow = c(2, 2))\npar(mar = c(3, 3, 2, 1))\n# Create a plot for each event type\nfor (event in event_types) {\n  # Filter data by event type\n  filtered_data &lt;- acled_df %&gt;%\n    filter(event_type == event & year == 2022 & quarter == 3)\n  \n  # Convert to ppp format\n  ppp_data &lt;- as.ppp(st_coordinates(filtered_data), st_bbox(filtered_data))\n  \n  # Handle duplicated points\n  ppp_data &lt;- rjitter(ppp_data, retry = TRUE, nsim = 1, drop = TRUE)\n  \n  # Extract conflict Event located within Myanmar\n  ppp_data &lt;- ppp_data[msrb_owin]\n  \n  # Re-scale\n  ppp_data.km &lt;- rescale(ppp_data, 55000, \"km\")\n  \n  # Plot density\n  plot(density(ppp_data.km, \n               sigma = bw.ppl, \n               edge = TRUE, \n               kernel = \"gaussian\"), \n       main = paste(\"2022 Q3 -\", event))\n}\n\n\n\n\n\n\n\n\n\n\n3.5.4 Q4\n\n\nClick to expand/collapse code\npar(mfrow = c(2, 2))\npar(mar = c(3, 3, 2, 1))\n# Create a plot for each event type\nfor (event in event_types) {\n  # Filter data by event type\n  filtered_data &lt;- acled_df %&gt;%\n    filter(event_type == event & year == 2022 & quarter == 4)\n  \n  # Convert to ppp format\n  ppp_data &lt;- as.ppp(st_coordinates(filtered_data), st_bbox(filtered_data))\n  \n  # Handle duplicated points\n  ppp_data &lt;- rjitter(ppp_data, retry = TRUE, nsim = 1, drop = TRUE)\n  \n  # Extract conflict Event located within Myanmar\n  ppp_data &lt;- ppp_data[msrb_owin]\n  \n  # Re-scale\n  ppp_data.km &lt;- rescale(ppp_data, 55000, \"km\")\n  \n  # Plot density\n  plot(density(ppp_data.km, \n               sigma = bw.ppl, \n               edge = TRUE, \n               kernel = \"gaussian\"), \n       main = paste(\"2022 Q4 -\", event))\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2022 Insight\n\n\n\n\nIn 2022, conflict levels remained high across all four event types. Most conflicts continued to occur along the boundary between Magway Division and Mandalay Division (central region). Toward end of the year, there was a noticeable increase in conflicts in the southern region, particularly in Battles and Violence Against Civilians events."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#ay2023",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#ay2023",
    "title": "Take-home Exercise 1: Geospatial Analytics for Social Good: Application of Spatial and Spatio-temporal Point Patterns Analysis to discover the geographical distribution of Armed Conflict in Myanmar",
    "section": "3.6 AY2023",
    "text": "3.6 AY2023\n\n3.6.1 Q1\n\n\nClick to expand/collapse code\npar(mfrow = c(2, 2))\npar(mar = c(3, 3, 2, 1))\n# Create a plot for each event type\nfor (event in event_types) {\n  # Filter data by event type\n  filtered_data &lt;- acled_df %&gt;%\n    filter(event_type == event & year == 2023 & quarter == 1)\n  \n  # Convert to ppp format\n  ppp_data &lt;- as.ppp(st_coordinates(filtered_data), st_bbox(filtered_data))\n  \n  # Handle duplicated points\n  ppp_data &lt;- rjitter(ppp_data, retry = TRUE, nsim = 1, drop = TRUE)\n  \n  # Extract conflict Event located within Myanmar\n  ppp_data &lt;- ppp_data[msrb_owin]\n  \n  # Re-scale\n  ppp_data.km &lt;- rescale(ppp_data, 55000, \"km\")\n  \n  # Plot density\n  plot(density(ppp_data.km, \n               sigma = bw.ppl, \n               edge = TRUE, \n               kernel = \"gaussian\"), \n       main = paste(\"2023 Q1 -\", event))\n}\n\n\n\n\n\n\n\n\n\n\n\n3.6.2 Q2\n\n\nClick to expand/collapse code\npar(mfrow = c(2, 2))\npar(mar = c(3, 3, 2, 1))\n# Create a plot for each event type\nfor (event in event_types) {\n  # Filter data by event type\n  filtered_data &lt;- acled_df %&gt;%\n    filter(event_type == event & year == 2023 & quarter == 2)\n  \n  # Convert to ppp format\n  ppp_data &lt;- as.ppp(st_coordinates(filtered_data), st_bbox(filtered_data))\n  \n  # Handle duplicated points\n  ppp_data &lt;- rjitter(ppp_data, retry = TRUE, nsim = 1, drop = TRUE)\n  \n  # Extract conflict Event located within Myanmar\n  ppp_data &lt;- ppp_data[msrb_owin]\n  \n  # Re-scale\n  ppp_data.km &lt;- rescale(ppp_data, 55000, \"km\")\n  \n  # Plot density\n  plot(density(ppp_data.km, \n               sigma = bw.ppl, \n               edge = TRUE, \n               kernel = \"gaussian\"), \n       main = paste(\"2023 Q2 -\", event))\n}\n\n\n\n\n\n\n\n\n\n\n\n3.6.3 Q3\n\n\nClick to expand/collapse code\npar(mfrow = c(2, 2))\npar(mar = c(3, 3, 2, 1))\n# Create a plot for each event type\nfor (event in event_types) {\n  # Filter data by event type\n  filtered_data &lt;- acled_df %&gt;%\n    filter(event_type == event & year == 2023 & quarter == 3)\n  \n  # Convert to ppp format\n  ppp_data &lt;- as.ppp(st_coordinates(filtered_data), st_bbox(filtered_data))\n  \n  # Handle duplicated points\n  ppp_data &lt;- rjitter(ppp_data, retry = TRUE, nsim = 1, drop = TRUE)\n  \n  # Extract conflict Event located within Myanmar\n  ppp_data &lt;- ppp_data[msrb_owin]\n  \n  # Re-scale\n  ppp_data.km &lt;- rescale(ppp_data, 55000, \"km\")\n  \n  # Plot density\n  plot(density(ppp_data.km, \n               sigma = bw.ppl, \n               edge = TRUE, \n               kernel = \"gaussian\"), \n       main = paste(\"2023 Q3 -\", event))\n}\n\n\n\n\n\n\n\n\n\n\n\n3.6.4 Q4\n\n\nClick to expand/collapse code\npar(mfrow = c(2, 2))\npar(mar = c(3, 3, 2, 1))\n# Create a plot for each event type\nfor (event in event_types) {\n  # Filter data by event type\n  filtered_data &lt;- acled_df %&gt;%\n    filter(event_type == event & year == 2023 & quarter == 4)\n  \n  # Convert to ppp format\n  ppp_data &lt;- as.ppp(st_coordinates(filtered_data), st_bbox(filtered_data))\n  \n  # Handle duplicated points\n  ppp_data &lt;- rjitter(ppp_data, retry = TRUE, nsim = 1, drop = TRUE)\n  \n  # Extract conflict Event located within Myanmar\n  ppp_data &lt;- ppp_data[msrb_owin]\n  \n  # Re-scale\n  ppp_data.km &lt;- rescale(ppp_data, 55000, \"km\")\n  \n  # Plot density\n  plot(density(ppp_data.km, \n               sigma = bw.ppl, \n               edge = TRUE, \n               kernel = \"gaussian\"), \n       main = paste(\"2023 Q4 -\", event))\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2023 Insight\n\n\n\n\nOverall in 2023, most of the conflict for the four type continued to occur along the boundary between Magway Division and Mandalay Division (Central Region).\nHowever, we can see a decreasing trend in Strategic Developments and Violence Against Civilians (can see from the scale bar at the side)\nConversely, Battles and Explosions/Remote Violence increased, with these events gradually shifting toward the boundary between Kachin State and Shan State (North-Ease Region)by the end of the year."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#ay2024",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#ay2024",
    "title": "Take-home Exercise 1: Geospatial Analytics for Social Good: Application of Spatial and Spatio-temporal Point Patterns Analysis to discover the geographical distribution of Armed Conflict in Myanmar",
    "section": "3.7 AY2024",
    "text": "3.7 AY2024\n\n3.7.1 Q1\n\n\nClick to expand/collapse code\npar(mfrow = c(2, 2))\npar(mar = c(3, 3, 2, 1))\n# Create a plot for each event type\nfor (event in event_types) {\n  # Filter data by event type\n  filtered_data &lt;- acled_df %&gt;%\n    filter(event_type == event & year == 2024 & quarter == 1)\n  \n  # Convert to ppp format\n  ppp_data &lt;- as.ppp(st_coordinates(filtered_data), st_bbox(filtered_data))\n  \n  # Handle duplicated points\n  ppp_data &lt;- rjitter(ppp_data, retry = TRUE, nsim = 1, drop = TRUE)\n  \n  # Extract conflict Event located within Myanmar\n  ppp_data &lt;- ppp_data[msrb_owin]\n  \n  # Re-scale\n  ppp_data.km &lt;- rescale(ppp_data, 55000, \"km\")\n  \n  # Plot density\n  plot(density(ppp_data.km, \n               sigma = bw.ppl, \n               edge = TRUE, \n               kernel = \"gaussian\"), \n       main = paste(\"2024 Q1 -\", event))\n}\n\n\n\n\n\n\n\n\n\n\n\n3.7.2 Q2\n\n\nClick to expand/collapse code\npar(mfrow = c(2, 2))\npar(mar = c(3, 3, 2, 1))\n# Create a plot for each event type\nfor (event in event_types) {\n  # Filter data by event type\n  filtered_data &lt;- acled_df %&gt;%\n    filter(event_type == event & year == 2024 & quarter == 2)\n  \n  # Convert to ppp format\n  ppp_data &lt;- as.ppp(st_coordinates(filtered_data), st_bbox(filtered_data))\n  \n  # Handle duplicated points\n  ppp_data &lt;- rjitter(ppp_data, retry = TRUE, nsim = 1, drop = TRUE)\n  \n  # Extract conflict Event located within Myanmar\n  ppp_data &lt;- ppp_data[msrb_owin]\n  \n  # Re-scale\n  ppp_data.km &lt;- rescale(ppp_data, 55000, \"km\")\n  \n  # Plot density\n  plot(density(ppp_data.km, \n               sigma = bw.ppl, \n               edge = TRUE, \n               kernel = \"gaussian\"), \n       main = paste(\"2024 Q2 -\", event))\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2024 Insight\n\n\n\n\nIn the first half of 2024, conflict trends remained similar to those of 2023, with the Central region being the primary hotspot, followed by the Southern region. However, there was a significant increase in conflict in Rakhine State during this period.\nIn term of co-relation between event types, Battles and Explosions/Remote Violence remained strongly correlated, while Strategic Developments and Violence Against Civilians also showed a continued correlation."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#kde-limitation",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#kde-limitation",
    "title": "Take-home Exercise 1: Geospatial Analytics for Social Good: Application of Spatial and Spatio-temporal Point Patterns Analysis to discover the geographical distribution of Armed Conflict in Myanmar",
    "section": "3.8 KDE Limitation",
    "text": "3.8 KDE Limitation\n\n\n\n\n\n\nKDE Limitation\n\n\n\n\nSince I am using bw.ppl for sigma, the bandwidth for each event type is adaptive. This creates a challenge when analyzing patterns across different event types and quarters. Sometimes, the KDE may not show many hotspots, even though the number of conflicts in that quarter is higher compared to another KDE with a wider spread of hotspots. ( As I observed that with a higher number on the scale bar, the KDE hotspots are fewer and more concentrated, while the KDE plot with a lower number on the scale bar shows more widespread hotspots.)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#choosing-function-and-number-of-simulations",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#choosing-function-and-number-of-simulations",
    "title": "Take-home Exercise 1: Geospatial Analytics for Social Good: Application of Spatial and Spatio-temporal Point Patterns Analysis to discover the geographical distribution of Armed Conflict in Myanmar",
    "section": "4.1 Choosing Function and Number of Simulations",
    "text": "4.1 Choosing Function and Number of Simulations\nI want to find out that whether the conflict of the different event type that happen quarterly is evenly distributed or clustered. Since to compute for each event quarterly will need a long time to compute. Therefore, I will use G-function for the analysis, as compare to the other three functions, G-function tend to be faster and I will choose 200 for the number of simulations(Note the number of simulation start from 0, so I will use 199 for nsim)."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#section",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#section",
    "title": "Take-home Exercise 1: Geospatial Analytics for Social Good: Application of Spatial and Spatio-temporal Point Patterns Analysis to discover the geographical distribution of Armed Conflict in Myanmar",
    "section": "4.2 2021",
    "text": "4.2 2021\n\n4.2.1 Q1\n\nevent_types &lt;- c(\"Battles\", \"Explosions/Remote violence\", \"Strategic developments\", \"Violence against civilians\")\n\n\n\nClick to expand/collapse code\npar(mfrow = c(2, 2))\n# Create a plot for each event type\nfor (event in event_types) {\n  # Filter data by event type\n  G_conflict &lt;- acled_df %&gt;%\n    filter(event_type == event & year == 2021 & quarter == 1)\n  \n  # Convert to ppp format\n  G_conflict_ppp &lt;- as.ppp(st_coordinates(G_conflict), st_bbox(G_conflict))\n  # Handle duplicated points\n  G_conflict_ppp &lt;- rjitter(G_conflict_ppp, retry = TRUE, nsim = 1, drop = TRUE)\n  # Extract event located within Myanmar\n  G_conflict_ppp = G_conflict_ppp[msrb_owin]\n  # Re-scale\n  G_conflict_ppp.km &lt;- rescale(G_conflict_ppp, 1000, \"km\")\n  # Generate simulations\n  G_conflict.csr &lt;- envelope(G_conflict_ppp.km, Gest, nsim = 199)\n  plot(G_conflict.csr, main = paste(event))\n}\n\n\n\n\n\n4.2.2 Q2\n\n\nClick to expand/collapse code\npar(mfrow = c(2, 2))\n# Create a plot for each event type\nfor (event in event_types) {\n  # Filter data by event type\n  G_conflict &lt;- acled_df %&gt;%\n    filter(event_type == event & year == 2021 & quarter == 2)\n  \n  # Convert to ppp format\n  G_conflict_ppp &lt;- as.ppp(st_coordinates(G_conflict), st_bbox(G_conflict))\n  # Handle duplicated points\n  G_conflict_ppp &lt;- rjitter(G_conflict_ppp, retry = TRUE, nsim = 1, drop = TRUE)\n  # Extract event located within Myanmar\n  G_conflict_ppp = G_conflict_ppp[msrb_owin]\n  # Re-scale\n  G_conflict_ppp.km &lt;- rescale(G_conflict_ppp, 1000, \"km\")\n  # Generate simulations\n  G_conflict.csr &lt;- envelope(G_conflict_ppp.km, Gest, nsim = 199)\n  plot(G_conflict.csr, main = paste(event))\n}\n\n\n\n\n\n4.2.3 Q3\n\n\nClick to expand/collapse code\npar(mfrow = c(2, 2))\n# Create a plot for each event type\nfor (event in event_types) {\n  # Filter data by event type\n  G_conflict &lt;- acled_df %&gt;%\n    filter(event_type == event & year == 2021 & quarter == 3)\n  \n  # Convert to ppp format\n  G_conflict_ppp &lt;- as.ppp(st_coordinates(G_conflict), st_bbox(G_conflict))\n  # Handle duplicated points\n  G_conflict_ppp &lt;- rjitter(G_conflict_ppp, retry = TRUE, nsim = 1, drop = TRUE)\n  # Extract event located within Myanmar\n  G_conflict_ppp = G_conflict_ppp[msrb_owin]\n  # Re-scale\n  G_conflict_ppp.km &lt;- rescale(G_conflict_ppp, 1000, \"km\")\n  # Generate simulations\n  G_conflict.csr &lt;- envelope(G_conflict_ppp.km, Gest, nsim = 199)\n  plot(G_conflict.csr, main = paste(event))\n}\n\n\n\n\n\n4.2.4 Q4\n\n\nClick to expand/collapse code\npar(mfrow = c(2, 2))\n\n# Create a plot for each event type\nfor (event in event_types) {\n  # Filter data by event type\n  G_conflict &lt;- acled_df %&gt;%\n    filter(event_type == event & year == 2021 & quarter == 4)\n  \n  # Convert to ppp format\n  G_conflict_ppp &lt;- as.ppp(st_coordinates(G_conflict), st_bbox(G_conflict))\n  # Handle duplicated points\n  G_conflict_ppp &lt;- rjitter(G_conflict_ppp, retry = TRUE, nsim = 1, drop = TRUE)\n  # Extract event located within Myanmar\n  G_conflict_ppp = G_conflict_ppp[msrb_owin]\n  # Re-scale\n  G_conflict_ppp.km &lt;- rescale(G_conflict_ppp, 1000, \"km\")\n  # Generate simulations\n  G_conflict.csr &lt;- envelope(G_conflict_ppp.km, Gest, nsim = 199)\n  plot(G_conflict.csr, main = paste(event))\n}"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#section-1",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#section-1",
    "title": "Take-home Exercise 1: Geospatial Analytics for Social Good: Application of Spatial and Spatio-temporal Point Patterns Analysis to discover the geographical distribution of Armed Conflict in Myanmar",
    "section": "4.3 2022",
    "text": "4.3 2022\n\n4.3.1 Q1\n\n\nClick to expand/collapse code\npar(mfrow = c(2, 2))\n\n# Create a plot for each event type\nfor (event in event_types) {\n  # Filter data by event type\n  G_conflict &lt;- acled_df %&gt;%\n    filter(event_type == event & year == 2022 & quarter == 1)\n  \n  # Convert to ppp format\n  G_conflict_ppp &lt;- as.ppp(st_coordinates(G_conflict), st_bbox(G_conflict))\n  # Handle duplicated points\n  G_conflict_ppp &lt;- rjitter(G_conflict_ppp, retry = TRUE, nsim = 1, drop = TRUE)\n  # Extract event located within Myanmar\n  G_conflict_ppp = G_conflict_ppp[msrb_owin]\n  # Re-scale\n  G_conflict_ppp.km &lt;- rescale(G_conflict_ppp, 1000, \"km\")\n  # Generate simulations\n  G_conflict.csr &lt;- envelope(G_conflict_ppp.km, Gest, nsim = 199)\n  plot(G_conflict.csr, main = paste(event))\n}\n\n\n\n\n\n4.3.2 Q2\n\n\nClick to expand/collapse code\npar(mfrow = c(2, 2))\n\n# Create a plot for each event type\nfor (event in event_types) {\n  # Filter data by event type\n  G_conflict &lt;- acled_df %&gt;%\n    filter(event_type == event & year == 2022 & quarter == 2)\n  \n  # Convert to ppp format\n  G_conflict_ppp &lt;- as.ppp(st_coordinates(G_conflict), st_bbox(G_conflict))\n  # Handle duplicated points\n  G_conflict_ppp &lt;- rjitter(G_conflict_ppp, retry = TRUE, nsim = 1, drop = TRUE)\n  # Extract event located within Myanmar\n  G_conflict_ppp = G_conflict_ppp[msrb_owin]\n  # Re-scale\n  G_conflict_ppp.km &lt;- rescale(G_conflict_ppp, 1000, \"km\")\n  # Generate simulations\n  G_conflict.csr &lt;- envelope(G_conflict_ppp.km, Gest, nsim = 199)\n  plot(G_conflict.csr, main = paste(event))\n}\n\n\n\n\n\n4.3.3 Q3\n\n\nClick to expand/collapse code\npar(mfrow = c(2, 2))\n\n# Create a plot for each event type\nfor (event in event_types) {\n  # Filter data by event type\n  G_conflict &lt;- acled_df %&gt;%\n    filter(event_type == event & year == 2022 & quarter == 3)\n  \n  # Convert to ppp format\n  G_conflict_ppp &lt;- as.ppp(st_coordinates(G_conflict), st_bbox(G_conflict))\n  # Handle duplicated points\n  G_conflict_ppp &lt;- rjitter(G_conflict_ppp, retry = TRUE, nsim = 1, drop = TRUE)\n  # Extract event located within Myanmar\n  G_conflict_ppp = G_conflict_ppp[msrb_owin]\n  # Re-scale\n  G_conflict_ppp.km &lt;- rescale(G_conflict_ppp, 1000, \"km\")\n  # Generate simulations\n  G_conflict.csr &lt;- envelope(G_conflict_ppp.km, Gest, nsim = 199)\n  plot(G_conflict.csr, main = paste(event))\n}\n\n\n\n\n\n4.3.4 Q4\n\n\nClick to expand/collapse code\npar(mfrow = c(2, 2))\n\n# Create a plot for each event type\nfor (event in event_types) {\n  # Filter data by event type\n  G_conflict &lt;- acled_df %&gt;%\n    filter(event_type == event & year == 2022 & quarter == 4)\n  \n  # Convert to ppp format\n  G_conflict_ppp &lt;- as.ppp(st_coordinates(G_conflict), st_bbox(G_conflict))\n  # Handle duplicated points\n  G_conflict_ppp &lt;- rjitter(G_conflict_ppp, retry = TRUE, nsim = 1, drop = TRUE)\n  # Extract event located within Myanmar\n  G_conflict_ppp = G_conflict_ppp[msrb_owin]\n  # Re-scale\n  G_conflict_ppp.km &lt;- rescale(G_conflict_ppp, 1000, \"km\")\n  # Generate simulations\n  G_conflict.csr &lt;- envelope(G_conflict_ppp.km, Gest, nsim = 199)\n  plot(G_conflict.csr, main = paste(event))\n}"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#section-2",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#section-2",
    "title": "Take-home Exercise 1: Geospatial Analytics for Social Good: Application of Spatial and Spatio-temporal Point Patterns Analysis to discover the geographical distribution of Armed Conflict in Myanmar",
    "section": "4.4 2023",
    "text": "4.4 2023\n\n4.4.1 Q1\n\n\nClick to expand/collapse code\npar(mfrow = c(2, 2))\n\n# Create a plot for each event type\nfor (event in event_types) {\n  # Filter data by event type\n  G_conflict &lt;- acled_df %&gt;%\n    filter(event_type == event & year == 2023 & quarter == 1)\n  \n  # Convert to ppp format\n  G_conflict_ppp &lt;- as.ppp(st_coordinates(G_conflict), st_bbox(G_conflict))\n  # Handle duplicated points\n  G_conflict_ppp &lt;- rjitter(G_conflict_ppp, retry = TRUE, nsim = 1, drop = TRUE)\n  # Extract event located within Myanmar\n  G_conflict_ppp = G_conflict_ppp[msrb_owin]\n  # Re-scale\n  G_conflict_ppp.km &lt;- rescale(G_conflict_ppp, 1000, \"km\")\n  # Generate simulations\n  G_conflict.csr &lt;- envelope(G_conflict_ppp.km, Gest, nsim = 199)\n  plot(G_conflict.csr, main = paste(event))\n}\n\n\n\n\n\n4.4.2 Q2\n\n\nClick to expand/collapse code\npar(mfrow = c(2, 2))\n\n# Create a plot for each event type\nfor (event in event_types) {\n  # Filter data by event type\n  G_conflict &lt;- acled_df %&gt;%\n    filter(event_type == event & year == 2023 & quarter == 2)\n  \n  # Convert to ppp format\n  G_conflict_ppp &lt;- as.ppp(st_coordinates(G_conflict), st_bbox(G_conflict))\n  # Handle duplicated points\n  G_conflict_ppp &lt;- rjitter(G_conflict_ppp, retry = TRUE, nsim = 1, drop = TRUE)\n  # Extract event located within Myanmar\n  G_conflict_ppp = G_conflict_ppp[msrb_owin]\n  # Re-scale\n  G_conflict_ppp.km &lt;- rescale(G_conflict_ppp, 1000, \"km\")\n  # Generate simulations\n  G_conflict.csr &lt;- envelope(G_conflict_ppp.km, Gest, nsim = 199)\n  plot(G_conflict.csr, main = paste(event))\n}\n\n\n\n\n\n4.4.3 Q3\n\n\nClick to expand/collapse code\npar(mfrow = c(2, 2))\n\n# Create a plot for each event type\nfor (event in event_types) {\n  # Filter data by event type\n  G_conflict &lt;- acled_df %&gt;%\n    filter(event_type == event & year == 2023 & quarter == 3)\n  \n  # Convert to ppp format\n  G_conflict_ppp &lt;- as.ppp(st_coordinates(G_conflict), st_bbox(G_conflict))\n  # Handle duplicated points\n  G_conflict_ppp &lt;- rjitter(G_conflict_ppp, retry = TRUE, nsim = 1, drop = TRUE)\n  # Extract event located within Myanmar\n  G_conflict_ppp = G_conflict_ppp[msrb_owin]\n  # Re-scale\n  G_conflict_ppp.km &lt;- rescale(G_conflict_ppp, 1000, \"km\")\n  # Generate simulations\n  G_conflict.csr &lt;- envelope(G_conflict_ppp.km, Gest, nsim = 199)\n  plot(G_conflict.csr, main = paste(event))\n}\n\n\n\n\n\n4.4.4 Q4\n\n\nClick to expand/collapse code\npar(mfrow = c(2, 2))\n\n# Create a plot for each event type\nfor (event in event_types) {\n  # Filter data by event type\n  G_conflict &lt;- acled_df %&gt;%\n    filter(event_type == event & year == 2023 & quarter == 4)\n  \n  # Convert to ppp format\n  G_conflict_ppp &lt;- as.ppp(st_coordinates(G_conflict), st_bbox(G_conflict))\n  # Handle duplicated points\n  G_conflict_ppp &lt;- rjitter(G_conflict_ppp, retry = TRUE, nsim = 1, drop = TRUE)\n  # Extract event located within Myanmar\n  G_conflict_ppp = G_conflict_ppp[msrb_owin]\n  # Re-scale\n  G_conflict_ppp.km &lt;- rescale(G_conflict_ppp, 1000, \"km\")\n  # Generate simulations\n  G_conflict.csr &lt;- envelope(G_conflict_ppp.km, Gest, nsim = 199)\n  plot(G_conflict.csr, main = paste(event))\n}"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#section-3",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#section-3",
    "title": "Take-home Exercise 1: Geospatial Analytics for Social Good: Application of Spatial and Spatio-temporal Point Patterns Analysis to discover the geographical distribution of Armed Conflict in Myanmar",
    "section": "4.5 2024",
    "text": "4.5 2024\n\n4.5.1 Q1\n\n\nClick to expand/collapse code\npar(mfrow = c(2, 2))\n\n# Create a plot for each event type\nfor (event in event_types) {\n  # Filter data by event type\n  G_conflict &lt;- acled_df %&gt;%\n    filter(event_type == event & year == 2024 & quarter == 1)\n  \n  # Convert to ppp format\n  G_conflict_ppp &lt;- as.ppp(st_coordinates(G_conflict), st_bbox(G_conflict))\n  # Handle duplicated points\n  G_conflict_ppp &lt;- rjitter(G_conflict_ppp, retry = TRUE, nsim = 1, drop = TRUE)\n  # Extract event located within Myanmar\n  G_conflict_ppp = G_conflict_ppp[msrb_owin]\n  # Re-scale\n  G_conflict_ppp.km &lt;- rescale(G_conflict_ppp, 1000, \"km\")\n  # Generate simulations\n  G_conflict.csr &lt;- envelope(G_conflict_ppp.km, Gest, nsim = 199)\n  plot(G_conflict.csr, main = paste(event))\n}\n\n\n\n\n\n4.5.2 Q2\n\n\nClick to expand/collapse code\npar(mfrow = c(2, 2))\n\n# Create a plot for each event type\nfor (event in event_types) {\n  # Filter data by event type\n  G_conflict &lt;- acled_df %&gt;%\n    filter(event_type == event & year == 2024 & quarter == 2)\n  \n  # Convert to ppp format\n  G_conflict_ppp &lt;- as.ppp(st_coordinates(G_conflict), st_bbox(G_conflict))\n  # Handle duplicated points\n  G_conflict_ppp &lt;- rjitter(G_conflict_ppp, retry = TRUE, nsim = 1, drop = TRUE)\n  # Extract event located within Myanmar\n  G_conflict_ppp = G_conflict_ppp[msrb_owin]\n  # Re-scale\n  G_conflict_ppp.km &lt;- rescale(G_conflict_ppp, 1000, \"km\")\n  # Generate simulations\n  G_conflict.csr &lt;- envelope(G_conflict_ppp.km, Gest, nsim = 199)\n  plot(G_conflict.csr, main = paste(event))\n}"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#insight-4",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#insight-4",
    "title": "Take-home Exercise 1: Geospatial Analytics for Social Good: Application of Spatial and Spatio-temporal Point Patterns Analysis to discover the geographical distribution of Armed Conflict in Myanmar",
    "section": "4.6 Insight",
    "text": "4.6 Insight\n\n\n\n\n\n\n2nd Order Spatial Point Pattern Analysis Insight\n\n\n\n\nFrom the quarterly plots, the observed line in most cases lies above the envelope, indicating clustering in the conflict locations. An exception is seen in 2021 Q1, where the plot for violence against civilians shows the observed line entering the envelope toward the end.\nOverall, this confirms that the spatial patterns for all four event types exhibit clustering over the years."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#further-into-the-hot-conflict-region",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#further-into-the-hot-conflict-region",
    "title": "Take-home Exercise 1: Geospatial Analytics for Social Good: Application of Spatial and Spatio-temporal Point Patterns Analysis to discover the geographical distribution of Armed Conflict in Myanmar",
    "section": "4.7 Further into the Hot Conflict Region",
    "text": "4.7 Further into the Hot Conflict Region\nAs observed in the G function graph for the whole of Myanmar, the plots are quite similar, providing limited insight into the conflict patterns. Therefore, based on the KDE generated in Section 3, I have chosen to focus on the central region, where most of the conflicts occur, specifically in Magway State, Mandalay State, and Sagaing State. To check whether the G-function graph within the 3 state will be able to provide any useful insight of the patterns.\nTo read the RDS file.\n\nmsrb_sub_reg &lt;- read_rds(\"data/rds/msrb_sub_reg.rds\")\nacled_df &lt;- read_rds(\"data/rds/acled_df.rds\")\n\nTo filter out the three state.\n\nselected_states &lt;- msrb_sub_reg %&gt;%\n  filter(state_name %in% c(\"Magway\", \"Mandalay\", \"Sagaing\"))\n\n\nconflcit_within_states &lt;- st_intersection(acled_df, selected_states)\n\nTo write and read the data in RDS format.\n\n write_rds(conflcit_within_states,\"data/rds/conflcit_within_states.rds\")\n\n\nconflcit_within_states &lt;- read_rds(\"data/rds/conflcit_within_states.rds\")\n\n\nnrow(acled_df)\n\n[1] 51553\n\nnrow(conflcit_within_states)\n\n[1] 23675\n\n\nWe can see that nearly 50% of the conflicts occurred within the three states over the period.\nLets to have a quite look of the plot.\n\n\nClick to expand/collapse code\ntm1 &lt;- tm_shape(selected_states) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\n  tm_text(\"state_name\", size = 1, col = \"blue\")\n\ntm2 &lt;- tm_shape(selected_states) +\n  tm_borders(alpha = 1, col = \"red\") +\n  tm_polygons() +\n  tm_shape(conflcit_within_states)+\n  tm_dots()\n\ntmap_arrange(tm1, tm2, ncol = 2)\n\n\n\n\n\n\n\n\n\nThe below chunk is to create owin object of the selected states.\n\nselected_states_owin &lt;- as.owin(selected_states)\n\nAs when I plot out the sample G-function plot, the observed line and the theoretical line are very far. Therefore I will use a low nsim (number of simulations) to speed up the computation and examine the pattern for each quarter.\n\nBattlesExplosions/Remote violenceStrategic developmentsViolence against civilians\n\n\n\n\nClick to expand/collapse code\npar(mfrow = c(2, 2))\n\nyears &lt;- 2021:2024\nquarters &lt;- c(1, 2, 3, 4) \nquarters_2024 &lt;- c(1, 2) \nfor (year in years) {\n  # Determine the quarters to loop through for each year\n  if (year == 2024) {\n    quarters_to_use &lt;- quarters_2024\n  } else {\n    quarters_to_use &lt;- quarters\n  }\n  \n  # Loop through each quarter\n  for (quarter in quarters_to_use) {\n    # Filter data based on the current year and quarter\n    filtered_data &lt;- conflcit_within_states %&gt;%\n      filter(event_type == \"Battles\" & year == year & quarter == quarter)\n    \n    # Convert to ppp format\n    G_conflict_ppp &lt;- as.ppp(st_coordinates(filtered_data), st_bbox(filtered_data))\n    # Handle duplicated points\n    G_conflict_ppp &lt;- rjitter(G_conflict_ppp, retry = TRUE, nsim = 1, drop = TRUE)\n    # Extract event located within the selected state\n    G_conflict_ppp = G_conflict_ppp[selected_states_owin]\n    # Re-scale\n    G_conflict_ppp.km &lt;- rescale(G_conflict_ppp, 1000, \"km\")\n    # Generate simulations\n    G_conflict.csr &lt;- envelope(G_conflict_ppp.km, Gest, nsim = 3)\n    plot(G_conflict.csr, main = paste(\"Battles G function - \", year, \" Q\", quarter))\n  }\n}\n\n\nGenerating 3 simulations of CSR  ...\n1, 2, \n3.\n\nDone.\n\n\nGenerating 3 simulations of CSR  ...\n1, 2, \n3.\n\nDone.\n\n\nGenerating 3 simulations of CSR  ...\n1, 2, \n3.\n\nDone.\n\n\nGenerating 3 simulations of CSR  ...\n1, 2, \n3.\n\nDone.\n\n\n\n\n\n\n\n\n\nGenerating 3 simulations of CSR  ...\n1, 2, \n3.\n\nDone.\n\n\nGenerating 3 simulations of CSR  ...\n1, 2, \n3.\n\nDone.\n\n\nGenerating 3 simulations of CSR  ...\n1, 2, \n3.\n\nDone.\n\n\nGenerating 3 simulations of CSR  ...\n1, 2, \n3.\n\nDone.\n\n\n\n\n\n\n\n\n\nGenerating 3 simulations of CSR  ...\n1, 2, \n3.\n\nDone.\n\n\nGenerating 3 simulations of CSR  ...\n1, 2, \n3.\n\nDone.\n\n\nGenerating 3 simulations of CSR  ...\n1, 2, \n3.\n\nDone.\n\n\nGenerating 3 simulations of CSR  ...\n1, 2, \n3.\n\nDone.\n\n\n\n\n\n\n\n\n\nGenerating 3 simulations of CSR  ...\n1, 2, \n3.\n\nDone.\n\n\nGenerating 3 simulations of CSR  ...\n1, 2, \n3.\n\nDone.\n\n\n\n\n\n\n\n\n\n\n\n\n\nClick to expand/collapse code\npar(mfrow = c(2, 2))\n\nyears &lt;- 2021:2024\nquarters &lt;- c(1, 2, 3, 4) \nquarters_2024 &lt;- c(1, 2) \nfor (year in years) {\n  # Determine the quarters to loop through for each year\n  if (year == 2024) {\n    quarters_to_use &lt;- quarters_2024\n  } else {\n    quarters_to_use &lt;- quarters\n  }\n  \n  # Loop through each quarter\n  for (quarter in quarters_to_use) {\n    # Filter data based on the current year and quarter\n    filtered_data &lt;- conflcit_within_states %&gt;%\n      filter(event_type == \"Explosions/Remote violence\" & year == year & quarter == quarter)\n    \n    # Convert to ppp format\n    G_conflict_ppp &lt;- as.ppp(st_coordinates(filtered_data), st_bbox(filtered_data))\n    # Handle duplicated points\n    G_conflict_ppp &lt;- rjitter(G_conflict_ppp, retry = TRUE, nsim = 1, drop = TRUE)\n    # Extract event located within the selected state\n    G_conflict_ppp = G_conflict_ppp[selected_states_owin]\n    # Re-scale\n    G_conflict_ppp.km &lt;- rescale(G_conflict_ppp, 1000, \"km\")\n    # Generate simulations\n    G_conflict.csr &lt;- envelope(G_conflict_ppp.km, Gest, nsim = 3)\n    plot(G_conflict.csr, main = paste(\"Explosions/Remote violence G function - \", year, \" Q\", quarter))\n  }\n}\n\n\nGenerating 3 simulations of CSR  ...\n1, 2, \n3.\n\nDone.\n\n\nGenerating 3 simulations of CSR  ...\n1, 2, \n3.\n\nDone.\n\n\nGenerating 3 simulations of CSR  ...\n1, 2, \n3.\n\nDone.\n\n\nGenerating 3 simulations of CSR  ...\n1, 2, \n3.\n\nDone.\n\n\n\n\n\n\n\n\n\nGenerating 3 simulations of CSR  ...\n1, 2, \n3.\n\nDone.\n\n\nGenerating 3 simulations of CSR  ...\n1, 2, \n3.\n\nDone.\n\n\nGenerating 3 simulations of CSR  ...\n1, 2, \n3.\n\nDone.\n\n\nGenerating 3 simulations of CSR  ...\n1, 2, \n3.\n\nDone.\n\n\n\n\n\n\n\n\n\nGenerating 3 simulations of CSR  ...\n1, 2, \n3.\n\nDone.\n\n\nGenerating 3 simulations of CSR  ...\n1, 2, \n3.\n\nDone.\n\n\nGenerating 3 simulations of CSR  ...\n1, 2, \n3.\n\nDone.\n\n\nGenerating 3 simulations of CSR  ...\n1, 2, \n3.\n\nDone.\n\n\n\n\n\n\n\n\n\nGenerating 3 simulations of CSR  ...\n1, 2, \n3.\n\nDone.\n\n\nGenerating 3 simulations of CSR  ...\n1, 2, \n3.\n\nDone.\n\n\n\n\n\n\n\n\n\n\n\n\n\nClick to expand/collapse code\npar(mfrow = c(2, 2))\n\nyears &lt;- 2021:2024\nquarters &lt;- c(1, 2, 3, 4) \nquarters_2024 &lt;- c(1, 2) \nfor (year in years) {\n  # Determine the quarters to loop through for each year\n  if (year == 2024) {\n    quarters_to_use &lt;- quarters_2024\n  } else {\n    quarters_to_use &lt;- quarters\n  }\n  \n  # Loop through each quarter\n  for (quarter in quarters_to_use) {\n    # Filter data based on the current year and quarter\n    filtered_data &lt;- conflcit_within_states %&gt;%\n      filter(event_type == \"Strategic developments\" & year == year & quarter == quarter)\n    \n    # Convert to ppp format\n    G_conflict_ppp &lt;- as.ppp(st_coordinates(filtered_data), st_bbox(filtered_data))\n    # Handle duplicated points\n    G_conflict_ppp &lt;- rjitter(G_conflict_ppp, retry = TRUE, nsim = 1, drop = TRUE)\n    # Extract event located within the selected state\n    G_conflict_ppp = G_conflict_ppp[selected_states_owin]\n    # Re-scale\n    G_conflict_ppp.km &lt;- rescale(G_conflict_ppp, 1000, \"km\")\n    # Generate simulations\n    G_conflict.csr &lt;- envelope(G_conflict_ppp.km, Gest, nsim = 3)\n    plot(G_conflict.csr, main = paste(\"Strategic developments G function - \", year, \" Q\", quarter))\n  }\n}\n\n\nGenerating 3 simulations of CSR  ...\n1, 2, \n3.\n\nDone.\n\n\nGenerating 3 simulations of CSR  ...\n1, 2, \n3.\n\nDone.\n\n\nGenerating 3 simulations of CSR  ...\n1, 2, \n3.\n\nDone.\n\n\nGenerating 3 simulations of CSR  ...\n1, 2, \n3.\n\nDone.\n\n\n\n\n\n\n\n\n\nGenerating 3 simulations of CSR  ...\n1, 2, \n3.\n\nDone.\n\n\nGenerating 3 simulations of CSR  ...\n1, 2, \n3.\n\nDone.\n\n\nGenerating 3 simulations of CSR  ...\n1, 2, \n3.\n\nDone.\n\n\nGenerating 3 simulations of CSR  ...\n1, 2, \n3.\n\nDone.\n\n\n\n\n\n\n\n\n\nGenerating 3 simulations of CSR  ...\n1, 2, \n3.\n\nDone.\n\n\nGenerating 3 simulations of CSR  ...\n1, 2, \n3.\n\nDone.\n\n\nGenerating 3 simulations of CSR  ...\n1, 2, \n3.\n\nDone.\n\n\nGenerating 3 simulations of CSR  ...\n1, 2, \n3.\n\nDone.\n\n\n\n\n\n\n\n\n\nGenerating 3 simulations of CSR  ...\n1, 2, \n3.\n\nDone.\n\n\nGenerating 3 simulations of CSR  ...\n1, 2, \n3.\n\nDone.\n\n\n\n\n\n\n\n\n\n\n\n\n\nClick to expand/collapse code\npar(mfrow = c(2, 2))\n\nyears &lt;- 2021:2024\nquarters &lt;- c(1, 2, 3, 4) \nquarters_2024 &lt;- c(1, 2) \nfor (year in years) {\n  # Determine the quarters to loop through for each year\n  if (year == 2024) {\n    quarters_to_use &lt;- quarters_2024\n  } else {\n    quarters_to_use &lt;- quarters\n  }\n  \n  # Loop through each quarter\n  for (quarter in quarters_to_use) {\n    # Filter data based on the current year and quarter\n    filtered_data &lt;- conflcit_within_states %&gt;%\n      filter(event_type == \"Violence against civilians\" & year == year & quarter == quarter)\n    \n    # Convert to ppp format\n    G_conflict_ppp &lt;- as.ppp(st_coordinates(filtered_data), st_bbox(filtered_data))\n    # Handle duplicated points\n    G_conflict_ppp &lt;- rjitter(G_conflict_ppp, retry = TRUE, nsim = 1, drop = TRUE)\n    # Extract event located within the selected state\n    G_conflict_ppp = G_conflict_ppp[selected_states_owin]\n    # Re-scale\n    G_conflict_ppp.km &lt;- rescale(G_conflict_ppp, 1000, \"km\")\n    # Generate simulations\n    G_conflict.csr &lt;- envelope(G_conflict_ppp.km, Gest, nsim = 3)\n    plot(G_conflict.csr, main = paste(\"Violence against civilians G function - \", year, \" Q\", quarter))\n  }\n}\n\n\nGenerating 3 simulations of CSR  ...\n1, 2, \n3.\n\nDone.\n\n\nGenerating 3 simulations of CSR  ...\n1, 2, \n3.\n\nDone.\n\n\nGenerating 3 simulations of CSR  ...\n1, 2, \n3.\n\nDone.\n\n\nGenerating 3 simulations of CSR  ...\n1, 2, \n3.\n\nDone.\n\n\n\n\n\n\n\n\n\nGenerating 3 simulations of CSR  ...\n1, 2, \n3.\n\nDone.\n\n\nGenerating 3 simulations of CSR  ...\n1, 2, \n3.\n\nDone.\n\n\nGenerating 3 simulations of CSR  ...\n1, 2, \n3.\n\nDone.\n\n\nGenerating 3 simulations of CSR  ...\n1, 2, \n3.\n\nDone.\n\n\n\n\n\n\n\n\n\nGenerating 3 simulations of CSR  ...\n1, 2, \n3.\n\nDone.\n\n\nGenerating 3 simulations of CSR  ...\n1, 2, \n3.\n\nDone.\n\n\nGenerating 3 simulations of CSR  ...\n1, 2, \n3.\n\nDone.\n\n\nGenerating 3 simulations of CSR  ...\n1, 2, \n3.\n\nDone.\n\n\n\n\n\n\n\n\n\nGenerating 3 simulations of CSR  ...\n1, 2, \n3.\n\nDone.\n\n\nGenerating 3 simulations of CSR  ...\n1, 2, \n3.\n\nDone.\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.7.1 Insight\n\n\n\n\n\n\nSelected State Insight\n\n\n\n\nDespite analyzing the G-function plots for each event type on a quarterly basis within the selected states, the resulting plots resemble those of the entire Myanmar. This suggests that the spatial distribution of conflicts remains consistent across different quarters."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#prepare-the-data",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#prepare-the-data",
    "title": "Take-home Exercise 1: Geospatial Analytics for Social Good: Application of Spatial and Spatio-temporal Point Patterns Analysis to discover the geographical distribution of Armed Conflict in Myanmar",
    "section": "5.1 Prepare the Data",
    "text": "5.1 Prepare the Data\nTo read acled_df from RDS.\n\nacled_df &lt;- read_rds( \"data/rds/acled_df.rds\")\n\nTo only select the necessary fields to save memory and create a year_quarter field ,make it as factors for later to plot the graph.\n\n\n\n\n\n\nNote\n\n\n\nyear_quarter need to be in numeric in order to compute STDKE.\n\n2021 Q1 =&gt; 2021.00\n2021 Q2 =&gt; 2021.25\n2021 Q3 =&gt; 2021.50\n2021 Q4 =&gt; 2021.75\n\n\n\n\nacled_df &lt;- acled_df %&gt;%\n  mutate(year_quarter = round(year + (quarter - 1) / 4, 2) )\n\nTo filter the data based on event_type.\n\nbattles_df &lt;- acled_df %&gt;%\n    filter(event_type == \"Battles\")\n\nexplosions_df &lt;- acled_df %&gt;%\n    filter(event_type == \"Explosions/Remote violence\")\n\nstrategic_df &lt;- acled_df %&gt;%\n    filter(event_type == \"Strategic developments\") \n\ncivilians_df &lt;- acled_df %&gt;%\n    filter(event_type == \"Violence against civilians\") \n\nSave the Data into rds.\n\nwrite_rds(acled_df, \"data/rds/acled_df.rds\") # to update the RDS file\nwrite_rds(battles_df, \"data/rds/battles_df.rds\")\nwrite_rds(explosions_df, \"data/rds/explosions_df.rds\")\nwrite_rds(strategic_df, \"data/rds/strategic_df.rds\")\nwrite_rds(civilians_df, \"data/rds/civilians_df.rds\")\n\n\nfiltered_msrb &lt;- read_rds(\"data/rds/filtered_msrb.rds\")\nbattles_df &lt;- read_rds(\"data/rds/battles_df.rds\")\nexplosions_df &lt;- read_rds(\"data/rds/explosions_df.rds\")\nstrategic_df &lt;- read_rds(\"data/rds/strategic_df.rds\")\ncivilians_df &lt;- read_rds(\"data/rds/civilians_df.rds\")"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#overview-1",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#overview-1",
    "title": "Take-home Exercise 1: Geospatial Analytics for Social Good: Application of Spatial and Spatio-temporal Point Patterns Analysis to discover the geographical distribution of Armed Conflict in Myanmar",
    "section": "5.2 Overview",
    "text": "5.2 Overview\nUsing tmap packages to plot over 2021 to 2024.\n\nBattlesExplosions/Remote violenceStrategic developmentsViolence against civilians\n\n\n\ntm_shape(filtered_msrb) +\n  tm_polygons() +\n  tm_shape(battles_df) +\n  tm_dots()\n\n\n\n\n\n\n\n\n\n\n\ntm_shape(filtered_msrb) +\n  tm_polygons() +\n  tm_shape(explosions_df) +\n  tm_dots()\n\n\n\n\n\n\n\n\n\n\n\ntm_shape(filtered_msrb) +\n  tm_polygons() +\n  tm_shape(strategic_df) +\n  tm_dots()\n\n\n\n\n\n\n\n\n\n\n\ntm_shape(filtered_msrb) +\n  tm_polygons() +\n  tm_shape(civilians_df) +\n  tm_dots()"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#visualizing-distribution-of-conflict-by-quarter",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#visualizing-distribution-of-conflict-by-quarter",
    "title": "Take-home Exercise 1: Geospatial Analytics for Social Good: Application of Spatial and Spatio-temporal Point Patterns Analysis to discover the geographical distribution of Armed Conflict in Myanmar",
    "section": "5.3 Visualizing Distribution of Conflict by Quarter",
    "text": "5.3 Visualizing Distribution of Conflict by Quarter\n\nBattlesExplosions/Remote violenceStrategic developmentsViolence against civilians\n\n\n\n\nClick to expand/collapse code\ntm_shape(filtered_msrb) +\n  tm_polygons() +\n  tm_shape(battles_df) +\n  tm_dots(size = 0.1) +\n  tm_facets(by=\"year_quarter\",\n            free.coords = FALSE, # To avoid the zoom issue of the display map\n            drop.units = TRUE,\n            nrow = 4)\n\n\n\n\n\n\n\n\n\n\n\n\n\nClick to expand/collapse code\ntm_shape(filtered_msrb) +\n  tm_polygons() +\n  tm_shape(explosions_df) +\n  tm_dots(size = 0.1) +\n  tm_facets(by=\"year_quarter\",\n            free.coords = FALSE, # To avoid the zoom issue of the display map\n            drop.units = TRUE,\n            nrow = 4)\n\n\n\n\n\n\n\n\n\n\n\n\n\nClick to expand/collapse code\ntm_shape(filtered_msrb) +\n  tm_polygons() +\n  tm_shape(strategic_df) +\n  tm_dots(size = 0.1) +\n  tm_facets(by=\"year_quarter\",\n            free.coords = FALSE, # To avoid the zoom issue of the display map\n            drop.units = TRUE,\n            nrow = 4)\n\n\n\n\n\n\n\n\n\n\n\n\n\nClick to expand/collapse code\ntm_shape(filtered_msrb) +\n  tm_polygons() +\n  tm_shape(civilians_df) +\n  tm_dots(size = 0.1) +\n  tm_facets(by=\"year_quarter\",\n            free.coords = FALSE, # To avoid the zoom issue of the display map\n            drop.units = TRUE,\n            nrow = 4)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#computing-stdke-by-quarter",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#computing-stdke-by-quarter",
    "title": "Take-home Exercise 1: Geospatial Analytics for Social Good: Application of Spatial and Spatio-temporal Point Patterns Analysis to discover the geographical distribution of Armed Conflict in Myanmar",
    "section": "5.3 Computing STDKE by Quarter",
    "text": "5.3 Computing STDKE by Quarter\n\n5.3.1 Creating ppp object.\nRemove unwanted fields. As ppp object only need mark field and geometry field from the data frame.\n\nbattles_quarter &lt;- battles_df %&gt;%\n  dplyr::select(year_quarter)\n\nexplosions_quarter &lt;- explosions_df %&gt;%\n  dplyr::select(year_quarter)\n\nstrategic_quarter &lt;- strategic_df %&gt;%\n  dplyr::select(year_quarter)\n\ncivilians_quarter &lt;- civilians_df %&gt;%\n  dplyr::select(year_quarter)\n\n\nbattles_quarter_ppp &lt;- as.ppp(battles_quarter)\nexplosions_quarter_ppp &lt;- as.ppp(explosions_quarter)\nstrategic_quarter_ppp &lt;- as.ppp(strategic_quarter)\ncivilians_quarter_ppp &lt;- as.ppp(civilians_quarter)\n\nCheck if there is duplicated point.\n\nany(duplicated(battles_quarter_ppp))\n\n[1] TRUE\n\nany(duplicated(explosions_quarter_ppp))\n\n[1] TRUE\n\nany(duplicated(strategic_quarter_ppp))\n\n[1] TRUE\n\nany(duplicated(civilians_quarter_ppp))\n\n[1] TRUE\n\n\nHandle duplicated points.\n\nbattles_quarter_ppp &lt;- rjitter(battles_quarter_ppp, retry = TRUE, nsim = 1, drop = TRUE)\nexplosions_quarter_ppp &lt;- rjitter(explosions_quarter_ppp, retry = TRUE, nsim = 1, drop = TRUE)\nstrategic_quarter_ppp &lt;- rjitter(strategic_quarter_ppp, retry = TRUE, nsim = 1, drop = TRUE)\ncivilians_quarter_ppp &lt;- rjitter(civilians_quarter_ppp, retry = TRUE, nsim = 1, drop = TRUE)\n\n\nany(duplicated(battles_quarter_ppp))\n\n[1] FALSE\n\nany(duplicated(explosions_quarter_ppp))\n\n[1] FALSE\n\nany(duplicated(strategic_quarter_ppp))\n\n[1] FALSE\n\nany(duplicated(civilians_quarter_ppp))\n\n[1] FALSE\n\n\nWrite the ppp object into RDS file, so that we can re-use them without running all the above steps again.\n\nwrite_rds(battles_quarter_ppp, \"data/rds/battles_quarter_ppp.rds\")\nwrite_rds(explosions_quarter_ppp, \"data/rds/explosions_quarter_ppp.rds\")\nwrite_rds(strategic_quarter_ppp, \"data/rds/strategic_quarter_ppp.rds\")\nwrite_rds(civilians_quarter_ppp, \"data/rds/civilians_quarter_ppp.rds\")\n\n\n\n5.3.2 Including Owin object\n\nmsrb_owin &lt;- as.owin(filtered_msrb)\n\n\nbattles_quarter_owin &lt;- battles_quarter_ppp[msrb_owin]\nexplosions_quarter_owin &lt;- explosions_quarter_ppp[msrb_owin]\nstrategic_quarter_owin &lt;- strategic_quarter_ppp[msrb_owin]\ncivilians_quarter_owin &lt;- civilians_quarter_ppp[msrb_owin]\n# summary(battles_quarter_owin)\n\n\n\n5.3.3 Compute STKDE for Events\nCompute STKDE for battles event.\n\n# Re-scale\nbattles_quarter_owin.km &lt;- rescale(battles_quarter_owin, 1000, \"km\")\nbattles_st_kde &lt;- spattemp.density(battles_quarter_owin.km)\nsummary(battles_st_kde)\n\nSpatiotemporal Kernel Density Estimate\n\nBandwidths\n  h = 56.7339 (spatial)\n  lambda = 0.0033 (temporal)\n\nNo. of observations\n  11970 \n\nSpatial bound\n  Type: polygonal\n  2D enclosure: [-210.0086, 724.6476] x [1087.92, 3158.467]\n\nTemporal bound\n  [2021, 2025]\n\nEvaluation\n  128 x 128 x 5 trivariate lattice\n  Density range: [0, 0.0001031669]\n\n\nExplosions/remote violence\n\n# Re-scale\nexplosions_quarter_owin.km &lt;- rescale(explosions_quarter_owin, 1000, \"km\")\nexplosions_st_kde &lt;- spattemp.density(explosions_quarter_owin.km)\nsummary(explosions_st_kde)\n\nSpatiotemporal Kernel Density Estimate\n\nBandwidths\n  h = 53.3839 (spatial)\n  lambda = 0.0032 (temporal)\n\nNo. of observations\n  12117 \n\nSpatial bound\n  Type: polygonal\n  2D enclosure: [-210.0086, 724.6476] x [1087.92, 3158.467]\n\nTemporal bound\n  [2021, 2025]\n\nEvaluation\n  128 x 128 x 5 trivariate lattice\n  Density range: [0, 0.0001419946]\n\n\nStrategic Developments\n\n# Re-scale\nstrategic_quarter_owin.km &lt;- rescale(strategic_quarter_owin, 1000, \"km\")\nstrategic_st_kde &lt;- spattemp.density(strategic_quarter_owin.km)\nsummary(strategic_st_kde)\n\nSpatiotemporal Kernel Density Estimate\n\nBandwidths\n  h = 52.5751 (spatial)\n  lambda = 0.0032 (temporal)\n\nNo. of observations\n  12028 \n\nSpatial bound\n  Type: polygonal\n  2D enclosure: [-210.0086, 724.6476] x [1087.92, 3158.467]\n\nTemporal bound\n  [2021, 2025]\n\nEvaluation\n  128 x 128 x 5 trivariate lattice\n  Density range: [0, 0.0001335652]\n\n\nViolence Against Civilians\n\n# Re-scale\ncivilians_quarter_owin.km &lt;- rescale(civilians_quarter_owin, 1000, \"km\")\ncivilians_st_kde &lt;- spattemp.density(civilians_quarter_owin.km)\nsummary(civilians_st_kde)\n\nSpatiotemporal Kernel Density Estimate\n\nBandwidths\n  h = 59.8258 (spatial)\n  lambda = 0.0054 (temporal)\n\nNo. of observations\n  6170 \n\nSpatial bound\n  Type: polygonal\n  2D enclosure: [-210.0086, 724.6476] x [1087.92, 3158.467]\n\nTemporal bound\n  [2021, 2025]\n\nEvaluation\n  128 x 128 x 5 trivariate lattice\n  Density range: [0, 6.598748e-05]\n\n\n\n\n5.3.4 STDKE Plot\nCreate variables for later use. Mainly times for the time field for STKDE and time_labels for the naming of the plot.\n\ntimes &lt;- seq(2021.00, 2024.25, by = 0.25)\ntime_labels &lt;- sapply(times, function(t) {\n  year &lt;- floor(t)\n  quarter &lt;- ((t - year) * 4) + 1\n  paste(year, \" Q\", quarter, sep = \"\")\n})\n\n\nBattlesExplosions/Remote ViolenceStrategic DevelopmentsViolence Against Civilians\n\n\n\npar(mfrow = c(2, 2))\nfor(i in seq_along(times)){\n  plot(battles_st_kde, times[i] ,\n       override.par=FALSE,\n       fix.range=TRUE,\n       main=paste(\"Battles STKDE in \",time_labels[i])\n       )\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npar(mfrow = c(2, 2))\nfor(i in seq_along(times)){\n  plot(explosions_st_kde, times[i] ,\n       override.par=FALSE,\n       fix.range=TRUE,\n       main=paste(\"Explosions/Remote Violence STKDE in \",time_labels[i])\n       )\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npar(mfrow = c(2, 2))\nfor(i in seq_along(times)){\n  plot(strategic_st_kde, times[i] ,\n       override.par=FALSE,\n       fix.range=TRUE,\n       main=paste(\"Strategic Developments STKDE in \",time_labels[i])\n       )\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npar(mfrow = c(2, 2))\nfor(i in seq_along(times)){\n  plot(civilians_st_kde, times[i] ,\n       override.par=FALSE,\n       fix.range=TRUE,\n       main=paste(\"Violence Against Civilians STKDE in \",time_labels[i])\n       )\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5.3.5 Animate the Plot\nTo better visualize the spatial-temporal KDE plot, I will use magick package in R to convert a sequence of PNG images into a dynamic GIF. Each PNG file represents spatial-temporal KDE plots for different quarters, and I am combining them into an animated GIF that visually depicts changes in data over time. The magick package allows me to handle and manipulate these images seamlessly, and I am setting parameters like the frame rate (fps) to control the speed of the animation.\n\nBattlesExplosions/Remote ViolenceStrategic DevelopmentsViolence Against Civilians\n\n\n\n\nClick to expand/collapse code\nfor(i in seq_along(times)){\n   # Define the file path\n  file_path &lt;- paste0(\"data/images/\", time_labels[i], \"_Battles.png\")\n  \n  # Open a PNG device\n  png(file_path, width = 800, height = 600)\n  plot(battles_st_kde, times[i] ,\n       override.par=FALSE,\n       fix.range=TRUE,\n       main=paste(\"Battles STKDE in \",time_labels[i])\n       )\n  dev.off()\n}\n\n\n\n\nClick to expand/collapse code\nimage_files &lt;- list.files(\"data/images/\", pattern = \"_Battles\\\\.png\", full.names = TRUE)\nimage_files &lt;- sort(image_files) \n# Read images into a list\nimages &lt;- image_read(image_files)\n\n# Create an animation\nanimation &lt;- image_animate(images, fps = 1)\n\n# Display the animation\nprint(animation)\n\n\n# A tibble: 14 × 7\n   format width height colorspace matte filesize density\n   &lt;chr&gt;  &lt;int&gt;  &lt;int&gt; &lt;chr&gt;      &lt;lgl&gt;    &lt;int&gt; &lt;chr&gt;  \n 1 gif      800    600 sRGB       TRUE         0 72x72  \n 2 gif      800    600 sRGB       TRUE         0 72x72  \n 3 gif      800    600 sRGB       TRUE         0 72x72  \n 4 gif      800    600 sRGB       TRUE         0 72x72  \n 5 gif      800    600 sRGB       TRUE         0 72x72  \n 6 gif      800    600 sRGB       TRUE         0 72x72  \n 7 gif      800    600 sRGB       TRUE         0 72x72  \n 8 gif      800    600 sRGB       TRUE         0 72x72  \n 9 gif      800    600 sRGB       TRUE         0 72x72  \n10 gif      800    600 sRGB       TRUE         0 72x72  \n11 gif      800    600 sRGB       TRUE         0 72x72  \n12 gif      800    600 sRGB       TRUE         0 72x72  \n13 gif      800    600 sRGB       TRUE         0 72x72  \n14 gif      800    600 sRGB       TRUE         0 72x72  \n\n\n\n\n\n\n\n\n\n\n\n\n\nClick to expand/collapse code\nfor(i in seq_along(times)){\n   # Define the file path\n  file_path &lt;- paste0(\"data/images/\", time_labels[i], \"_Explosions.png\")\n  \n  # Open a PNG device\n  png(file_path, width = 800, height = 600)\n  plot(explosions_st_kde, times[i] ,\n       override.par=FALSE,\n       fix.range=TRUE,\n       main=paste(\"Explosions/Remote Violence STKDE in \",time_labels[i])\n       )\n  dev.off()\n}\n\n\n\n\nClick to expand/collapse code\nimage_files &lt;- list.files(\"data/images/\", pattern = \"_Explosions\\\\.png\", full.names = TRUE)\nimage_files &lt;- sort(image_files) \n# Read images into a list\nimages &lt;- image_read(image_files)\n\n# Create an animation\nanimation &lt;- image_animate(images, fps = 1)\n\n# Display the animation\nprint(animation)\n\n\n# A tibble: 14 × 7\n   format width height colorspace matte filesize density\n   &lt;chr&gt;  &lt;int&gt;  &lt;int&gt; &lt;chr&gt;      &lt;lgl&gt;    &lt;int&gt; &lt;chr&gt;  \n 1 gif      800    600 sRGB       TRUE         0 72x72  \n 2 gif      800    600 sRGB       TRUE         0 72x72  \n 3 gif      800    600 sRGB       TRUE         0 72x72  \n 4 gif      800    600 sRGB       TRUE         0 72x72  \n 5 gif      800    600 sRGB       TRUE         0 72x72  \n 6 gif      800    600 sRGB       TRUE         0 72x72  \n 7 gif      800    600 sRGB       TRUE         0 72x72  \n 8 gif      800    600 sRGB       TRUE         0 72x72  \n 9 gif      800    600 sRGB       TRUE         0 72x72  \n10 gif      800    600 sRGB       TRUE         0 72x72  \n11 gif      800    600 sRGB       TRUE         0 72x72  \n12 gif      800    600 sRGB       TRUE         0 72x72  \n13 gif      800    600 sRGB       TRUE         0 72x72  \n14 gif      800    600 sRGB       TRUE         0 72x72  \n\n\n\n\n\n\n\n\n\n\n\n\n\nClick to expand/collapse code\nfor(i in seq_along(times)){\n   # Define the file path\n  file_path &lt;- paste0(\"data/images/\", time_labels[i], \"_Strategic.png\")\n  \n  # Open a PNG device\n  png(file_path, width = 800, height = 600)\n  plot(explosions_st_kde, times[i] ,\n       override.par=FALSE,\n       fix.range=TRUE,\n       main=paste(\"Strategic Developments STKDE in \",time_labels[i])\n       )\n  dev.off()\n}\n\n\n\n\nClick to expand/collapse code\nimage_files &lt;- list.files(\"data/images/\", pattern = \"_Strategic\\\\.png\", full.names = TRUE)\nimage_files &lt;- sort(image_files) \n# Read images into a list\nimages &lt;- image_read(image_files)\n\n# Create an animation\nanimation &lt;- image_animate(images, fps = 1)\n\n# Display the animation\nprint(animation)\n\n\n# A tibble: 14 × 7\n   format width height colorspace matte filesize density\n   &lt;chr&gt;  &lt;int&gt;  &lt;int&gt; &lt;chr&gt;      &lt;lgl&gt;    &lt;int&gt; &lt;chr&gt;  \n 1 gif      800    600 sRGB       TRUE         0 72x72  \n 2 gif      800    600 sRGB       TRUE         0 72x72  \n 3 gif      800    600 sRGB       TRUE         0 72x72  \n 4 gif      800    600 sRGB       TRUE         0 72x72  \n 5 gif      800    600 sRGB       TRUE         0 72x72  \n 6 gif      800    600 sRGB       TRUE         0 72x72  \n 7 gif      800    600 sRGB       TRUE         0 72x72  \n 8 gif      800    600 sRGB       TRUE         0 72x72  \n 9 gif      800    600 sRGB       TRUE         0 72x72  \n10 gif      800    600 sRGB       TRUE         0 72x72  \n11 gif      800    600 sRGB       TRUE         0 72x72  \n12 gif      800    600 sRGB       TRUE         0 72x72  \n13 gif      800    600 sRGB       TRUE         0 72x72  \n14 gif      800    600 sRGB       TRUE         0 72x72  \n\n\n\n\n\n\n\n\n\n\n\n\n\nClick to expand/collapse code\nfor(i in seq_along(times)){\n   # Define the file path\n  file_path &lt;- paste0(\"data/images/\", time_labels[i], \"_Civilians.png\")\n  \n  # Open a PNG device\n  png(file_path, width = 800, height = 600)\n  plot(explosions_st_kde, times[i] ,\n       override.par=FALSE,\n       fix.range=TRUE,\n       main=paste(\"Violence Against Civilians STKDE in \",time_labels[i])\n       )\n  dev.off()\n}\n\n\n\n\nClick to expand/collapse code\nimage_files &lt;- list.files(\"data/images/\", pattern = \"_Civilians\\\\.png\", full.names = TRUE)\nimage_files &lt;- sort(image_files) \n# Read images into a list\nimages &lt;- image_read(image_files)\n\n# Create an animation\nanimation &lt;- image_animate(images, fps = 1)\n\n# Display the animation\nprint(animation)\n\n\n# A tibble: 14 × 7\n   format width height colorspace matte filesize density\n   &lt;chr&gt;  &lt;int&gt;  &lt;int&gt; &lt;chr&gt;      &lt;lgl&gt;    &lt;int&gt; &lt;chr&gt;  \n 1 gif      800    600 sRGB       TRUE         0 72x72  \n 2 gif      800    600 sRGB       TRUE         0 72x72  \n 3 gif      800    600 sRGB       TRUE         0 72x72  \n 4 gif      800    600 sRGB       TRUE         0 72x72  \n 5 gif      800    600 sRGB       TRUE         0 72x72  \n 6 gif      800    600 sRGB       TRUE         0 72x72  \n 7 gif      800    600 sRGB       TRUE         0 72x72  \n 8 gif      800    600 sRGB       TRUE         0 72x72  \n 9 gif      800    600 sRGB       TRUE         0 72x72  \n10 gif      800    600 sRGB       TRUE         0 72x72  \n11 gif      800    600 sRGB       TRUE         0 72x72  \n12 gif      800    600 sRGB       TRUE         0 72x72  \n13 gif      800    600 sRGB       TRUE         0 72x72  \n14 gif      800    600 sRGB       TRUE         0 72x72  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5.3.6 Insights\n\n\n\n\n\n\nSTKDE Insights\n\n\n\n\nIn term of conflict density, we can see that for all four event types, conflicts are primarily occur at the central regions (boundary between Sagaing State, Magway State and Mandalay Sate ) and southern regions(Yangon State, Mon State and Kayin State)\nAs for conflict intensity over the years. We can see that the pattern are similar for the four event types. The intensity increasing start from 2021 Q1 and reach peak in 2022 Q4, and then shows a declining trend from 2023 through 2024 Q2.\nIn terms of the correlation between event types, battles and explosions/remote violence are more closely related, particularly in the southern regions, where the density patterns for both event types are quite similar. On the other hand, strategic developments and violence against civilians are more aligned with each other, as their patterns and densities show similar trends over the same period."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#exploring-using-tmap-functions",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#exploring-using-tmap-functions",
    "title": "Take-home Exercise 1: Geospatial Analytics for Social Good: Application of Spatial and Spatio-temporal Point Patterns Analysis to discover the geographical distribution of Armed Conflict in Myanmar",
    "section": "5.4 Exploring using tmap functions",
    "text": "5.4 Exploring using tmap functions\nI am using a fixed bandwidth of 50km instead of a dynamic one (like bw.ppl). By employing a fixed bandwidth, the smoothing is uniform across the entire area, ensuring consistent density representation across different quarters.\nIn this section, I am not rechecking for duplicate points in the ppp object, as the issue with duplicates was already addressed and resolved in section 5.1.\nI will be using continuous style (style = “cont” ) to plot the tmap, as compare with other style, continuous style give me the most smooth representation of the density.\nExample of using default style.\n\n\nBattlesExplosions/Remote ViolenceStrategic DevelopmentsViolence Against Civilians\n\n\n\n\nClick to expand/collapse code\ntimes &lt;- seq(2021.00, 2024.25, by = 0.25)  # From 2021 Q1 to 2024 Q2\ntime_labels &lt;- sapply(times, function(t) {\n  year &lt;- floor(t)\n  quarter &lt;- ((t - year) * 4) + 1\n  paste(year, \"Q\", quarter, sep = \"\")\n})\n# Create an empty vector to store density values and list to store the KDE layers\nall_densities &lt;- c()\ndensities &lt;- list()\n\nfor (i in seq_along(times)) {\n  conflict_ppp &lt;- battles_quarter_ppp[battles_quarter$year_quarter == times[i]]\n  conflict_ppp &lt;- conflict_ppp[msrb_owin]\n  conflict_ppp.km &lt;- rescale(conflict_ppp, 1000, \"km\")\n  density &lt;- density(conflict_ppp.km, sigma = 50, edge = TRUE)\n  \n  # Convert density to raster and extract values\n  kde_raster &lt;- raster(density)\n  projection(kde_raster) &lt;- CRS(\"+init=EPSG:3414\")\n  \n  # Extract density values and add to all_densities\n  all_densities &lt;- c(all_densities, values(kde_raster))\n  densities[[i]] &lt;- kde_raster\n}\n\nmax_density &lt;- max(all_densities, na.rm = TRUE)\n\n# Create an empty list to store the tmap objects\ntmaps &lt;- list()\n# Loop through each time period, create the KDE map, and add it to the list\nfor (i in seq_along(times)) {\n  tm &lt;- tm_shape(densities[[i]] ) +\n    tm_raster(palette = \"YlOrRd\", \n              title = \"Conflict Density\",\n              style = \"cont\",\n              breaks = seq(0, max_density, length.out = 5)) +\n    tm_layout(title = paste(\"Battles Conflict Density - \", time_labels[i]),\n              legend.position = c(\"left\", \"bottom\"), \n              frame = FALSE)\n\n  # Add the tmap object to the list\n  tmaps[[i]] &lt;- tm\n}\ntmap_animation(tmaps, filename = \"website_files/gif/battles_conflict_density_animation.gif\", delay = 100, width = 800, height = 600)\n\n\n\n\n\n\n\nClick to expand/collapse code\ntimes &lt;- seq(2021.00, 2024.25, by = 0.25)  # From 2021 Q1 to 2024 Q2\ntime_labels &lt;- sapply(times, function(t) {\n  year &lt;- floor(t)\n  quarter &lt;- ((t - year) * 4) + 1\n  paste(year, \"Q\", quarter, sep = \"\")\n})\n# Create an empty vector to store density values and list to store the KDE layers\nall_densities &lt;- c()\ndensities &lt;- list()\n\nfor (i in seq_along(times)) {\n  conflict_ppp &lt;- explosions_quarter_ppp[explosions_quarter$year_quarter == times[i]]\n  conflict_ppp &lt;- conflict_ppp[msrb_owin]\n  conflict_ppp.km &lt;- rescale(conflict_ppp, 1000, \"km\")\n  density &lt;- density(conflict_ppp.km, sigma = 50, edge = TRUE)\n  \n  # Convert density to raster and extract values\n  kde_raster &lt;- raster(density)\n  projection(kde_raster) &lt;- CRS(\"+init=EPSG:3414\")\n  \n  # Extract density values and add to all_densities\n  all_densities &lt;- c(all_densities, values(kde_raster))\n  densities[[i]] &lt;- kde_raster\n}\n\nmax_density &lt;- max(all_densities, na.rm = TRUE)\n\n# Create an empty list to store the tmap objects\ntmaps &lt;- list()\n# Loop through each time period, create the KDE map, and add it to the list\nfor (i in seq_along(times)) {\n  tm &lt;- tm_shape(densities[[i]] ) +\n    tm_raster(palette = \"YlOrRd\", \n              title = \"Conflict Density\",\n              style = \"cont\",\n              breaks = seq(0, max_density, length.out = 5)) +\n    tm_layout(title = paste(\"Explosions/Remote Violence Conflict Density - \", time_labels[i]),\n              legend.position = c(\"left\", \"bottom\"), \n              frame = FALSE)\n\n  # Add the tmap object to the list\n  tmaps[[i]] &lt;- tm\n}\ntmap_animation(tmaps, filename = \"website_files/gif/explosions_conflict_density_animation.gif\", delay = 100, width = 800, height = 600)\n\n\n\n\n\n\n\nClick to expand/collapse code\ntimes &lt;- seq(2021.00, 2024.25, by = 0.25)  # From 2021 Q1 to 2024 Q2\ntime_labels &lt;- sapply(times, function(t) {\n  year &lt;- floor(t)\n  quarter &lt;- ((t - year) * 4) + 1\n  paste(year, \"Q\", quarter, sep = \"\")\n})\n# Create an empty vector to store density values and list to store the KDE layers\nall_densities &lt;- c()\ndensities &lt;- list()\n\nfor (i in seq_along(times)) {\n  conflict_ppp &lt;- strategic_quarter_ppp[strategic_quarter$year_quarter == times[i]]\n  conflict_ppp &lt;- conflict_ppp[msrb_owin]\n  conflict_ppp.km &lt;- rescale(conflict_ppp, 1000, \"km\")\n  density &lt;- density(conflict_ppp.km, sigma = 50, edge = TRUE)\n  \n  # Convert density to raster and extract values\n  kde_raster &lt;- raster(density)\n  projection(kde_raster) &lt;- CRS(\"+init=EPSG:3414\")\n  \n  # Extract density values and add to all_densities\n  all_densities &lt;- c(all_densities, values(kde_raster))\n  densities[[i]] &lt;- kde_raster\n}\n\nmax_density &lt;- max(all_densities, na.rm = TRUE)\n\n# Create an empty list to store the tmap objects\ntmaps &lt;- list()\n# Loop through each time period, create the KDE map, and add it to the list\nfor (i in seq_along(times)) {\n  tm &lt;- tm_shape(densities[[i]] ) +\n    tm_raster(palette = \"YlOrRd\", \n              title = \"Conflict Density\",\n              style = \"cont\",\n              breaks = seq(0, max_density, length.out = 5)) +\n    tm_layout(title = paste(\"Strategic Developments Conflict Density - \", time_labels[i]),\n              legend.position = c(\"left\", \"bottom\"), \n              frame = FALSE)\n\n  # Add the tmap object to the list\n  tmaps[[i]] &lt;- tm\n}\ntmap_animation(tmaps, filename = \"website_files/gif/strategic_conflict_density_animation.gif\", delay = 100, width = 800, height = 600)\n\n\n\n\n\n\n\nClick to expand/collapse code\ntimes &lt;- seq(2021.00, 2024.25, by = 0.25)  # From 2021 Q1 to 2024 Q2\ntime_labels &lt;- sapply(times, function(t) {\n  year &lt;- floor(t)\n  quarter &lt;- ((t - year) * 4) + 1\n  paste(year, \"Q\", quarter, sep = \"\")\n})\n# Create an empty vector to store density values and list to store the KDE layers\nall_densities &lt;- c()\ndensities &lt;- list()\n\nfor (i in seq_along(times)) {\n  conflict_ppp &lt;- civilians_quarter_ppp[civilians_quarter$year_quarter == times[i]]\n  conflict_ppp &lt;- conflict_ppp[msrb_owin]\n  conflict_ppp.km &lt;- rescale(conflict_ppp, 1000, \"km\")\n  density &lt;- density(conflict_ppp.km, sigma = 50, edge = TRUE)\n  \n  # Convert density to raster and extract values\n  kde_raster &lt;- raster(density)\n  projection(kde_raster) &lt;- CRS(\"+init=EPSG:3414\")\n  \n  # Extract density values and add to all_densities\n  all_densities &lt;- c(all_densities, values(kde_raster))\n  densities[[i]] &lt;- kde_raster\n}\n\nmax_density &lt;- max(all_densities, na.rm = TRUE)\n\n# Create an empty list to store the tmap objects\ntmaps &lt;- list()\n# Loop through each time period, create the KDE map, and add it to the list\nfor (i in seq_along(times)) {\n  tm &lt;- tm_shape(densities[[i]] ) +\n    tm_raster(palette = \"YlOrRd\", \n              title = \"Conflict Density\",\n              style = \"cont\",\n              breaks = seq(0, max_density, length.out = 5)) +\n    tm_layout(title = paste(\"Violence Against Civilians Conflict Density - \", time_labels[i]),\n              legend.position = c(\"left\", \"bottom\"), \n              frame = FALSE)\n\n  # Add the tmap object to the list\n  tmaps[[i]] &lt;- tm\n}\ntmap_animation(tmaps, filename = \"website_files/gif/civilians_conflict_density_animation.gif\", delay = 100, width = 800, height = 600)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#data-preparation",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#data-preparation",
    "title": "Take-home Exercise 1: Geospatial Analytics for Social Good: Application of Spatial and Spatio-temporal Point Patterns Analysis to discover the geographical distribution of Armed Conflict in Myanmar",
    "section": "6.1 Data preparation",
    "text": "6.1 Data preparation\nAs we want to perform 2nd-order spatio-temporal point patterns analysis, which we can re-use the quarterly ppp object that we have.\n\nfiltered_msrb &lt;- read_rds(\"data/rds/filtered_msrb.rds\")\nbattles_quarter_ppp &lt;- read_rds(\"data/rds/battles_quarter_ppp.rds\")\nexplosions_quarter_ppp &lt;- read_rds(\"data/rds/explosions_quarter_ppp.rds\")\nstrategic_quarter_ppp &lt;- read_rds(\"data/rds/strategic_quarter_ppp.rds\")\ncivilians_quarter_ppp &lt;- read_rds(\"data/rds/civilians_quarter_ppp.rds\")\n\nTo check any duplicated points.\n\nany(duplicated(battles_quarter_ppp))\n\n[1] FALSE\n\nany(duplicated(explosions_quarter_ppp))\n\n[1] FALSE\n\nany(duplicated(strategic_quarter_ppp))\n\n[1] FALSE\n\nany(duplicated(civilians_quarter_ppp))\n\n[1] FALSE\n\n\nTo ensure that, our ppp object have time data associated with the points, we can use str function to check.\n\nstr(battles_quarter_ppp)\n\nList of 6\n $ window    :List of 4\n  ..$ type  : chr \"rectangle\"\n  ..$ xrange: num [1:2] -207135 591876\n  ..$ yrange: num [1:2] 1103500 3026505\n  ..$ units :List of 3\n  .. ..$ singular  : chr \"unit\"\n  .. ..$ plural    : chr \"units\"\n  .. ..$ multiplier: num 1\n  .. ..- attr(*, \"class\")= chr \"unitname\"\n  ..- attr(*, \"class\")= chr \"owin\"\n $ n         : int 12049\n $ x         : num [1:12049] 214364 198094 189680 146645 261929 ...\n $ y         : num [1:12049] 2452234 2499436 2533033 2427884 2556098 ...\n $ markformat: chr \"vector\"\n $ marks     : num [1:12049] 2024 2024 2024 2024 2024 ...\n - attr(*, \"class\")= chr \"ppp\"\n\n\nNotice that marks field store our time field in quarters (2021.00 =&gt; 2021 Q1, 2021.25 =&gt; 2021 Q2 etc).\nTo ensure the field is compatible with the PCFhat function, I need to convert the values from double to integer. This involves transforming 2021.00 to 1, 2021.25 to 2, and so on, up to 2024.25 being converted to 14.\n\nconvert_to_integer &lt;- function(decimal) {\n  year &lt;- floor(decimal)\n  quarter &lt;- (decimal - year) * 4 + 1\n  quarter &lt;- round(quarter)\n  return((year - 2021) * 4 + quarter)\n}\n\n# Convert marks to integers\nbattles_quarter_ppp$marks &lt;- sapply(battles_quarter_ppp$marks, convert_to_integer)\nexplosions_quarter_ppp$marks &lt;- sapply(explosions_quarter_ppp$marks, convert_to_integer)\nstrategic_quarter_ppp$marks &lt;- sapply(strategic_quarter_ppp$marks, convert_to_integer)\ncivilians_quarter_ppp$marks &lt;- sapply(civilians_quarter_ppp$marks, convert_to_integer)\n\nTo use the PCFhat function, an error will occur if there are negative values in the x or y coordinates. I noticed that the CRS 32647, which we used to convert the sf object into a ppp object, generates negative values. Therefore, the code below offsets the negative x-coordinates by shifting all points to the right.\n\n# Transform and adjust filtered_msrb\nbbox &lt;- st_bbox(filtered_msrb)\n# As I am using Myanmar region to find out the offset value for x-coordinates.\noffset_x &lt;- -min(bbox$xmin, bbox$xmax)\n\n# Adjust x-coordinates, as STIKhat cannot handle negative coordinates.\nbattles_quarter_ppp$x &lt;- battles_quarter_ppp$x + offset_x\nexplosions_quarter_ppp$x &lt;- explosions_quarter_ppp$x + offset_x\nstrategic_quarter_ppp$x &lt;- strategic_quarter_ppp$x + offset_x\ncivilians_quarter_ppp$x &lt;- civilians_quarter_ppp$x + offset_x"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#plotting-pair-correlation-function",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#plotting-pair-correlation-function",
    "title": "Take-home Exercise 1: Geospatial Analytics for Social Good: Application of Spatial and Spatio-temporal Point Patterns Analysis to discover the geographical distribution of Armed Conflict in Myanmar",
    "section": "6.2 Plotting Pair Correlation function",
    "text": "6.2 Plotting Pair Correlation function\nThe code chunk below will create an stpp object for the battles data and verify that it is correctly formatted as an stpp object. Then, plot the Pair Correlation Function (PCF).\nSince the plot doesn’t display when rendered in the browser, I will save the image generated from the plotPCF function and use it for website display.\n\nBattlesExplosions/Remote violenceStrategic developmentsViolence against civilians\n\n\n\n\nClick to expand/collapse code\nlibrary(stpp)\nbattles_stpp &lt;- as.3dpoints(\n  x = battles_quarter_ppp$x /1000, # change it in km\n  y = battles_quarter_ppp$y /1000, # change it in km\n  t = battles_quarter_ppp$marks\n)\nclass(battles_stpp)\nu &lt;- seq(0,1000,by=50)\nv &lt;- seq(0,14,by=1)\n# Use PCFhat function from stpp package\nbattles_pcf_result &lt;- PCFhat(\n  xyt = battles_stpp,\n  t.region=c(1,14),\n  dist=u,\n  times=v,\n)\nplotPCF(battles_pcf_result, type = \"contour\", main = \"Battle - Contour Plot\")\nplotPCF(battles_pcf_result, type = \"persp\", main = \"Battle - Perspective Plot\")\nplotPCF(battles_pcf_result, type = \"image\", main = \"Battle - Image Plot\")\n\n\n  \n\n\n\n\nClick to expand/collapse code\nexplosions_stpp &lt;- as.3dpoints(\n  x = explosions_quarter_ppp$x /1000, # change it in km\n  y = explosions_quarter_ppp$y /1000, # change it in km\n  t = explosions_quarter_ppp$marks\n)\nclass(explosions_stpp)\n\nu &lt;- seq(0,1000,by=50)\nv &lt;- seq(0,14,by=1)\n# Use PCFhat function from stpp package\nexplosions_pcf_result &lt;- PCFhat(\n  xyt = explosions_stpp,\n  t.region=c(1,14),\n  dist=u,\n  times=v,\n)\n\nplotPCF(explosions_pcf_result, type = \"contour\", main = \"Explosions/Remote violence - Contour Plot\")\nplotPCF(explosions_pcf_result, type = \"persp\", main = \"Explosions/Remote violence - Perspective Plot\")\nplotPCF(explosions_pcf_result, type = \"image\", main = \"Explosions/Remote violence - Image Plot\")\n\n\n  \n\n\n\n\nClick to expand/collapse code\nstrategic_stpp &lt;- as.3dpoints(\n  x = strategic_quarter_ppp$x /1000, # change it in km\n  y = strategic_quarter_ppp$y /1000, # change it in km\n  t = strategic_quarter_ppp$marks\n)\nclass(strategic_stpp)\n\nu &lt;- seq(0,1000,by=50)\nv &lt;- seq(0,14,by=1)\n# Use PCFhat function from stpp package\nstrategic_pcf_result &lt;- PCFhat(\n  xyt = strategic_stpp,\n  t.region=c(1,14),\n  dist=u,\n  times=v,\n)\n\nplotPCF(strategic_pcf_result, type = \"contour\", main = \"Strategic developments - Contour Plot\")\nplotPCF(strategic_pcf_result, type = \"persp\", main = \"Strategic developments - Perspective Plot\")\nplotPCF(strategic_pcf_result, type = \"image\", main = \"Strategic developments - Image Plot\")\n\n\n  \n\n\n\n\nClick to expand/collapse code\ncivilians_stpp &lt;- as.3dpoints(\n  x = civilians_quarter_ppp$x /1000, # change it in km\n  y = civilians_quarter_ppp$y /1000, # change it in km\n  t = civilians_quarter_ppp$marks\n)\nclass(civilians_stpp)\n\nu &lt;- seq(0,1000,by=50)\nv &lt;- seq(0,14,by=1)\n# Use PCFhat function from stpp package\ncivilians_pcf_result &lt;- PCFhat(\n  xyt = civilians_stpp,\n  t.region=c(1,14),\n  dist=u,\n  times=v,\n)\n\nplotPCF(civilians_pcf_result, type = \"contour\", main = \"Violence against civilians - Contour Plot\")\nplotPCF(civilians_pcf_result, type = \"persp\", main = \"Violence against civilians - Perspective Plot\")\nplotPCF(civilians_pcf_result, type = \"image\", main = \"Violence against civilians - Image Plot\")"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#insight-6",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#insight-6",
    "title": "Take-home Exercise 1: Geospatial Analytics for Social Good: Application of Spatial and Spatio-temporal Point Patterns Analysis to discover the geographical distribution of Armed Conflict in Myanmar",
    "section": "6.3 Insight",
    "text": "6.3 Insight\n\n\n\n\n\n\nNote\n\n\n\n\nx-axis = distance; y-axis = time\nHow to interpret PCF values:\n\nPCF = 1: Indicates random distribution of events.\nPCF &gt; 1: indicate clustering of events, meaning events are likely occur close to each other spatially and temporally.\nPCF &lt;1: indicate inhibition in space-time, meaning events are less likely to occur close to each other spatially and temporally.\n\nBased on the image plot of the 4 event types, I observed that among the 4 Spatio-temporal PCF plots, there are two heavy density area:\n\n1 =&gt; distance from 0 to 100 km\n2 =&gt; distance from 280 to 350 km\nBased on the KDE analysis, I suspect these two areas represent the central region (bordering Magway, Mandalay, and Sagaing states) and the southern region (around Bago, Yangon, and Mon states). As the KDE suggests that these regions have experienced the highest concentration of events.\n\nThe Spatio-temporal PCF plots also support my previous insight from the KDE regarding the correlation between event types. Battles and Explosions/Remote Violence appear to be correlated, while Strategic Developments and Violence Against Civilians also show a correlation."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "IS415-GAA",
    "section": "",
    "text": "Welcome to IS415 - Geospatial Analytics and Application (AY24/25 T1). In this website, you will find my work for this course.\n\n\n\n\n\n\n\n\n\n\n\n\nHands-on Exercise 06: Global/Local Measures of Spatial Autocorrelation\n\n\n\n\n\n\nPan Mingwei\n\n\nSep 19, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nHands-On Exercise 05: Spatial Weights and Applications\n\n\n\n\n\n\nPan Mingwei\n\n\nSep 13, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nHands-On Exercise 03: 1st and 2nd Order Spatial Point Patterns Analysis Methods\n\n\n\n\n\n\nPan Mingwei\n\n\nAug 27, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nHands-On Exercise 02: Thematic Mapping and Geovisualisation with R\n\n\n\n\n\n\nPan Mingwei\n\n\nAug 22, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nHands-on Exercise 01:Geospatial Data Science with R\n\n\n\n\n\n\nPan Mingwei\n\n\nAug 15, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html",
    "title": "Hands-on Exercise 06: Global/Local Measures of Spatial Autocorrelation",
    "section": "",
    "text": "In this hands-on exercise, I will learn how to compute Global Measures of Spatial Autocorrelation (GMSA) and Local Measures of Spatial Autocorrelation (LMSA) by using spdep package.\n\n\nEvaluate the overall degree of spatial dependence (autocorrelation) across the entire study area.\n\nMoran’s I: A widely used global indicator that measures whether a variable is spatially clustered, dispersed, or randomly distributed.\n\nI &gt; 0: Clustered, observation tend to be similar.\nI &lt; 0: Dispersed, observations tend to be dissimilar.\napproximately zero: observations are arranged randomly over space.\n\nGeary’s C: Similar to Moran’s I but more sensitive to local differences.\n\nC &gt; 1: Dispersed, observations tend to be dissimilar.\nC &lt; 1: Clustered, observations tend to be similar.\nC = 1: Observations are arranged randomly over space.\n\n\nBy the end of this hands-on exercise, I will be able to :\n\ncompute Global Spatial Autocorrelation (GSA) statistics by using appropriate functions of spdeppackage,\n\nplot Moran scatterplot,\ncompute and plot spatial correlogram using appropriate function of spdep package.\n\nprovide statistically correct interpretation of GSA statistics.\n\n\n\n\nAssess the degree of spatial autocorrelation at a local, rather than global, scale to identify clusters or outliers in specific areas.\n\nLocal Moran’s I: Measures how much a given location contributes to the overall Moran’s I, identifying local clusters and outliers.\n\nLocal cluster: Significant and positive if location i is associated with relatively high values of the surrounding locations.\nLocal outlier: Significant and negative if location i is associated with relatively low values in surrounding locations.\n\nGetis-Ord Gi*:Measures local “hotspots” and “cold spots” where values are significantly higher or lower than expected.\n\nHot spot area: Significant and positive if location i is associated with relatively high values of the surrounding locations.\nCold spot area: Significant and negative if location i is associated with relatively low values in surrounding locations.\n\n\nBy the end of the hands-on, I will be able to :\n\ncompute Local Indicator of Spatial Association (LISA) statistics for detecting clusters and outliers by using appropriate functions spdep package;\ncompute Getis-Ord’s Gi-statistics for detecting hot spot or/and cold spot area by using appropriate functions of spdep package; and\nto visualise the analysis output by using tmap package."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#what-is-global-measures-of-spatial-autocorrelation-gmsa",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#what-is-global-measures-of-spatial-autocorrelation-gmsa",
    "title": "Hands-on Exercise 06: Global/Local Measures of Spatial Autocorrelation",
    "section": "",
    "text": "Evaluate the overall degree of spatial dependence (autocorrelation) across the entire study area.\n\nMoran’s I: A widely used global indicator that measures whether a variable is spatially clustered, dispersed, or randomly distributed.\n\nI &gt; 0: Clustered, observation tend to be similar.\nI &lt; 0: Dispersed, observations tend to be dissimilar.\napproximately zero: observations are arranged randomly over space.\n\nGeary’s C: Similar to Moran’s I but more sensitive to local differences.\n\nC &gt; 1: Dispersed, observations tend to be dissimilar.\nC &lt; 1: Clustered, observations tend to be similar.\nC = 1: Observations are arranged randomly over space.\n\n\nBy the end of this hands-on exercise, I will be able to :\n\ncompute Global Spatial Autocorrelation (GSA) statistics by using appropriate functions of spdeppackage,\n\nplot Moran scatterplot,\ncompute and plot spatial correlogram using appropriate function of spdep package.\n\nprovide statistically correct interpretation of GSA statistics."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#what-is-local-measures-of-spatial-autocorrelation-lmsa",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#what-is-local-measures-of-spatial-autocorrelation-lmsa",
    "title": "Hands-on Exercise 06: Global/Local Measures of Spatial Autocorrelation",
    "section": "",
    "text": "Assess the degree of spatial autocorrelation at a local, rather than global, scale to identify clusters or outliers in specific areas.\n\nLocal Moran’s I: Measures how much a given location contributes to the overall Moran’s I, identifying local clusters and outliers.\n\nLocal cluster: Significant and positive if location i is associated with relatively high values of the surrounding locations.\nLocal outlier: Significant and negative if location i is associated with relatively low values in surrounding locations.\n\nGetis-Ord Gi*:Measures local “hotspots” and “cold spots” where values are significantly higher or lower than expected.\n\nHot spot area: Significant and positive if location i is associated with relatively high values of the surrounding locations.\nCold spot area: Significant and negative if location i is associated with relatively low values in surrounding locations.\n\n\nBy the end of the hands-on, I will be able to :\n\ncompute Local Indicator of Spatial Association (LISA) statistics for detecting clusters and outliers by using appropriate functions spdep package;\ncompute Getis-Ord’s Gi-statistics for detecting hot spot or/and cold spot area by using appropriate functions of spdep package; and\nto visualise the analysis output by using tmap package."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#the-analytical-question",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#the-analytical-question",
    "title": "Hands-on Exercise 06: Global/Local Measures of Spatial Autocorrelation",
    "section": "2.1 The Analytical Question",
    "text": "2.1 The Analytical Question\nIn spatial policy, one of the main development objective of the local government and planners is to ensure equal distribution of development in the province. Our task in this study, hence, is to apply appropriate spatial statistical methods to discover if development are even distributed geographically. If the answer is No. Then, our next question will be “is there sign of spatial clustering?”. And, if the answer for this question is yes, then our next question will be “where are these clusters?”\nIn this case study, we are interested to examine the spatial pattern of a selected development indicator (i.e. GDP per capita) of Hunan Provice, People Republic of China."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#the-study-area-and-data",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#the-study-area-and-data",
    "title": "Hands-on Exercise 06: Global/Local Measures of Spatial Autocorrelation",
    "section": "2.2 The Study Area and Data",
    "text": "2.2 The Study Area and Data\n\nHunan province administrative boundary layer at county level. This is a geospatial data set in ESRI shapefile format.\nHunan_2012.csv: This csv file contains selected Hunan’s local development indicators in 2012."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#setting-the-analytical-tools",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#setting-the-analytical-tools",
    "title": "Hands-on Exercise 06: Global/Local Measures of Spatial Autocorrelation",
    "section": "2.3 Setting the Analytical Tools",
    "text": "2.3 Setting the Analytical Tools\n\nsf is use for importing and handling geospatial data in R,\ntidyverse is mainly use for wrangling attribute data in R,\nspdep will be used to compute spatial weights, global and local spatial autocorrelation statistics, and\ntmap will be used to prepare cartographic quality chropleth map.\n\nTo check if the packages have been installed in R and load the packages into the current R environment.\n\npacman::p_load(sf, spdep, tmap, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#import-shapefile",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#import-shapefile",
    "title": "Hands-on Exercise 06: Global/Local Measures of Spatial Autocorrelation",
    "section": "3.1 Import shapefile",
    "text": "3.1 Import shapefile\nUsing st_read() of sf package to import Hunan shapefile into R. It will be in sf object.\n\nhunan_sf &lt;- st_read(dsn = \"data/geospatial\", \n                 layer = \"Hunan\")\n\nReading layer `Hunan' from data source \n  `/Users/mingwei/Desktop/SMU/Y3S1/IS415/xXxPMWxXx/IS415-GAA/Hands-on_Ex/Hands-on_Ex06/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#import-csv-file",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#import-csv-file",
    "title": "Hands-on Exercise 06: Global/Local Measures of Spatial Autocorrelation",
    "section": "3.2 Import CSV File",
    "text": "3.2 Import CSV File\nUsing read_csv() of readr package. The output is R data frame class.\n\nhunan2012 &lt;- read_csv(\"data/aspatial/Hunan_2012.csv\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#performing-relational-join",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#performing-relational-join",
    "title": "Hands-on Exercise 06: Global/Local Measures of Spatial Autocorrelation",
    "section": "3.3 Performing Relational Join",
    "text": "3.3 Performing Relational Join\nUsing left_join() of dplyr package to update the attribute table of hunan_sf with the attribute fields of hunan2012 dataframe.\n\nhunan_sf &lt;- left_join(hunan_sf,hunan2012) %&gt;%\n  dplyr::select(1:4, 7, 15)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#visualising-regional-development-indicator",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#visualising-regional-development-indicator",
    "title": "Hands-on Exercise 06: Global/Local Measures of Spatial Autocorrelation",
    "section": "3.4 Visualising Regional Development Indicator",
    "text": "3.4 Visualising Regional Development Indicator\nTo prepare a basemap and a choropleth map showing the distribution of GDPPC 2021 using qtm() of tmap package.\n\nequal &lt;- tm_shape(hunan_sf) +\n  tm_fill(\"GDPPC\",\n          n = 5,\n          style = \"equal\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Equal interval classification\")\n\nquantile &lt;- tm_shape(hunan_sf) +\n  tm_fill(\"GDPPC\",\n          n = 5,\n          style = \"quantile\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Equal quantile classification\")\n\ntmap_arrange(equal, \n             quantile, \n             asp=1, \n             ncol=2)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#computing-contiguity-spatial-weights",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#computing-contiguity-spatial-weights",
    "title": "Hands-on Exercise 06: Global/Local Measures of Spatial Autocorrelation",
    "section": "4.1 Computing Contiguity Spatial Weights",
    "text": "4.1 Computing Contiguity Spatial Weights\nBefore we can compute the global spatial autocorrelation statistics, we need to construct a spatial weights of the study area. The spatial weights is used to define the neighbourhood relationships between the geographical units (i.e. county) in the study area.\nIn the code chunk below, poly2nb() of spdep package is used to compute contiguity weight matrices for the study area. This function builds a neighbours list based on regions with contiguous boundaries. If you look at the documentation you will see that you can pass a “queen” argument that takes TRUE or FALSE as options. If you do not specify this argument the default is set to TRUE, that is, if you don’t specify queen = FALSE this function will return a list of first order neighbours using the Queen criteria.\nMore specifically, the code chunk below is used to compute Queen contiguity weight matrix.\n\nwm_q &lt;- poly2nb(hunan_sf, \n                queen=TRUE)\nsummary(wm_q)\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 11 \n 2  2 12 16 24 14 11  4  2  1 \n2 least connected regions:\n30 65 with 1 link\n1 most connected region:\n85 with 11 links\n\n\nThe summary report above shows that there are 88 area units in Hunan. The most connected area unit(85) has 11 neighbours. There are two area units(30,65) with only one neighbours."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#row-standardised-weights-matrix",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#row-standardised-weights-matrix",
    "title": "Hands-on Exercise 06: Global/Local Measures of Spatial Autocorrelation",
    "section": "4.2 Row-Standardised Weights Matrix",
    "text": "4.2 Row-Standardised Weights Matrix\nNext, we need to assign weights to each neighboring polygon. In our case, each neighboring polygon will be assigned equal weight (style=“W”). This is accomplished by assigning the fraction 1/(#ofneighbors) to each neighboring county then summing the weighted income values. While this is the most intuitive way to summaries the neighbors’ values it has one drawback in that polygons along the edges of the study area will base their lagged values on fewer polygons thus potentially over- or under-estimating the true nature of the spatial autocorrelation in the data. For this example, we’ll stick with the style=“W” option for simplicity’s sake but note that other more robust options are available, notably style=“B”.\n\nrswm_q &lt;- nb2listw(wm_q, \n                   style=\"W\", \n                   zero.policy = TRUE)\nrswm_q\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: W \nWeights constants summary:\n   n   nn S0       S1       S2\nW 88 7744 88 37.86334 365.9147\n\n\n\nclass(wm_q)\n\n[1] \"nb\"\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nThe input of nb2listw() must be an object of class nb. The syntax of the function has two major arguments, namely style and zero.poly.\nstyle can take values “W”, “B”, “C”, “U”, “minmax” and “S”. B is the basic binary coding, W is row standardised (sums over all links to n), C is globally standardised (sums over all links to n), U is equal to C divided by the number of neighbours (sums over all links to unity), while S is the variance-stabilizing coding scheme proposed by Tiefelsdorf et al. 1999, p. 167-168 (sums over all links to n).\nIf zero policy is set to TRUE, weights vectors of zero length are inserted for regions without neighbour in the neighbours list. These will in turn generate lag values of zero, equivalent to the sum of products of the zero row t(rep(0, length=length(neighbours))) %*% x, for arbitrary numerical vector x of length length(neighbours). The spatially lagged value of x for the zero-neighbour region will then be zero, which may (or may not) be a sensible choice."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#marons-i-test",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#marons-i-test",
    "title": "Hands-on Exercise 06: Global/Local Measures of Spatial Autocorrelation",
    "section": "5.1 Maron’s I Test",
    "text": "5.1 Maron’s I Test\nUsing moran.test() of spdep.\n\nmoran.test(hunan_sf$GDPPC, \n           listw=rswm_q, \n           zero.policy = TRUE, \n           na.action=na.omit)\n\n\n    Moran I test under randomisation\n\ndata:  hunan_sf$GDPPC  \nweights: rswm_q    \n\nMoran I statistic standard deviate = 4.7351, p-value = 1.095e-06\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.300749970      -0.011494253       0.004348351 \n\n\n\n\n\n\n\n\nNote\n\n\n\n\nI = 0.3007, indicates positive spatial autocorrelation, meaning that areas with similar values of GDPPC are geographically clustered together.\nExpectation = -0.01149, indicates that for null hypothesis of no spatial autocorrelation . But the fact that the observed statistic(0.3007) is significantly higher than the expectation value suggests evidence against the null hypothesis.\np-value = 1.095e-06, indicate that the null hypothesis can be rejected."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#computing-monte-carlo-morans-i",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#computing-monte-carlo-morans-i",
    "title": "Hands-on Exercise 06: Global/Local Measures of Spatial Autocorrelation",
    "section": "5.2 Computing Monte Carlo Moran’s I",
    "text": "5.2 Computing Monte Carlo Moran’s I\nThe code chunk below performs permutation test for Moran’s I statistic by using moran.mc() of spdep. A total of 1000 simulation will be performed.\n\nset.seed(1234)\nbperm= moran.mc(hunan_sf$GDPPC, \n                listw=rswm_q, \n                nsim=999, \n                zero.policy = TRUE, \n                na.action=na.omit)\nbperm\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  hunan_sf$GDPPC \nweights: rswm_q  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.30075, observed rank = 1000, p-value = 0.001\nalternative hypothesis: greater\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nStatistic = 0.30075, indicates a postive spatial autocorrelation, meaning that regions with similar GDPPC values are more likely to located near each other.\np-value = 0.001(smaller than 0.05), indicates that we can reject the null hypothesis(that locations do not depend on GDPPC values at other locaitons)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#visualising-monte-carlo-morans-i",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#visualising-monte-carlo-morans-i",
    "title": "Hands-on Exercise 06: Global/Local Measures of Spatial Autocorrelation",
    "section": "5.3 Visualising Monte Carlo Moran’s I",
    "text": "5.3 Visualising Monte Carlo Moran’s I\nWe can use hist() and abline() of R Graphics to examine the simulated Moran’s I test statistics in greater details.\n\nmean(bperm$res[1:999])\n\n[1] -0.01504572\n\n\n\nvar(bperm$res[1:999])\n\n[1] 0.004371574\n\n\n\nsummary(bperm$res[1:999])\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-0.18339 -0.06168 -0.02125 -0.01505  0.02611  0.27593 \n\n\n\nhist(bperm$res, \n     freq=TRUE, \n     breaks=20, \n     xlab=\"Simulated Moran's I\")\nabline(v=0, \n       col=\"red\") \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nNotices that the distribution of the simulated Moran’s I values is primarily skewed toward the negative side, suggesting that the data points exhibit a dispersed spatial pattern.\n\n\n\n\nbperm_df &lt;- data.frame(simulated_morans_I = bperm$res)\n\n# Plot the histogram using ggplot2\nggplot(bperm_df, aes(x = simulated_morans_I)) +\n  geom_histogram(binwidth = diff(range(bperm_df$simulated_morans_I)) / 20, \n                 fill = \"grey\", color = \"black\") + \n  geom_vline(xintercept = 0, color = \"red\", linetype = \"solid\", size = 1) +\n  labs(x = \"Simulated Moran's I\", \n       y = \"Frequency\", \n       title = \"Histogram of Simulated Moran's I Values\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#gearys-c-test",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#gearys-c-test",
    "title": "Hands-on Exercise 06: Global/Local Measures of Spatial Autocorrelation",
    "section": "6.1 Geary’s C test",
    "text": "6.1 Geary’s C test\nThe code chunk below performs Geary’s C test for spatial autocorrelation by using geary.test() of spdep.\n\ngeary.test(hunan_sf$GDPPC, listw=rswm_q)\n\n\n    Geary C test under randomisation\n\ndata:  hunan_sf$GDPPC \nweights: rswm_q   \n\nGeary C statistic standard deviate = 3.6108, p-value = 0.0001526\nalternative hypothesis: Expectation greater than statistic\nsample estimates:\nGeary C statistic       Expectation          Variance \n        0.6907223         1.0000000         0.0073364 \n\n\n\n\n\n\n\n\nNote\n\n\n\n\nC = 0.6907223 (&lt; 1), indicates that regions with similar GDPPC values are more likely to located near each other(clustered).\np-value = 0..0001526, means the spatial autocorrelation is unlikely to have occurred by random, which we can reject the null hypothesis."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#computing-monte-carlo-gearys-c",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#computing-monte-carlo-gearys-c",
    "title": "Hands-on Exercise 06: Global/Local Measures of Spatial Autocorrelation",
    "section": "6.2 Computing Monte Carlo Geary’s C",
    "text": "6.2 Computing Monte Carlo Geary’s C\nTo performs permutation test for Geary’s C statistic by using geary.mc() of spdep.\n\nset.seed(1234)\nbperm=geary.mc(hunan_sf$GDPPC, \n               listw=rswm_q, \n               nsim=999)\nbperm\n\n\n    Monte-Carlo simulation of Geary C\n\ndata:  hunan_sf$GDPPC \nweights: rswm_q  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.69072, observed rank = 1, p-value = 0.001\nalternative hypothesis: greater\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nC = 0.69072, indicates that regions with similar GDPPC values are more likely to located near each other(clustered).\np-value = 0.001, indicates we can reject the null hypothesis(random)."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#visualising-the-monte-carlo-gearys-c",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#visualising-the-monte-carlo-gearys-c",
    "title": "Hands-on Exercise 06: Global/Local Measures of Spatial Autocorrelation",
    "section": "6.3 Visualising the Monte Carlo Geary’s C",
    "text": "6.3 Visualising the Monte Carlo Geary’s C\nplot a histogram to reveal the distribution of the simulated values.\n\nmean(bperm$res[1:999])\n\n[1] 1.004402\n\n\n\nvar(bperm$res[1:999])\n\n[1] 0.007436493\n\n\n\nsummary(bperm$res[1:999])\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.7142  0.9502  1.0052  1.0044  1.0595  1.2722 \n\n\n\nhist(bperm$res, freq=TRUE, breaks=20, xlab=\"Simulated Geary c\")\nabline(v=1, col=\"red\") \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nThe histogram appear to follow a normal distribution.\n\n\n\n\nbperm_df &lt;- data.frame(simulated_morans_I = bperm$res)\n\nggplot(bperm_df, aes(x = simulated_morans_I)) +\n  geom_histogram(binwidth = diff(range(bperm_df$simulated_morans_I)) / 20, \n                 fill = \"grey\", color = \"black\") + \n  geom_vline(xintercept = 1, color = \"red\", linetype = \"solid\", size = 1) +\n  labs(x = \"Simulated Geary C\", \n       y = \"Frequency\", \n       title = \"Histogram of Simulated Geary C Values\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#compute-morans-i-correlogram",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#compute-morans-i-correlogram",
    "title": "Hands-on Exercise 06: Global/Local Measures of Spatial Autocorrelation",
    "section": "7.1 Compute Moran’s I correlogram",
    "text": "7.1 Compute Moran’s I correlogram\nUsing sp.correlogram() of spdep package is used to compute a 6-lag spatial correlogram of GDPPC. The global spatial autocorrelation used in Moran’s I.\n\nMI_corr &lt;- sp.correlogram(wm_q, \n                          hunan_sf$GDPPC, \n                          order=6, \n                          method=\"I\", \n                          style=\"W\")\nplot(MI_corr)\n\n\n\n\n\n\n\n\nBy plotting the output might not allow us to provide complete interpretation. This is because not all autocorrelation values are statistically significant. Hence, it is important for us to examine the full analysis report by printing out the analysis results as in the code chunk below.\n\nprint(MI_corr)\n\nSpatial correlogram for hunan_sf$GDPPC \nmethod: Moran's I\n         estimate expectation   variance standard deviate Pr(I) two sided    \n1 (88)  0.3007500  -0.0114943  0.0043484           4.7351       2.189e-06 ***\n2 (88)  0.2060084  -0.0114943  0.0020962           4.7505       2.029e-06 ***\n3 (88)  0.0668273  -0.0114943  0.0014602           2.0496        0.040400 *  \n4 (88)  0.0299470  -0.0114943  0.0011717           1.2107        0.226015    \n5 (88) -0.1530471  -0.0114943  0.0012440          -4.0134       5.984e-05 ***\n6 (88) -0.1187070  -0.0114943  0.0016791          -2.6164        0.008886 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nLag 1 and 2: with I value 0.3 and 0.2, indicates strong positive spatial autocorrelation , means the regions are similar GDPPC for the Lag 1 and 2. (clustering)\nL3 and 4, with I value 0.06 and 0.02, indicates a weaker positive spatial autocorrelation compare with lag 1 and 2. (Notice the p-value also increased)\nL5 and 6, with I value -0.15 and -0.11, indicates regions with dissimilar GDPPC are more likely to be neighbors. (dispersed)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#compute-gearys-c-correlogram-and-plot",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#compute-gearys-c-correlogram-and-plot",
    "title": "Hands-on Exercise 06: Global/Local Measures of Spatial Autocorrelation",
    "section": "7.2 Compute Geary’s C correlogram and plot",
    "text": "7.2 Compute Geary’s C correlogram and plot\nusing sp.correlogram() of spdep package is used to compute a 6-lag spatial correlogram of GDPPC. The global spatial autocorrelation used in Geary’s C.\n\nGC_corr &lt;- sp.correlogram(wm_q, \n                          hunan_sf$GDPPC, \n                          order=6, \n                          method=\"C\", \n                          style=\"W\")\nplot(GC_corr)\n\n\n\n\n\n\n\n\n\nprint(GC_corr)\n\nSpatial correlogram for hunan_sf$GDPPC \nmethod: Geary's C\n        estimate expectation  variance standard deviate Pr(I) two sided    \n1 (88) 0.6907223   1.0000000 0.0073364          -3.6108       0.0003052 ***\n2 (88) 0.7630197   1.0000000 0.0049126          -3.3811       0.0007220 ***\n3 (88) 0.9397299   1.0000000 0.0049005          -0.8610       0.3892612    \n4 (88) 1.0098462   1.0000000 0.0039631           0.1564       0.8757128    \n5 (88) 1.2008204   1.0000000 0.0035568           3.3673       0.0007592 ***\n6 (88) 1.0773386   1.0000000 0.0058042           1.0151       0.3100407    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nLag 1 and 2, with C value of 0.69 and 0.75, meaning that nearby regions tend to habe similar GDPPC values(clustered).\nLag 3 , 4 and 6, with C value of 0.93 ,1.009 and 1.07, very close to 1, meaning no significant spatial autocorrelation at lag 3 and 4.\nLag 5, with C value of 1.2 meaning neighboring regions are more liekly to have dissimilar GDPPC value."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#computing-contiguity-spatial-weights-1",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#computing-contiguity-spatial-weights-1",
    "title": "Hands-on Exercise 06: Global/Local Measures of Spatial Autocorrelation",
    "section": "8.1 Computing Contiguity Spatial Weights",
    "text": "8.1 Computing Contiguity Spatial Weights\nBefore we can compute the local spatial autocorrelation statistics, we need to construct a spatial weights of the study area. The spatial weights is used to define the neighbourhood relationships between the geographical units (i.e. county) in the study area.\nBelow code chunk, we are using poly2nb of spdep package to compute contiguity weight matrices for the study area. This function builds a neighbours list based on regions with contiguous boundaries. If you look at the documentation you will see that you can pass a “queen” argument that takes TRUE or FALSE as options. If you do not specify this argument the default is set to TRUE, that is, if you don’t specify queen = FALSE this function will return a list of first order neighbours using the Queen criteria.\nTo perform Queen contiguity weight matrix.\n\nwm_q &lt;- poly2nb(hunan_sf, \n                queen=TRUE)\nsummary(wm_q)\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 11 \n 2  2 12 16 24 14 11  4  2  1 \n2 least connected regions:\n30 65 with 1 link\n1 most connected region:\n85 with 11 links\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe summary report above shows that there are 88 area units in Hunan. The most connected area unit has 11 neighbours. There are two area units with only one neighbours."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#row-standardised-weights-matrix-1",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#row-standardised-weights-matrix-1",
    "title": "Hands-on Exercise 06: Global/Local Measures of Spatial Autocorrelation",
    "section": "8.2 Row-Standardised Weights Matrix",
    "text": "8.2 Row-Standardised Weights Matrix\nNext, we need to assign weights to each neighboring polygon. In our case, each neighboring polygon will be assigned equal weight (style=“W”). This is accomplished by assigning the fraction 1/(#ofneighbors) to each neighboring county then summing the weighted income values. While this is the most intuitive way to summaries the neighbors’ values it has one drawback in that polygons along the edges of the study area will base their lagged values on fewer polygons thus potentially over- or under-estimating the true nature of the spatial autocorrelation in the data. For this example, we’ll stick with the style=“W” option for simplicity’s sake but note that other more robust options are available, notably style=“B”.\n\nrswm_q &lt;- nb2listw(wm_q, \n                   style=\"W\", \n                   zero.policy = TRUE)\nrswm_q\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: W \nWeights constants summary:\n   n   nn S0       S1       S2\nW 88 7744 88 37.86334 365.9147"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#computing-local-morans-i",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#computing-local-morans-i",
    "title": "Hands-on Exercise 06: Global/Local Measures of Spatial Autocorrelation",
    "section": "8.3 Computing Local Moran’s I",
    "text": "8.3 Computing Local Moran’s I\nUsing localmoran() function of spdep package. It computes li values, given a set of zi value and a listw object providing neightbour weighting information for the polygon associated with the zi value\n\nfips &lt;- order(hunan_sf$County)\nlocalMI &lt;- localmoran(hunan_sf$GDPPC, rswm_q)\nhead(localMI)\n\n            Ii          E.Ii       Var.Ii        Z.Ii Pr(z != E(Ii))\n1 -0.001468468 -2.815006e-05 4.723841e-04 -0.06626904      0.9471636\n2  0.025878173 -6.061953e-04 1.016664e-02  0.26266425      0.7928094\n3 -0.011987646 -5.366648e-03 1.133362e-01 -0.01966705      0.9843090\n4  0.001022468 -2.404783e-07 5.105969e-06  0.45259801      0.6508382\n5  0.014814881 -6.829362e-05 1.449949e-03  0.39085814      0.6959021\n6 -0.038793829 -3.860263e-04 6.475559e-03 -0.47728835      0.6331568\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nIi: the local Moran’s I statistics\nE.Ii: the expectation of local moran statistic under the randomisation hypothesis\nVar.Ii: the variance of local moran statistic under the randomisation hypothesis\nZ.Ii:the standard deviate of local moran statistic\nPr(): the p-value of local moran statistic\n\n\n\nTo list the content of the local Moran matrix using printCoefmat().\n\nprintCoefmat(data.frame(\n  localMI[fips,], \n  row.names=hunan_sf$County[fips]),\n  check.names=FALSE)\n\n                       Ii        E.Ii      Var.Ii        Z.Ii Pr.z....E.Ii..\nAnhua         -2.2493e-02 -5.0048e-03  5.8235e-02 -7.2467e-02         0.9422\nAnren         -3.9932e-01 -7.0111e-03  7.0348e-02 -1.4791e+00         0.1391\nAnxiang       -1.4685e-03 -2.8150e-05  4.7238e-04 -6.6269e-02         0.9472\nBaojing        3.4737e-01 -5.0089e-03  8.3636e-02  1.2185e+00         0.2230\nChaling        2.0559e-02 -9.6812e-04  2.7711e-02  1.2932e-01         0.8971\nChangning     -2.9868e-05 -9.0010e-09  1.5105e-07 -7.6828e-02         0.9388\nChangsha       4.9022e+00 -2.1348e-01  2.3194e+00  3.3590e+00         0.0008\nChengbu        7.3725e-01 -1.0534e-02  2.2132e-01  1.5895e+00         0.1119\nChenxi         1.4544e-01 -2.8156e-03  4.7116e-02  6.8299e-01         0.4946\nCili           7.3176e-02 -1.6747e-03  4.7902e-02  3.4200e-01         0.7324\nDao            2.1420e-01 -2.0824e-03  4.4123e-02  1.0297e+00         0.3032\nDongan         1.5210e-01 -6.3485e-04  1.3471e-02  1.3159e+00         0.1882\nDongkou        5.2918e-01 -6.4461e-03  1.0748e-01  1.6338e+00         0.1023\nFenghuang      1.8013e-01 -6.2832e-03  1.3257e-01  5.1198e-01         0.6087\nGuidong       -5.9160e-01 -1.3086e-02  3.7003e-01 -9.5104e-01         0.3416\nGuiyang        1.8240e-01 -3.6908e-03  3.2610e-02  1.0305e+00         0.3028\nGuzhang        2.8466e-01 -8.5054e-03  1.4152e-01  7.7931e-01         0.4358\nHanshou        2.5878e-02 -6.0620e-04  1.0167e-02  2.6266e-01         0.7928\nHengdong       9.9964e-03 -4.9063e-04  6.7742e-03  1.2742e-01         0.8986\nHengnan        2.8064e-02 -3.2160e-04  3.7597e-03  4.6294e-01         0.6434\nHengshan      -5.8201e-03 -3.0437e-05  5.1076e-04 -2.5618e-01         0.7978\nHengyang       6.2997e-02 -1.3046e-03  2.1865e-02  4.3486e-01         0.6637\nHongjiang      1.8790e-01 -2.3019e-03  3.1725e-02  1.0678e+00         0.2856\nHuarong       -1.5389e-02 -1.8667e-03  8.1030e-02 -4.7503e-02         0.9621\nHuayuan        8.3772e-02 -8.5569e-04  2.4495e-02  5.4072e-01         0.5887\nHuitong        2.5997e-01 -5.2447e-03  1.1077e-01  7.9685e-01         0.4255\nJiahe         -1.2431e-01 -3.0550e-03  5.1111e-02 -5.3633e-01         0.5917\nJianghua       2.8651e-01 -3.8280e-03  8.0968e-02  1.0204e+00         0.3076\nJiangyong      2.4337e-01 -2.7082e-03  1.1746e-01  7.1800e-01         0.4728\nJingzhou       1.8270e-01 -8.5106e-04  2.4363e-02  1.1759e+00         0.2396\nJinshi        -1.1988e-02 -5.3666e-03  1.1334e-01 -1.9667e-02         0.9843\nJishou        -2.8680e-01 -2.6305e-03  4.4028e-02 -1.3543e+00         0.1756\nLanshan        6.3334e-02 -9.6365e-04  2.0441e-02  4.4972e-01         0.6529\nLeiyang        1.1581e-02 -1.4948e-04  2.5082e-03  2.3422e-01         0.8148\nLengshuijiang -1.7903e+00 -8.2129e-02  2.1598e+00 -1.1623e+00         0.2451\nLi             1.0225e-03 -2.4048e-07  5.1060e-06  4.5260e-01         0.6508\nLianyuan      -1.4672e-01 -1.8983e-03  1.9145e-02 -1.0467e+00         0.2952\nLiling         1.3774e+00 -1.5097e-02  4.2601e-01  2.1335e+00         0.0329\nLinli          1.4815e-02 -6.8294e-05  1.4499e-03  3.9086e-01         0.6959\nLinwu         -2.4621e-03 -9.0703e-06  1.9258e-04 -1.7676e-01         0.8597\nLinxiang       6.5904e-02 -2.9028e-03  2.5470e-01  1.3634e-01         0.8916\nLiuyang        3.3688e+00 -7.7502e-02  1.5180e+00  2.7972e+00         0.0052\nLonghui        8.0801e-01 -1.1377e-02  1.5538e-01  2.0787e+00         0.0376\nLongshan       7.5663e-01 -1.1100e-02  3.1449e-01  1.3690e+00         0.1710\nLuxi           1.8177e-01 -2.4855e-03  3.4249e-02  9.9561e-01         0.3194\nMayang         2.1852e-01 -5.8773e-03  9.8049e-02  7.1663e-01         0.4736\nMiluo          1.8704e+00 -1.6927e-02  2.7925e-01  3.5715e+00         0.0004\nNan           -9.5789e-03 -4.9497e-04  6.8341e-03 -1.0988e-01         0.9125\nNingxiang      1.5607e+00 -7.3878e-02  8.0012e-01  1.8274e+00         0.0676\nNingyuan       2.0910e-01 -7.0884e-03  8.2306e-02  7.5356e-01         0.4511\nPingjiang     -9.8964e-01 -2.6457e-03  5.6027e-02 -4.1698e+00         0.0000\nQidong         1.1806e-01 -2.1207e-03  2.4747e-02  7.6396e-01         0.4449\nQiyang         6.1966e-02 -7.3374e-04  8.5743e-03  6.7712e-01         0.4983\nRucheng       -3.6992e-01 -8.8999e-03  2.5272e-01 -7.1814e-01         0.4727\nSangzhi        2.5053e-01 -4.9470e-03  6.8000e-02  9.7972e-01         0.3272\nShaodong      -3.2659e-02 -3.6592e-05  5.0546e-04 -1.4510e+00         0.1468\nShaoshan       2.1223e+00 -5.0227e-02  1.3668e+00  1.8583e+00         0.0631\nShaoyang       5.9499e-01 -1.1253e-02  1.3012e-01  1.6807e+00         0.0928\nShimen        -3.8794e-02 -3.8603e-04  6.4756e-03 -4.7729e-01         0.6332\nShuangfeng     9.2835e-03 -2.2867e-03  3.1516e-02  6.5174e-02         0.9480\nShuangpai      8.0591e-02 -3.1366e-04  8.9838e-03  8.5358e-01         0.3933\nSuining        3.7585e-01 -3.5933e-03  4.1870e-02  1.8544e+00         0.0637\nTaojiang      -2.5394e-01 -1.2395e-03  1.4477e-02 -2.1002e+00         0.0357\nTaoyuan        1.4729e-02 -1.2039e-04  8.5103e-04  5.0903e-01         0.6107\nTongdao        4.6482e-01 -6.9870e-03  1.9879e-01  1.0582e+00         0.2900\nWangcheng      4.4220e+00 -1.1067e-01  1.3596e+00  3.8873e+00         0.0001\nWugang         7.1003e-01 -7.8144e-03  1.0710e-01  2.1935e+00         0.0283\nXiangtan       2.4530e-01 -3.6457e-04  3.2319e-03  4.3213e+00         0.0000\nXiangxiang     2.6271e-01 -1.2703e-03  2.1290e-02  1.8092e+00         0.0704\nXiangyin       5.4525e-01 -4.7442e-03  7.9236e-02  1.9539e+00         0.0507\nXinhua         1.1810e-01 -6.2649e-03  8.6001e-02  4.2409e-01         0.6715\nXinhuang       1.5725e-01 -4.1820e-03  3.6648e-01  2.6667e-01         0.7897\nXinning        6.8928e-01 -9.6674e-03  2.0328e-01  1.5502e+00         0.1211\nXinshao        5.7578e-02 -8.5932e-03  1.1769e-01  1.9289e-01         0.8470\nXintian       -7.4050e-03 -5.1493e-03  1.0877e-01 -6.8395e-03         0.9945\nXupu           3.2406e-01 -5.7468e-03  5.7735e-02  1.3726e+00         0.1699\nYanling       -6.9021e-02 -5.9211e-04  9.9306e-03 -6.8667e-01         0.4923\nYizhang       -2.6844e-01 -2.2463e-03  4.7588e-02 -1.2202e+00         0.2224\nYongshun       6.3064e-01 -1.1350e-02  1.8830e-01  1.4795e+00         0.1390\nYongxing       4.3411e-01 -9.0735e-03  1.5088e-01  1.1409e+00         0.2539\nYou            7.8750e-02 -7.2728e-03  1.2116e-01  2.4714e-01         0.8048\nYuanjiang      2.0004e-04 -1.7760e-04  2.9798e-03  6.9181e-03         0.9945\nYuanling       8.7298e-03 -2.2981e-06  2.3221e-05  1.8121e+00         0.0700\nYueyang        4.1189e-02 -1.9768e-04  2.3113e-03  8.6085e-01         0.3893\nZhijiang       1.0476e-01 -7.8123e-04  1.3100e-02  9.2214e-01         0.3565\nZhongfang     -2.2685e-01 -2.1455e-03  3.5927e-02 -1.1855e+00         0.2358\nZhuzhou        3.2864e-01 -5.2432e-04  7.2391e-03  3.8688e+00         0.0001\nZixing        -7.6849e-01 -8.8210e-02  9.4057e-01 -7.0144e-01         0.4830\n\n\n\n8.3.1 Mapping the local Moran’s I\nBefore mapping the local Moran’s I map, it is wise to append the local Moran’s I dataframe (i.e. localMI) onto hunan_sf SpatialPolygonDataFrame. The code chunks below can be used to perform the task. The out SpatialPolygonDataFrame is called hunan_sf.localMI.\n\nhunan_sf.localMI &lt;- cbind(hunan_sf,localMI) %&gt;%\n  rename(Pr.Ii = Pr.z....E.Ii..)\n\n\n\n8.3.2 Mapping the local Moran’s I values\nPlot the local Moran’s I value (Ii field) using tmap package.\n\ntm_shape(hunan_sf.localMI) +\n  tm_fill(col = \"Ii\", \n          style = \"pretty\",\n          palette = \"RdBu\",\n          title = \"local moran statistics\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n8.3.3 Mapping local Moran’s I p-values\nAs from the above plot, we can see there are both positive and negative Ii value. Hence, it will be useful to consider the p-values for each of these values as well.\n\ntm_shape(hunan_sf.localMI) +\n  tm_fill(col = \"Pr.Ii\", \n          breaks=c(-Inf, 0.001, 0.01, 0.05, 0.1, Inf),\n          palette=\"-Blues\", \n          title = \"local Moran's I p-values\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n8.3.4 Mapping both local Moran’s I values and p-values\nFor better interpretation, it is better to plot both plot side by side.\n\nlocalMI.map &lt;- tm_shape(hunan_sf.localMI) +\n  tm_fill(col = \"Ii\", \n          style = \"pretty\", \n          title = \"local moran statistics\") +\n  tm_borders(alpha = 0.5)\n\npvalue.map &lt;- tm_shape(hunan_sf.localMI) +\n  tm_fill(col = \"Pr.Ii\", \n          breaks=c(-Inf, 0.001, 0.01, 0.05, 0.1, Inf),\n          palette=\"-Blues\", \n          title = \"local Moran's I p-values\") +\n  tm_borders(alpha = 0.5)\n\ntmap_arrange(localMI.map, pvalue.map, asp=1, ncol=2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nNotice that some area, with negative Ii value and with very low p-value(&lt; 0.001). This indicates that the observed spatial relationship is unlikely due to random chance. So we can say these area are spatial outlier.\nHowever, there are also area with negative Ii value and with a high p-value(&gt; 0.1). This indicates that although the area might appear to be a spatial outlier, but the result is not statistically significant and it could have occurred by chance. Hence, no strong conclusion can be drawn about these areas."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#plotting-moran-scatter-plot",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#plotting-moran-scatter-plot",
    "title": "Hands-on Exercise 06: Global/Local Measures of Spatial Autocorrelation",
    "section": "9.1 Plotting Moran Scatter-plot",
    "text": "9.1 Plotting Moran Scatter-plot\nThe Moran scatterplot is an illustration of the relationship between the values of the chosen attribute at each location and the average value of the same attribute at neighboring locations.\nThe code chunk below plots the Moran scatterplot of GDPPC 2012 by using moran.plot() of spdep.\n\nnci &lt;- moran.plot(hunan_sf$GDPPC, rswm_q,\n                  labels=as.character(hunan_sf$County), \n                  xlab=\"GDPPC 2012\", \n                  ylab=\"Spatially Lag GDPPC 2012\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNotice that the plot is split in 4 quadrants. The top right corner belongs to areas that have high GDPPC and are surrounded by other areas that have the average level of GDPPC. This are the high-high locations in the lesson slide."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#plotting-moran-scatterplot-with-standardised-variable",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#plotting-moran-scatterplot-with-standardised-variable",
    "title": "Hands-on Exercise 06: Global/Local Measures of Spatial Autocorrelation",
    "section": "9.2 Plotting Moran scatterplot with standardised variable",
    "text": "9.2 Plotting Moran scatterplot with standardised variable\nFirst we will use scale() to centers and scales the variable. Here centering is done by subtracting the mean (omitting NAs) the corresponding columns, and scaling is done by dividing the (centered) variable by their standard deviations.\n\nhunan_sf$Z.GDPPC &lt;- scale(hunan_sf$GDPPC) %&gt;% \n  as.vector \n\nas.vector =&gt; to chance the data type into vector that will map neatly into dataframe.\n\nnci2 &lt;- moran.plot(hunan_sf$Z.GDPPC, rswm_q,\n                   labels=as.character(hunan_sf$County),\n                   xlab=\"z-GDPPC 2012\", \n                   ylab=\"Spatially Lag z-GDPPC 2012\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#preparing-lisa-map-classes",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#preparing-lisa-map-classes",
    "title": "Hands-on Exercise 06: Global/Local Measures of Spatial Autocorrelation",
    "section": "9.3 Preparing LISA map classes",
    "text": "9.3 Preparing LISA map classes\nThe code chunks below show the steps to prepare a LISA cluster map.\n\nquadrant &lt;- vector(mode=\"numeric\",length=nrow(localMI))\n\nNext, derives the spatially lagged variable of interest (i.e. GDPPC) and centers the spatially lagged variable around its mean.\n\nhunan_sf$lag_GDPPC &lt;- lag.listw(rswm_q, hunan_sf$GDPPC)\nDV &lt;- hunan_sf$lag_GDPPC - mean(hunan_sf$lag_GDPPC)     \n\nThis is follow by centering the local Moran’s around the mean.\n\nLM_I &lt;- localMI[,1] - mean(localMI[,1])    \n\nNext, we will set a statistical significance level for the local Moran.\n\nsignif &lt;- 0.05       \n\nThese four command lines define the low-low (1), low-high (2), high-low (3) and high-high (4) categories.\n\nquadrant[DV &lt;0 & LM_I&gt;0] &lt;- 1\nquadrant[DV &gt;0 & LM_I&lt;0] &lt;- 2\nquadrant[DV &lt;0 & LM_I&lt;0] &lt;- 3  \nquadrant[DV &gt;0 & LM_I&gt;0] &lt;- 4      \n\nLastly, places non-significant Moran in the category 0.\n\nquadrant[localMI[,5]&gt;signif] &lt;- 0\n\nIn fact, we can combined all the steps into one single code chunk as shown below:\n\nquadrant &lt;- vector(mode=\"numeric\",length=nrow(localMI))\nhunan_sf$lag_GDPPC &lt;- lag.listw(rswm_q, hunan_sf$GDPPC)\nDV &lt;- hunan_sf$lag_GDPPC - mean(hunan_sf$lag_GDPPC)     \nLM_I &lt;- localMI[,1]   \nsignif &lt;- 0.05       \nquadrant[DV &lt;0 & LM_I&gt;0] &lt;- 1\nquadrant[DV &gt;0 & LM_I&lt;0] &lt;- 2\nquadrant[DV &lt;0 & LM_I&lt;0] &lt;- 3  \nquadrant[DV &gt;0 & LM_I&gt;0] &lt;- 4    \nquadrant[localMI[,5]&gt;signif] &lt;- 0"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#plotting-lisa-map",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#plotting-lisa-map",
    "title": "Hands-on Exercise 06: Global/Local Measures of Spatial Autocorrelation",
    "section": "9.5 Plotting LISA map",
    "text": "9.5 Plotting LISA map\nTo plot the LISA map.\n\nhunan_sf.localMI$quadrant &lt;- quadrant\ncolors &lt;- c(\"#ffffff\", \"#2c7bb6\", \"#abd9e9\", \"#fdae61\", \"#d7191c\")\nclusters &lt;- c(\"insignificant\", \"low-low\", \"low-high\", \"high-low\", \"high-high\")\n\ntm_shape(hunan_sf.localMI) +\n  tm_fill(col = \"quadrant\", \n          style = \"cat\", \n          palette = colors[c(sort(unique(quadrant)))+1], \n          labels = clusters[c(sort(unique(quadrant)))+1],\n          popup.vars = c(\"\")) +\n  tm_view(set.zoom.limits = c(11,17)) +\n  tm_borders(alpha=0.5)\n\n\n\n\n\n\n\n\nFor effective interpretation, it is better to plot both the local Moran’s I values map and its corresponding p-values map next to each other.\nTo create such visualisation.\n\ngdppc &lt;- qtm(hunan_sf, \"GDPPC\")\n\nhunan_sf.localMI$quadrant &lt;- quadrant\ncolors &lt;- c(\"#ffffff\", \"#2c7bb6\", \"#abd9e9\", \"#fdae61\", \"#d7191c\")\nclusters &lt;- c(\"insignificant\", \"low-low\", \"low-high\", \"high-low\", \"high-high\")\n\nLISAmap &lt;- tm_shape(hunan_sf.localMI) +\n  tm_fill(col = \"quadrant\", \n          style = \"cat\", \n          palette = colors[c(sort(unique(quadrant)))+1], \n          labels = clusters[c(sort(unique(quadrant)))+1],\n          popup.vars = c(\"\")) +\n  tm_view(set.zoom.limits = c(11,17)) +\n  tm_borders(alpha=0.5)\n\ntmap_arrange(gdppc, LISAmap,\n             asp=1, ncol=2)\n\n\n\n\n\n\n\n\nWith I map and p-value map.\n\ntmap_arrange(localMI.map, pvalue.map,\n             asp=1, ncol=2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nNotice that the p-value map and the LISA have a strong co-relation, as for the areas that with low p-value. The LISA map will tend to have significant results(with color)."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#getis-and-ords-g-statistics",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#getis-and-ords-g-statistics",
    "title": "Hands-on Exercise 06: Global/Local Measures of Spatial Autocorrelation",
    "section": "10.1 Getis and Ord’s G-Statistics",
    "text": "10.1 Getis and Ord’s G-Statistics\nAn alternative spatial statistics to detect spatial anomalies is the Getis and Ord’s G-statistics (Getis and Ord, 1972; Ord and Getis, 1995). It looks at neighbours within a defined proximity to identify where either high or low values clutser spatially. Here, statistically significant hot-spots are recognised as areas of high values where other areas within a neighbourhood range also share high values too.\nThe analysis consists of three steps:\n\nDeriving spatial weight matrix\nComputing Gi statistics\nMapping Gi statistics"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#deriving-distance-based-weight-matrix",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#deriving-distance-based-weight-matrix",
    "title": "Hands-on Exercise 06: Global/Local Measures of Spatial Autocorrelation",
    "section": "10.2 Deriving distance-based weight matrix",
    "text": "10.2 Deriving distance-based weight matrix\nFirst, we need to define a new set of neighbours. Whist the spatial autocorrelation considered units which shared borders, for Getis-Ord we are defining neighbours based on distance.\nThere are two type of distance-based proximity matrix, they are:\n\nfixed distance weight matrix; and\nadaptive distance weight matrix.\n\n\n10.2.1 Deriving the centroid\nWe will need points to associate with each polygon before we can make our connectivity graph. It will be a little more complicated than just running st_centroid() on the sf object: us.bound. We need the coordinates in a separate data frame for this to work. To do this we will use a mapping function. The mapping function applies a given function to each element of a vector and returns a vector of the same length. Our input vector will be the geometry column of us.bound. Our function will be st_centroid(). We will be using map_dbl variation of map from the purrr package. For more documentation, check out map documentation\nTo get our longitude values we map the st_centroid() function over the geometry column of us.bound and access the longitude value through double bracket notation [[]] and 1. This allows us to get only the longitude, which is the first value in each centroid.\n\nlongitude &lt;- map_dbl(hunan_sf$geometry, ~st_centroid(.x)[[1]])\n\nWe do the same for latitude with one key difference. We access the second value per each centroid with [[2]].\n\nlatitude &lt;- map_dbl(hunan_sf$geometry, ~st_centroid(.x)[[2]])\n\nNow that we have latitude and longitude, we use cbind to put longitude and latitude into the same object.\n\ncoords &lt;- cbind(longitude, latitude)\n\n\n\n10.2.2 Determine the cut-off distance\nFirstly, we need to determine the upper limit for distance band by using the steps below:\n\nReturn a matrix with the indices of points belonging to the set of the k nearest neighbours of each other by using knearneigh() of spdep.\nConvert the knn object returned by knearneigh() into a neighbours list of class nb with a list of integer vectors containing neighbour region number ids by using knn2nb().\nReturn the length of neighbour relationship edges by using nbdists() of spdep. The function returns in the units of the coordinates if the coordinates are projected, in km otherwise.\nRemove the list structure of the returned object by using unlist().\n\n\n#coords &lt;- coordinates(hunan)\nk1 &lt;- knn2nb(knearneigh(coords))\nk1dists &lt;- unlist(nbdists(k1, coords, longlat = TRUE))\nsummary(k1dists)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  24.79   32.57   38.01   39.07   44.52   61.79 \n\n\n\n\n\n\n\n\nNote\n\n\n\nThe summary report shows that the largest first nearest neighbour distance is 61.79 km, so using this as the upper threshold gives certainty that all units will have at least one neighbour.\n\n\n\n\n10.2.3 Computing fixed distance weight matrix\nNow, we will compute the distance weight matrix by using dnearneigh() as shown in the code chunk below.\n\nwm_d62 &lt;- dnearneigh(coords, 0, 62, longlat = TRUE)\nwm_d62\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 324 \nPercentage nonzero weights: 4.183884 \nAverage number of links: 3.681818 \n\n\nNext, nb2listw() is used to convert the nb object into spatial weights object.\n\nwm62_lw &lt;- nb2listw(wm_d62, style = 'B')\nsummary(wm62_lw)\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 324 \nPercentage nonzero weights: 4.183884 \nAverage number of links: 3.681818 \nLink number distribution:\n\n 1  2  3  4  5  6 \n 6 15 14 26 20  7 \n6 least connected regions:\n6 15 30 32 56 65 with 1 link\n7 most connected regions:\n21 28 35 45 50 52 82 with 6 links\n\nWeights style: B \nWeights constants summary:\n   n   nn  S0  S1   S2\nB 88 7744 324 648 5440"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#computing-adaptive-distance-weight-matrix",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#computing-adaptive-distance-weight-matrix",
    "title": "Hands-on Exercise 06: Global/Local Measures of Spatial Autocorrelation",
    "section": "10.3 Computing adaptive distance weight matrix",
    "text": "10.3 Computing adaptive distance weight matrix\nOne of the characteristics of fixed distance weight matrix is that more densely settled areas (usually the urban areas) tend to have more neighbours and the less densely settled areas (usually the rural counties) tend to have lesser neighbours. Having many neighbours smoothes the neighbour relationship across more neighbours.\nIt is possible to control the numbers of neighbours directly using k-nearest neighbours, either accepting asymmetric neighbours or imposing symmetry as shown in the code chunk below.\n\nknn &lt;- knn2nb(knearneigh(coords, k=8))\nknn\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 704 \nPercentage nonzero weights: 9.090909 \nAverage number of links: 8 \nNon-symmetric neighbours list\n\n\nNext, nb2listw() is used to convert the nb object into spatial weights object\n\nknn_lw &lt;- nb2listw(knn, style = 'B')\nsummary(knn_lw)\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 704 \nPercentage nonzero weights: 9.090909 \nAverage number of links: 8 \nNon-symmetric neighbours list\nLink number distribution:\n\n 8 \n88 \n88 least connected regions:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 with 8 links\n88 most connected regions:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 with 8 links\n\nWeights style: B \nWeights constants summary:\n   n   nn  S0   S1    S2\nB 88 7744 704 1300 23014"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#gi-statistics-using-fixed-distance",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#gi-statistics-using-fixed-distance",
    "title": "Hands-on Exercise 06: Global/Local Measures of Spatial Autocorrelation",
    "section": "11.1 Gi statistics using fixed distance",
    "text": "11.1 Gi statistics using fixed distance\n\nfips &lt;- order(hunan_sf$County)\ngi.fixed &lt;- localG(hunan_sf$GDPPC, wm62_lw)\ngi.fixed\n\n [1]  0.436075843 -0.265505650 -0.073033665  0.413017033  0.273070579\n [6] -0.377510776  2.863898821  2.794350420  5.216125401  0.228236603\n[11]  0.951035346 -0.536334231  0.176761556  1.195564020 -0.033020610\n[16]  1.378081093 -0.585756761 -0.419680565  0.258805141  0.012056111\n[21] -0.145716531 -0.027158687 -0.318615290 -0.748946051 -0.961700582\n[26] -0.796851342 -1.033949773 -0.460979158 -0.885240161 -0.266671512\n[31] -0.886168613 -0.855476971 -0.922143185 -1.162328599  0.735582222\n[36] -0.003358489 -0.967459309 -1.259299080 -1.452256513 -1.540671121\n[41] -1.395011407 -1.681505286 -1.314110709 -0.767944457 -0.192889342\n[46]  2.720804542  1.809191360 -1.218469473 -0.511984469 -0.834546363\n[51] -0.908179070 -1.541081516 -1.192199867 -1.075080164 -1.631075961\n[56] -0.743472246  0.418842387  0.832943753 -0.710289083 -0.449718820\n[61] -0.493238743 -1.083386776  0.042979051  0.008596093  0.136337469\n[66]  2.203411744  2.690329952  4.453703219 -0.340842743 -0.129318589\n[71]  0.737806634 -1.246912658  0.666667559  1.088613505 -0.985792573\n[76]  1.233609606 -0.487196415  1.626174042 -1.060416797  0.425361422\n[81] -0.837897118 -0.314565243  0.371456331  4.424392623 -0.109566928\n[86]  1.364597995 -1.029658605 -0.718000620\nattr(,\"internals\")\n               Gi      E(Gi)        V(Gi)        Z(Gi) Pr(z != E(Gi))\n [1,] 0.064192949 0.05747126 2.375922e-04  0.436075843   6.627817e-01\n [2,] 0.042300020 0.04597701 1.917951e-04 -0.265505650   7.906200e-01\n [3,] 0.044961480 0.04597701 1.933486e-04 -0.073033665   9.417793e-01\n [4,] 0.039475779 0.03448276 1.461473e-04  0.413017033   6.795941e-01\n [5,] 0.049767939 0.04597701 1.927263e-04  0.273070579   7.847990e-01\n [6,] 0.008825335 0.01149425 4.998177e-05 -0.377510776   7.057941e-01\n [7,] 0.050807266 0.02298851 9.435398e-05  2.863898821   4.184617e-03\n [8,] 0.083966739 0.04597701 1.848292e-04  2.794350420   5.200409e-03\n [9,] 0.115751554 0.04597701 1.789361e-04  5.216125401   1.827045e-07\n[10,] 0.049115587 0.04597701 1.891013e-04  0.228236603   8.194623e-01\n[11,] 0.045819180 0.03448276 1.420884e-04  0.951035346   3.415864e-01\n[12,] 0.049183846 0.05747126 2.387633e-04 -0.536334231   5.917276e-01\n[13,] 0.048429181 0.04597701 1.924532e-04  0.176761556   8.596957e-01\n[14,] 0.034733752 0.02298851 9.651140e-05  1.195564020   2.318667e-01\n[15,] 0.011262043 0.01149425 4.945294e-05 -0.033020610   9.736582e-01\n[16,] 0.065131196 0.04597701 1.931870e-04  1.378081093   1.681783e-01\n[17,] 0.027587075 0.03448276 1.385862e-04 -0.585756761   5.580390e-01\n[18,] 0.029409313 0.03448276 1.461397e-04 -0.419680565   6.747188e-01\n[19,] 0.061466754 0.05747126 2.383385e-04  0.258805141   7.957856e-01\n[20,] 0.057656917 0.05747126 2.371303e-04  0.012056111   9.903808e-01\n[21,] 0.066518379 0.06896552 2.820326e-04 -0.145716531   8.841452e-01\n[22,] 0.045599896 0.04597701 1.928108e-04 -0.027158687   9.783332e-01\n[23,] 0.030646753 0.03448276 1.449523e-04 -0.318615290   7.500183e-01\n[24,] 0.035635552 0.04597701 1.906613e-04 -0.748946051   4.538897e-01\n[25,] 0.032606647 0.04597701 1.932888e-04 -0.961700582   3.362000e-01\n[26,] 0.035001352 0.04597701 1.897172e-04 -0.796851342   4.255374e-01\n[27,] 0.012746354 0.02298851 9.812587e-05 -1.033949773   3.011596e-01\n[28,] 0.061287917 0.06896552 2.773884e-04 -0.460979158   6.448136e-01\n[29,] 0.014277403 0.02298851 9.683314e-05 -0.885240161   3.760271e-01\n[30,] 0.009622875 0.01149425 4.924586e-05 -0.266671512   7.897221e-01\n[31,] 0.014258398 0.02298851 9.705244e-05 -0.886168613   3.755267e-01\n[32,] 0.005453443 0.01149425 4.986245e-05 -0.855476971   3.922871e-01\n[33,] 0.043283712 0.05747126 2.367109e-04 -0.922143185   3.564539e-01\n[34,] 0.020763514 0.03448276 1.393165e-04 -1.162328599   2.451020e-01\n[35,] 0.081261843 0.06896552 2.794398e-04  0.735582222   4.619850e-01\n[36,] 0.057419907 0.05747126 2.338437e-04 -0.003358489   9.973203e-01\n[37,] 0.013497133 0.02298851 9.624821e-05 -0.967459309   3.333145e-01\n[38,] 0.019289310 0.03448276 1.455643e-04 -1.259299080   2.079223e-01\n[39,] 0.025996272 0.04597701 1.892938e-04 -1.452256513   1.464303e-01\n[40,] 0.016092694 0.03448276 1.424776e-04 -1.540671121   1.233968e-01\n[41,] 0.035952614 0.05747126 2.379439e-04 -1.395011407   1.630124e-01\n[42,] 0.031690963 0.05747126 2.350604e-04 -1.681505286   9.266481e-02\n[43,] 0.018750079 0.03448276 1.433314e-04 -1.314110709   1.888090e-01\n[44,] 0.015449080 0.02298851 9.638666e-05 -0.767944457   4.425202e-01\n[45,] 0.065760689 0.06896552 2.760533e-04 -0.192889342   8.470456e-01\n[46,] 0.098966900 0.05747126 2.326002e-04  2.720804542   6.512325e-03\n[47,] 0.085415780 0.05747126 2.385746e-04  1.809191360   7.042128e-02\n[48,] 0.038816536 0.05747126 2.343951e-04 -1.218469473   2.230456e-01\n[49,] 0.038931873 0.04597701 1.893501e-04 -0.511984469   6.086619e-01\n[50,] 0.055098610 0.06896552 2.760948e-04 -0.834546363   4.039732e-01\n[51,] 0.033405005 0.04597701 1.916312e-04 -0.908179070   3.637836e-01\n[52,] 0.043040784 0.06896552 2.829941e-04 -1.541081516   1.232969e-01\n[53,] 0.011297699 0.02298851 9.615920e-05 -1.192199867   2.331829e-01\n[54,] 0.040968457 0.05747126 2.356318e-04 -1.075080164   2.823388e-01\n[55,] 0.023629663 0.04597701 1.877170e-04 -1.631075961   1.028743e-01\n[56,] 0.006281129 0.01149425 4.916619e-05 -0.743472246   4.571958e-01\n[57,] 0.063918654 0.05747126 2.369553e-04  0.418842387   6.753313e-01\n[58,] 0.070325003 0.05747126 2.381374e-04  0.832943753   4.048765e-01\n[59,] 0.025947288 0.03448276 1.444058e-04 -0.710289083   4.775249e-01\n[60,] 0.039752578 0.04597701 1.915656e-04 -0.449718820   6.529132e-01\n[61,] 0.049934283 0.05747126 2.334965e-04 -0.493238743   6.218439e-01\n[62,] 0.030964195 0.04597701 1.920248e-04 -1.083386776   2.786368e-01\n[63,] 0.058129184 0.05747126 2.343319e-04  0.042979051   9.657182e-01\n[64,] 0.046096514 0.04597701 1.932637e-04  0.008596093   9.931414e-01\n[65,] 0.012459080 0.01149425 5.008051e-05  0.136337469   8.915545e-01\n[66,] 0.091447733 0.05747126 2.377744e-04  2.203411744   2.756574e-02\n[67,] 0.049575872 0.02298851 9.766513e-05  2.690329952   7.138140e-03\n[68,] 0.107907212 0.04597701 1.933581e-04  4.453703219   8.440175e-06\n[69,] 0.019616151 0.02298851 9.789454e-05 -0.340842743   7.332220e-01\n[70,] 0.032923393 0.03448276 1.454032e-04 -0.129318589   8.971056e-01\n[71,] 0.030317663 0.02298851 9.867859e-05  0.737806634   4.606320e-01\n[72,] 0.019437582 0.03448276 1.455870e-04 -1.246912658   2.124295e-01\n[73,] 0.055245460 0.04597701 1.932838e-04  0.666667559   5.049845e-01\n[74,] 0.074278054 0.05747126 2.383538e-04  1.088613505   2.763244e-01\n[75,] 0.013269580 0.02298851 9.719982e-05 -0.985792573   3.242349e-01\n[76,] 0.049407829 0.03448276 1.463785e-04  1.233609606   2.173484e-01\n[77,] 0.028605749 0.03448276 1.455139e-04 -0.487196415   6.261191e-01\n[78,] 0.039087662 0.02298851 9.801040e-05  1.626174042   1.039126e-01\n[79,] 0.031447120 0.04597701 1.877464e-04 -1.060416797   2.889550e-01\n[80,] 0.064005294 0.05747126 2.359641e-04  0.425361422   6.705732e-01\n[81,] 0.044606529 0.05747126 2.357330e-04 -0.837897118   4.020885e-01\n[82,] 0.063700493 0.06896552 2.801427e-04 -0.314565243   7.530918e-01\n[83,] 0.051142205 0.04597701 1.933560e-04  0.371456331   7.102977e-01\n[84,] 0.102121112 0.04597701 1.610278e-04  4.424392623   9.671399e-06\n[85,] 0.021901462 0.02298851 9.843172e-05 -0.109566928   9.127528e-01\n[86,] 0.064931813 0.04597701 1.929430e-04  1.364597995   1.723794e-01\n[87,] 0.031747344 0.04597701 1.909867e-04 -1.029658605   3.031703e-01\n[88,] 0.015893319 0.02298851 9.765131e-05 -0.718000620   4.727569e-01\nattr(,\"cluster\")\n [1] Low  Low  High High High High High High High Low  Low  High Low  Low  Low \n[16] High High High High Low  High High Low  Low  High Low  Low  Low  Low  Low \n[31] Low  Low  Low  High Low  Low  Low  Low  Low  Low  High Low  Low  Low  Low \n[46] High High Low  Low  Low  Low  High Low  Low  Low  Low  Low  High Low  Low \n[61] Low  Low  Low  High High High Low  High Low  Low  High Low  High High Low \n[76] High Low  Low  Low  Low  Low  Low  High High Low  High Low  Low \nLevels: Low High\nattr(,\"gstari\")\n[1] FALSE\nattr(,\"call\")\nlocalG(x = hunan_sf$GDPPC, listw = wm62_lw)\nattr(,\"class\")\n[1] \"localG\"\n\n\nThe output of localG() is a vector of G or Gstar values, with attributes “gstari” set to TRUE or FALSE, “call” set to the function call, and class “localG”.\nThe Gi statistics is represented as a Z-score. Greater values represent a greater intensity of clustering and the direction (positive or negative) indicates high or low clusters.\nNext, we will join the Gi values to their corresponding hunan sf data frame by using the code chunk below.\n\nhunan_sf.gi &lt;- cbind(hunan_sf, as.matrix(gi.fixed)) %&gt;%\n  rename(gstat_fixed = as.matrix.gi.fixed.)\n\nIn fact, the code chunk above performs three tasks. First, it convert the output vector (i.e. gi.fixed) into r matrix object by using as.matrix(). Next, cbind() is used to join hunan@data and gi.fixed matrix to produce a new SpatialPolygonDataFrame called hunan.gi. Lastly, the field name of the gi values is renamed to gstat_fixed by using rename()."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#mapping-gi-values-with-fixed-distance-weights",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#mapping-gi-values-with-fixed-distance-weights",
    "title": "Hands-on Exercise 06: Global/Local Measures of Spatial Autocorrelation",
    "section": "11.2 Mapping Gi values with fixed distance weights",
    "text": "11.2 Mapping Gi values with fixed distance weights\nThe code chunk below shows the functions used to map the Gi values derived using fixed distance weight matrix.\n\ngdppc &lt;- qtm(hunan_sf, \"GDPPC\")\n\nGimap &lt;-tm_shape(hunan_sf.gi) +\n  tm_fill(col = \"gstat_fixed\", \n          style = \"pretty\",\n          palette=\"-RdBu\",\n          title = \"local Gi\") +\n  tm_borders(alpha = 0.5)\n\ntmap_arrange(gdppc, Gimap, asp=1, ncol=2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nWe can see that the area with high GDPPC neighbours, will be relatively with a higher Gi value (hot spot). Where the area with a lower high GDPPC neighbours are most likely are plot as cold spot."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#gi-statistics-using-adaptive-distance",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#gi-statistics-using-adaptive-distance",
    "title": "Hands-on Exercise 06: Global/Local Measures of Spatial Autocorrelation",
    "section": "11.3 Gi statistics using adaptive distance",
    "text": "11.3 Gi statistics using adaptive distance\nThe code chunk below are used to compute the Gi values for GDPPC2012 by using an adaptive distance weight matrix (i.e knb_lw).\n\nfips &lt;- order(hunan_sf$County)\ngi.adaptive &lt;- localG(hunan_sf$GDPPC, knn_lw)\nhunan_sf.gi &lt;- cbind(hunan_sf, as.matrix(gi.adaptive)) %&gt;%\n  rename(gstat_adaptive = as.matrix.gi.adaptive.)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#mapping-gi-values-with-adaptive-distance-weights",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#mapping-gi-values-with-adaptive-distance-weights",
    "title": "Hands-on Exercise 06: Global/Local Measures of Spatial Autocorrelation",
    "section": "11.4 Mapping Gi values with adaptive distance weights",
    "text": "11.4 Mapping Gi values with adaptive distance weights\nTo visualise the locations of hot spot and cold spot areas. The choropleth mapping functions of tmap package will be used to map the Gi values.\nThe code chunk below shows the functions used to map the Gi values derived using fixed distance weight matrix.\n\ngdppc&lt;- qtm(hunan_sf, \"GDPPC\")\n\nGimap &lt;- tm_shape(hunan_sf.gi) + \n  tm_fill(col = \"gstat_adaptive\", \n          style = \"pretty\", \n          palette=\"-RdBu\", \n          title = \"local Gi\") + \n  tm_borders(alpha = 0.5)\n\ntmap_arrange(gdppc, \n             Gimap, \n             asp=1, \n             ncol=2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nWith adaptive distance weights, a larger area becomes a hot spot, while a smaller area becomes a cold spot compared to fixed distance weights."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html",
    "title": "Hands-On Exercise 02: Thematic Mapping and Geovisualisation with R",
    "section": "",
    "text": "Thematic mapping uses map symbols to visualize invisible geographic properties, such as population, temperature, crime rate, and property prices. Geovisualization enhances understanding by graphically representing places, phenomena, or processes, engaging the human eye-brain system for spatial cognition. This hands-on exercise will be on how to create accurate and functional choropleth maps using the tmap package in R."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#datasets",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#datasets",
    "title": "Hands-On Exercise 02: Thematic Mapping and Geovisualisation with R",
    "section": "3.1 Datasets",
    "text": "3.1 Datasets\nTwo data set will be used to create the choropleth map. They are:\n\nMaster Plan 2014 Subzone Boundary (Web) (i.e. MP14_SUBZONE_WEB_PL) in ESRI shapefile format. It can be downloaded at data.gov.sg This is a geospatial data. It consists of the geographical boundary of Singapore at the planning subzone level. The data is based on URA Master Plan 2014.\nSingapore Residents by Planning Area / Subzone, Age Group, Sex and Type of Dwelling, June 2011-2020 in csv format (i.e. respopagesextod2011to2020.csv). This is an aspatial data fie. It can be downloaded at Department of Statistics, Singapore Although it does not contain any coordinates values, but it’s PA and SZ fields can be used as unique identifiers to geocode to MP14_SUBZONE_WEB_PL shapefile."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#importing-geospatial-data-into-r",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#importing-geospatial-data-into-r",
    "title": "Hands-On Exercise 02: Thematic Mapping and Geovisualisation with R",
    "section": "3.2 Importing Geospatial Data into R",
    "text": "3.2 Importing Geospatial Data into R\nWe will using st_read() function from sf package to import shape file into R as a data frame named mpsz.\n\nmpsz &lt;- st_read(dsn = \"data/geospatial/MasterPlan2014SubzoneBoundaryWebSHP\", layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `/Users/mingwei/Desktop/SMU/Y3S1/IS415/xXxPMWxXx/IS415-GAA/Hands-on_Ex/Hands-on_Ex02/data/geospatial/MasterPlan2014SubzoneBoundaryWebSHP' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\nTo examine the content of mpsz:\n\nmpsz\n\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\nFirst 10 features:\n   OBJECTID SUBZONE_NO       SUBZONE_N SUBZONE_C CA_IND      PLN_AREA_N\n1         1          1    MARINA SOUTH    MSSZ01      Y    MARINA SOUTH\n2         2          1    PEARL'S HILL    OTSZ01      Y          OUTRAM\n3         3          3       BOAT QUAY    SRSZ03      Y SINGAPORE RIVER\n4         4          8  HENDERSON HILL    BMSZ08      N     BUKIT MERAH\n5         5          3         REDHILL    BMSZ03      N     BUKIT MERAH\n6         6          7  ALEXANDRA HILL    BMSZ07      N     BUKIT MERAH\n7         7          9   BUKIT HO SWEE    BMSZ09      N     BUKIT MERAH\n8         8          2     CLARKE QUAY    SRSZ02      Y SINGAPORE RIVER\n9         9         13 PASIR PANJANG 1    QTSZ13      N      QUEENSTOWN\n10       10          7       QUEENSWAY    QTSZ07      N      QUEENSTOWN\n   PLN_AREA_C       REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR\n1          MS CENTRAL REGION       CR 5ED7EB253F99252E 2014-12-05 31595.84\n2          OT CENTRAL REGION       CR 8C7149B9EB32EEFC 2014-12-05 28679.06\n3          SR CENTRAL REGION       CR C35FEFF02B13E0E5 2014-12-05 29654.96\n4          BM CENTRAL REGION       CR 3775D82C5DDBEFBD 2014-12-05 26782.83\n5          BM CENTRAL REGION       CR 85D9ABEF0A40678F 2014-12-05 26201.96\n6          BM CENTRAL REGION       CR 9D286521EF5E3B59 2014-12-05 25358.82\n7          BM CENTRAL REGION       CR 7839A8577144EFE2 2014-12-05 27680.06\n8          SR CENTRAL REGION       CR 48661DC0FBA09F7A 2014-12-05 29253.21\n9          QT CENTRAL REGION       CR 1F721290C421BFAB 2014-12-05 22077.34\n10         QT CENTRAL REGION       CR 3580D2AFFBEE914C 2014-12-05 24168.31\n     Y_ADDR SHAPE_Leng SHAPE_Area                       geometry\n1  29220.19   5267.381  1630379.3 MULTIPOLYGON (((31495.56 30...\n2  29782.05   3506.107   559816.2 MULTIPOLYGON (((29092.28 30...\n3  29974.66   1740.926   160807.5 MULTIPOLYGON (((29932.33 29...\n4  29933.77   3313.625   595428.9 MULTIPOLYGON (((27131.28 30...\n5  30005.70   2825.594   387429.4 MULTIPOLYGON (((26451.03 30...\n6  29991.38   4428.913  1030378.8 MULTIPOLYGON (((25899.7 297...\n7  30230.86   3275.312   551732.0 MULTIPOLYGON (((27746.95 30...\n8  30222.86   2208.619   290184.7 MULTIPOLYGON (((29351.26 29...\n9  29893.78   6571.323  1084792.3 MULTIPOLYGON (((20996.49 30...\n10 30104.18   3454.239   631644.3 MULTIPOLYGON (((24472.11 29...\n\n\n\n\n\n\n\n\nNote\n\n\n\nIt will only display the first 10 rows of the data frame, because in R by default, only a limited number of are displayed, often the first 10 rows only.\n\n\nTo display more rows, we can use functions like print() , head() or modifying R’s global settings (Tools &gt; Global &gt; Console).\n\nprint(mpsz, n = 20)  # This will print the first 20 rows\n\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\nFirst 20 features:\n   OBJECTID SUBZONE_NO               SUBZONE_N SUBZONE_C CA_IND\n1         1          1            MARINA SOUTH    MSSZ01      Y\n2         2          1            PEARL'S HILL    OTSZ01      Y\n3         3          3               BOAT QUAY    SRSZ03      Y\n4         4          8          HENDERSON HILL    BMSZ08      N\n5         5          3                 REDHILL    BMSZ03      N\n6         6          7          ALEXANDRA HILL    BMSZ07      N\n7         7          9           BUKIT HO SWEE    BMSZ09      N\n8         8          2             CLARKE QUAY    SRSZ02      Y\n9         9         13         PASIR PANJANG 1    QTSZ13      N\n10       10          7               QUEENSWAY    QTSZ07      N\n11       11         12              KENT RIDGE    QTSZ12      N\n12       12          6         ALEXANDRA NORTH    BMSZ06      N\n13       13          1             MARINA EAST    MESZ01      Y\n14       14          5        INSTITUTION HILL    RVSZ05      Y\n15       15          1          ROBERTSON QUAY    SRSZ01      Y\n16       16          1 JURONG ISLAND AND BUKOM    WISZ01      N\n17       17          3                  SUDONG    WISZ03      N\n18       18          2                 SEMAKAU    WISZ02      N\n19       19          2          SOUTHERN GROUP    SISZ02      N\n20       20          1                 SENTOSA    SISZ01      N\n         PLN_AREA_N PLN_AREA_C       REGION_N REGION_C          INC_CRC\n1      MARINA SOUTH         MS CENTRAL REGION       CR 5ED7EB253F99252E\n2            OUTRAM         OT CENTRAL REGION       CR 8C7149B9EB32EEFC\n3   SINGAPORE RIVER         SR CENTRAL REGION       CR C35FEFF02B13E0E5\n4       BUKIT MERAH         BM CENTRAL REGION       CR 3775D82C5DDBEFBD\n5       BUKIT MERAH         BM CENTRAL REGION       CR 85D9ABEF0A40678F\n6       BUKIT MERAH         BM CENTRAL REGION       CR 9D286521EF5E3B59\n7       BUKIT MERAH         BM CENTRAL REGION       CR 7839A8577144EFE2\n8   SINGAPORE RIVER         SR CENTRAL REGION       CR 48661DC0FBA09F7A\n9        QUEENSTOWN         QT CENTRAL REGION       CR 1F721290C421BFAB\n10       QUEENSTOWN         QT CENTRAL REGION       CR 3580D2AFFBEE914C\n11       QUEENSTOWN         QT CENTRAL REGION       CR 601BA309A1AAC731\n12      BUKIT MERAH         BM CENTRAL REGION       CR 4DC4BF8D86594CBF\n13      MARINA EAST         ME CENTRAL REGION       CR 782A2FAF53029A34\n14     RIVER VALLEY         RV CENTRAL REGION       CR C3C22D1EE31757BD\n15  SINGAPORE RIVER         SR CENTRAL REGION       CR DF71BB5EC3C9FFD1\n16  WESTERN ISLANDS         WI    WEST REGION       WR 699F7210FBF1AFA8\n17  WESTERN ISLANDS         WI    WEST REGION       WR F718C723E08FBD51\n18  WESTERN ISLANDS         WI    WEST REGION       WR E69207D4F76DEEA3\n19 SOUTHERN ISLANDS         SI CENTRAL REGION       CR 5809FC547293EA2D\n20 SOUTHERN ISLANDS         SI CENTRAL REGION       CR A6FCDC9C447952CB\n   FMEL_UPD_D   X_ADDR   Y_ADDR SHAPE_Leng SHAPE_Area\n1  2014-12-05 31595.84 29220.19   5267.381  1630379.3\n2  2014-12-05 28679.06 29782.05   3506.107   559816.2\n3  2014-12-05 29654.96 29974.66   1740.926   160807.5\n4  2014-12-05 26782.83 29933.77   3313.625   595428.9\n5  2014-12-05 26201.96 30005.70   2825.594   387429.4\n6  2014-12-05 25358.82 29991.38   4428.913  1030378.8\n7  2014-12-05 27680.06 30230.86   3275.312   551732.0\n8  2014-12-05 29253.21 30222.86   2208.619   290184.7\n9  2014-12-05 22077.34 29893.78   6571.323  1084792.3\n10 2014-12-05 24168.31 30104.18   3454.239   631644.3\n11 2014-12-05 23464.84 29725.37   7439.548  1826848.6\n12 2014-12-05 26548.25 30519.39   2907.051   293706.4\n13 2014-12-05 32344.05 30103.25   6470.950  1844060.7\n14 2014-12-05 28465.40 30711.22   2842.526   392563.3\n15 2014-12-05 28416.85 30409.36   4995.758   506589.0\n16 2014-12-05 13012.88 27225.87  68083.936 36707720.9\n17 2014-12-05 15931.76 19579.07  24759.066  4207271.1\n18 2014-12-05 21206.33 20465.81  18703.681  4963787.1\n19 2014-12-05 29815.09 23412.59  25626.977  2206319.5\n20 2014-12-05 27593.94 25813.35  17496.194  4919132.4\n                         geometry\n1  MULTIPOLYGON (((31495.56 30...\n2  MULTIPOLYGON (((29092.28 30...\n3  MULTIPOLYGON (((29932.33 29...\n4  MULTIPOLYGON (((27131.28 30...\n5  MULTIPOLYGON (((26451.03 30...\n6  MULTIPOLYGON (((25899.7 297...\n7  MULTIPOLYGON (((27746.95 30...\n8  MULTIPOLYGON (((29351.26 29...\n9  MULTIPOLYGON (((20996.49 30...\n10 MULTIPOLYGON (((24472.11 29...\n11 MULTIPOLYGON (((23332.77 30...\n12 MULTIPOLYGON (((26231.96 30...\n13 MULTIPOLYGON (((33214.62 29...\n14 MULTIPOLYGON (((28481.45 30...\n15 MULTIPOLYGON (((28087.34 30...\n16 MULTIPOLYGON (((14557.7 304...\n17 MULTIPOLYGON (((15772.59 21...\n18 MULTIPOLYGON (((19843.41 21...\n19 MULTIPOLYGON (((29712.51 23...\n20 MULTIPOLYGON (((26858.1 266...\n\nhead(mpsz, 10) # Default is 6 rows, constraint by the global setting(10 rows) as well\n\nSimple feature collection with 10 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 20660.53 ymin: 28369.47 xmax: 32362.39 ymax: 30684.55\nProjected CRS: SVY21\n   OBJECTID SUBZONE_NO       SUBZONE_N SUBZONE_C CA_IND      PLN_AREA_N\n1         1          1    MARINA SOUTH    MSSZ01      Y    MARINA SOUTH\n2         2          1    PEARL'S HILL    OTSZ01      Y          OUTRAM\n3         3          3       BOAT QUAY    SRSZ03      Y SINGAPORE RIVER\n4         4          8  HENDERSON HILL    BMSZ08      N     BUKIT MERAH\n5         5          3         REDHILL    BMSZ03      N     BUKIT MERAH\n6         6          7  ALEXANDRA HILL    BMSZ07      N     BUKIT MERAH\n7         7          9   BUKIT HO SWEE    BMSZ09      N     BUKIT MERAH\n8         8          2     CLARKE QUAY    SRSZ02      Y SINGAPORE RIVER\n9         9         13 PASIR PANJANG 1    QTSZ13      N      QUEENSTOWN\n10       10          7       QUEENSWAY    QTSZ07      N      QUEENSTOWN\n   PLN_AREA_C       REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR\n1          MS CENTRAL REGION       CR 5ED7EB253F99252E 2014-12-05 31595.84\n2          OT CENTRAL REGION       CR 8C7149B9EB32EEFC 2014-12-05 28679.06\n3          SR CENTRAL REGION       CR C35FEFF02B13E0E5 2014-12-05 29654.96\n4          BM CENTRAL REGION       CR 3775D82C5DDBEFBD 2014-12-05 26782.83\n5          BM CENTRAL REGION       CR 85D9ABEF0A40678F 2014-12-05 26201.96\n6          BM CENTRAL REGION       CR 9D286521EF5E3B59 2014-12-05 25358.82\n7          BM CENTRAL REGION       CR 7839A8577144EFE2 2014-12-05 27680.06\n8          SR CENTRAL REGION       CR 48661DC0FBA09F7A 2014-12-05 29253.21\n9          QT CENTRAL REGION       CR 1F721290C421BFAB 2014-12-05 22077.34\n10         QT CENTRAL REGION       CR 3580D2AFFBEE914C 2014-12-05 24168.31\n     Y_ADDR SHAPE_Leng SHAPE_Area                       geometry\n1  29220.19   5267.381  1630379.3 MULTIPOLYGON (((31495.56 30...\n2  29782.05   3506.107   559816.2 MULTIPOLYGON (((29092.28 30...\n3  29974.66   1740.926   160807.5 MULTIPOLYGON (((29932.33 29...\n4  29933.77   3313.625   595428.9 MULTIPOLYGON (((27131.28 30...\n5  30005.70   2825.594   387429.4 MULTIPOLYGON (((26451.03 30...\n6  29991.38   4428.913  1030378.8 MULTIPOLYGON (((25899.7 297...\n7  30230.86   3275.312   551732.0 MULTIPOLYGON (((27746.95 30...\n8  30222.86   2208.619   290184.7 MULTIPOLYGON (((29351.26 29...\n9  29893.78   6571.323  1084792.3 MULTIPOLYGON (((20996.49 30...\n10 30104.18   3454.239   631644.3 MULTIPOLYGON (((24472.11 29..."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#importing-attribute-data-into-r",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#importing-attribute-data-into-r",
    "title": "Hands-On Exercise 02: Thematic Mapping and Geovisualisation with R",
    "section": "3.3 Importing Attribute Data into R",
    "text": "3.3 Importing Attribute Data into R\nNext, we will use read_csv() function of readr package to import  respopagsex2011to2020.csv file into RStudio and save the file into an R dataframe called popdata.\n\npopdata &lt;- read_csv(\"data/aspatial/respopagesextod2011to2020.csv\")\n\nRows: 984656 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): PA, SZ, AG, Sex, TOD\ndbl (2): Pop, Time\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#data-preparation",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#data-preparation",
    "title": "Hands-On Exercise 02: Thematic Mapping and Geovisualisation with R",
    "section": "3.4 Data Preparation",
    "text": "3.4 Data Preparation\nBefore a thematic map can be prepared, we are required to prepare a data table with year 2020 values. The data table should include the variables PA, SZ, YOUNG, ECONOMY ACTIVE, AGED, TOTAL, DEPENDENCY.\n\nPA: Planning Area,\nSZ: Subzone,\nYOUNG: age group 0 to 4 until age group 20 to 24,\nECONOMY ACTIVE: age group 25-29 until age group 60-64,\nAGED: age group 65 and above,\nTOTAL: all age group, and\nDEPENDENCY: the ratio between young and aged against economy active group\n\n\n3.4.1 Data Wrangling\nThe following data wrangling and transformation functions will be used:\n\npivot_wider() of tidyr package, and\nmutate(), filter(), group_by() and select() of dplyr package\n\n\npopdata2020 &lt;- popdata %&gt;%\n  filter(Time == 2020) %&gt;%\n  group_by(PA, SZ, AG) %&gt;%\n  summarise(`POP` = sum(`Pop`)) %&gt;%\n  ungroup()%&gt;%\n  pivot_wider(names_from=AG, \n              values_from=POP) %&gt;%\n  mutate(YOUNG = rowSums(.[3:6])\n         +rowSums(.[14])) %&gt;%\nmutate(`ECONOMY ACTIVE` = rowSums(.[7:13])+\nrowSums(.[15]))%&gt;%\nmutate(`AGED`=rowSums(.[16:21])) %&gt;%\nmutate(`TOTAL`=rowSums(.[3:21])) %&gt;%  \nmutate(`DEPENDENCY` = (`YOUNG` + `AGED`)\n/`ECONOMY ACTIVE`) %&gt;%\n  select(`PA`, `SZ`, `YOUNG`, \n       `ECONOMY ACTIVE`, `AGED`, \n       `TOTAL`, `DEPENDENCY`)\n\n`summarise()` has grouped output by 'PA', 'SZ'. You can override using the\n`.groups` argument.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nYOUNG =&gt; instead of including col 12 (50_to_54), we should including col 14(5_to_9) from popdata.\nECONOMY ACTIVE =&gt; Instead of from col 7 to 11 and 13 to 15, it should be 7 to 13(until 55_to_59) and col 15(60_to_64)\n\n\n\n\n3.4.2 Joining the attribute data and geospatial data\nBefore we can perform the georelational join, we need to convert the values in PA and SZ fields to uppercase. This is because the values of PA and SZ fields are made up of upper- and lowercase. SUBZONE_N and PLN_AREA_N are in uppercase.\n\npopdata2020 &lt;- popdata2020 %&gt;%\n  mutate_at(.vars = vars(PA, SZ), \n          .funs = list(toupper)) %&gt;%\n  filter(`ECONOMY ACTIVE` &gt; 0)\n\nAfter convert PA and SZ field to uppercase, then we can use left_join() of dplyr  to join the geographical data and attribute table using planning subzone name e.g. SUBZONE_N and SZ as the common identifier.\n\nmpsz_pop2020 &lt;- left_join(mpsz, popdata2020, by = c(\"SUBZONE_N\" = \"SZ\"))\n\nThing to learn from the code chunk above:\n\nleft_join() of dplyr package is used with mpsz simple feature data frame as the left data table is to ensure that the output will be a simple features data frame.\n\n\nwrite_rds(mpsz_pop2020, \"data/rds/mpszpop2020.rds\")\n\n\n\n\n\n\n\nTip\n\n\n\nRDS files take less space than textual data formats like CSV, JSON(CSV file is about 64MB and RDS file only 1MB). RDS also reads and writes faster than those."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#plotting-a-choropleth-map-quickly-by-using-qtm",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#plotting-a-choropleth-map-quickly-by-using-qtm",
    "title": "Hands-On Exercise 02: Thematic Mapping and Geovisualisation with R",
    "section": "4.1 Plotting a choropleth map quickly by using qtm()",
    "text": "4.1 Plotting a choropleth map quickly by using qtm()\nThe easiest and quickest to draw a choropleth map using tmap is using qtm(). It is concise and provides a good default visualisation in many cases.\n\ntmap_mode(\"plot\")\n\ntmap mode set to plotting\n\nqtm(mpsz_pop2020, \n    fill = \"DEPENDENCY\")\n\n\n\n\n\n\n\n\n\ntmap_mode() with “plot” option is used to produce a static map. For interactive mode, “view” option should be used.\nfill argument is used to map the attribute (i.e. DEPENDENCY)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#creating-a-choropleth-map-by-using-tmaps-elements",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#creating-a-choropleth-map-by-using-tmaps-elements",
    "title": "Hands-On Exercise 02: Thematic Mapping and Geovisualisation with R",
    "section": "4.2 Creating a choropleth map by using tmap’s elements",
    "text": "4.2 Creating a choropleth map by using tmap’s elements\nDespite its usefulness of drawing a choropleth map quickly and easily, the disadvantge of qtm() is that it makes aesthetics of individual layers harder to control. To draw a high quality cartographic choropleth map as shown in the figure below, tmap’s drawing elements should be used.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"Dependency ratio\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\n\n\n\n\nNext, we will be using tmap functions that can be used to plot additional element in the map above.\n\n4.2.1 Drawing a base map using tm_shape()\nThe basic building block of tmap is tm_shape() followed by one or more layer elemments such as tm_fill() and tm_polygons().\ntm_shape() is used to define the input data (i.e mpsz_pop2020) and tm_polygons() is used to draw the planning subzone polygons\n\ntm_shape(mpsz_pop2020) + tm_polygons()\n\n\n\n\n\n\n\n\n\n\n4.2.2 Drawing a choropleth map using tm_polygons()\nTo draw a choropleth map showing the geographical distribution of a selected variable by planning subzone, we just need to assign the target variable such as Dependency to tm_polygons().\n\ntm_shape(mpsz_pop2020) + tm_polygons(\"DEPENDENCY\")\n\n\n\n\n\n\n\n\nThings to learn from tm_polygons():\n\nThe default interval binning used to draw the choropleth map is called “pretty”. A detailed discussion of the data classification methods supported by tmap will be provided in sub-section 4.3.\nThe default colour scheme used is YlOrRd of ColorBrewer. You will learn more about the color scheme in sub-section 4.4.\nBy default, Missing value will be shaded in grey.\n\n\n\n4.2.3 Drawing a choropleth map using tm_fill() and tm_border()\nActually, tm_polygons() is a wraper of tm_fill() and tm_border(). tm_fill() shades the polygons by using the default colour scheme and tm_borders() adds the borders of the shapefile onto the choropleth map.\n\ntm_shape(mpsz_pop2020) + tm_fill(\"DEPENDENCY\")\n\n\n\n\n\n\n\n\nNotice that the planning subzones are shared according to the respective dependecy values\nTo add the boundary of the planning subzones, tm_borders will be used as shown in the code chunk below.\n\ntm_shape(mpsz_pop2020) + tm_fill(\"DEPENDENCY\") + \n  tm_borders(col = \"blue\" ,lwd = 0.5, lty = \"longdash\",  alpha = 1)\n\n\n\n\n\n\n\n\nNotice that light-gray border lines have been added on the choropleth map.\n\n\n\n\n\n\nNote\n\n\n\nThe alpha argument is used to define transparency number between 0 (totally transparent) and 1 (not transparent). By default, the alpha value of the col is used (normally 1).\nBeside alpha argument, there are three other arguments for tm_borders(), they are:\n\ncol = border colour,\nlwd = border line width. The default is 1, and\nlty = border line type. The default is “solid”. \"blank\", \"solid\", \"dashed\", \"dotted\", \"dotdash\",\"longdash\", or \"twodash\", where \"blank\" uses ‘invisible lines’ (i.e., does not draw them)."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#data-classification-methods-of-tmap",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#data-classification-methods-of-tmap",
    "title": "Hands-On Exercise 02: Thematic Mapping and Geovisualisation with R",
    "section": "4.3 Data classification methods of tmap",
    "text": "4.3 Data classification methods of tmap\nMost choropleth maps employ some methods of data classification. The point of classification is to take a large number of observations and group them into data ranges or classes.\ntmap provides a total ten data classification methods, namely: fixed, sd, equal, pretty (default), quantile, kmeans, hclust, bclust, fisher, and jenks.\nTo define a data classification method, the style argument of tm_fill() or tm_polygons() will be used.\n\n4.3.1 Plotting choropleth maps with built-in classification methods\ntmap provides a total ten data classification methods:\n\nquantile,\nequal,\npretty (default),\nfixed,\nsd,\nkmeans,\nhclust,\nbclust,\nfisher, and\njenks.\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"quantile\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\nequal data classification method is used.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"equal\") +\n  tm_borders(col = \"red\" ,lwd = 0.5, alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe distribution of quantile data classification method are more evenly distributed then equal data classification method.\n\n\nOther data classification methods.\npretty(default)\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\nfixed\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"fixed\",\n          breaks = c(0, 0.5, 1, 1.5, 2, Inf)) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nFor style = “fixed”, we need to provide a set of fixed breaks manually. If breaks are not supplied, it leads to Error in if (lw == -Inf) lw &lt;- breaks[2] : argument is of length zero, as the function doesn’t know how to classify the data.\n*** With help from ChatGPT ***\n\n\nsd\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"sd\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\nkmeans\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"kmeans\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\nhclust\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"hclust\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\nbclust\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"bclust\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\nCommittee Member: 1(1) 2(1) 3(1) 4(1) 5(1) 6(1) 7(1) 8(1) 9(1) 10(1)\nComputing Hierarchical Clustering\n\n\nfisher\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"fisher\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\njenks\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"jenks\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\nExploring using the same classification methods with different numbers of classes(i.e. 2,6,10,20)\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 2,\n          palette = \"Dark2\", \n          style = \"kmeans\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 6,\n          palette = \"Dark2\", \n          style = \"kmeans\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 10,\n          palette = \"Dark2\", \n          style = \"kmeans\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 20,\n          palette = \"Dark2\", \n          style = \"kmeans\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nWhen n increase, there will be more classification, which will be able to provide more insightful information for us to analysis.\n\n\n\n\n4.3.2 Plotting choropleth map with custom break\nFor all the built-in styles, the category breaks are computed internally. In order to override these defaults, the breakpoints can be set explicitly by means of the breaks argument to the tm_fill(). It is important to note that, in tmap the breaks include a minimum and maximum. As a result, in order to end up with n categories, n+1 elements must be specified in the breaks option (the values must be in increasing order).\nBefore we get started, it is always a good practice to get some descriptive statistics on the variable before setting the break points. Code chunk below will be used to compute and display the descriptive statistics of DEPENDENCY field.\n\nsummary(mpsz_pop2020$DEPENDENCY)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n 0.0000  0.6519  0.7025  0.7742  0.7645 19.0000      92 \n\n\nWith reference to the results above, we set break point at 0.60, 0.70, 0.80, and 0.90. In addition, we also need to include a minimum and maximum, which we set at 0 and 100. Our breaks vector is thus c(0, 0.60, 0.70, 0.80, 0.90, 1.00)\nNow, we will plot the choropleth map\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          breaks = c(0, 0.60, 0.70, 0.80, 0.90, 1.00)) +\n  tm_borders(alpha = 0.5)\n\nWarning: Values have found that are higher than the highest break"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#colour-scheme",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#colour-scheme",
    "title": "Hands-On Exercise 02: Thematic Mapping and Geovisualisation with R",
    "section": "4.4 Colour Scheme",
    "text": "4.4 Colour Scheme\ntmap supports colour ramps either defined by the user or a set of predefined colour ramps from the RColorBrewer package.\n\n4.4.1 Using ColourBrewer palette\nTo change the colour, we assign the preferred colour to palette argument of tm_fill()\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 6,\n          style = \"quantile\",\n          palette = \"Blues\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\nTo reverse the colour shading, add a “-” prefix\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 6,\n          style = \"quantile\",\n          palette = \"-Blues\") +\n  tm_borders(alpha = 0.5)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#map-layouts",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#map-layouts",
    "title": "Hands-On Exercise 02: Thematic Mapping and Geovisualisation with R",
    "section": "4.5 Map Layouts",
    "text": "4.5 Map Layouts\nMap layout refers to the combination of all map elements into a cohensive map. Map elements include among others the objects to be mapped, the title, the scale bar, the compass, margins and aspects ratios. Colour settings and data classification methods covered in the previous section relate to the palette and break-points are used to affect how the map looks.\n\n4.5.1 Map Legend\nIn tmap, several legend options are provided to change the placement, format and appearance of the legend.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"jenks\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone \\n(Jenks classification)\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            legend.outside = FALSE,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n4.5.2 Map style\ntmap allows a wide variety of layout settings to be changed. They can be called by using tmap_style().\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"classic\")\n\ntmap style set to \"classic\"\n\n\nother available styles are: \"white\", \"gray\", \"natural\", \"cobalt\", \"col_blind\", \"albatross\", \"beaver\", \"bw\", \"watercolor\" \n\n\n\n\n\n\n\n\n\n\ntmap_style()\n\ncurrent tmap style is \"classic\"\n\n\nother available styles are: \"white\", \"gray\", \"natural\", \"cobalt\", \"col_blind\", \"albatross\", \"beaver\", \"bw\", \"watercolor\" \n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"cobalt\")\n\ntmap style set to \"cobalt\"\n\n\nother available styles are: \"white\", \"gray\", \"natural\", \"col_blind\", \"albatross\", \"beaver\", \"bw\", \"classic\", \"watercolor\" \n\n\n\n\n\n\n\n\n\n\n\n4.5.3 Cartographic Furniture\nBeside map style, tmap also also provides arguments to draw other map furniture such as compass, scale bar and grid lines.\nIn the code chunk below, tm_compass(), tm_scale_bar() and tm_grid() are used to add compass, scale bar and grid lines onto the choropleth map.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"No. of persons\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio \\nby planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar(width = 0.15) +\n  tm_grid(lwd = 0.1, alpha = 0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\n\n\n\n\nTo reset the default style\n\ntmap_style(\"white\")\n\ntmap style set to \"white\"\n\n\nother available styles are: \"gray\", \"natural\", \"cobalt\", \"col_blind\", \"albatross\", \"beaver\", \"bw\", \"classic\", \"watercolor\""
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#drawing-small-multiple-choropleth-maps",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#drawing-small-multiple-choropleth-maps",
    "title": "Hands-On Exercise 02: Thematic Mapping and Geovisualisation with R",
    "section": "4.6 Drawing Small Multiple Choropleth Maps",
    "text": "4.6 Drawing Small Multiple Choropleth Maps\nSmall multiple maps, also referred to as facet maps, are composed of many maps arrange side-by-side, and sometimes stacked vertically. Small multiple maps enable the visualisation of how spatial relationships change with respect to another variable, such as time.\nIn tmap, small multiple maps can be plotted in three ways:\n\nby assigning multiple values to at least one of the asthetic arguments,\nby defining a group-by variable in tm_facets(), and\nby creating multiple stand-alone maps with tmap_arrange().\n\n\n4.6.1 By assigning multiple values to at least one of the aesthetic arguments\nIn this example, small multiple choropleth maps are created by defining ncols in tm_fill()\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(c(\"YOUNG\", \"AGED\"),\n          style = \"equal\", \n          palette = \"plasma\") +\n  tm_layout(legend.position = c(\"right\", \"bottom\")) +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"white\")\n\ntmap style set to \"white\"\n\n\nother available styles are: \"gray\", \"natural\", \"cobalt\", \"col_blind\", \"albatross\", \"beaver\", \"bw\", \"classic\", \"watercolor\" \n\n\n\n\n\n\n\n\n\nIn this example, small multiple choropleth maps are created by assigning multiple values to at least one of the aesthetic arguments\n\ntm_shape(mpsz_pop2020)+ \n  tm_polygons(c(\"DEPENDENCY\",\"AGED\"),\n          style = c(\"equal\", \"quantile\"), \n          palette = list(\"Blues\",\"Greens\")) +\n  tm_layout(legend.position = c(\"right\", \"bottom\"))\n\n\n\n\n\n\n\n\n\n\n4.6.2 By defining a group-by variable in tm_facets()\nIn this example, multiple small choropleth maps are created by using tm_facets().\n\ntm_shape(mpsz_pop2020) +\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"Blues\",\n          thres.poly = 0) + \n  tm_facets(by=\"REGION_N\", \n            free.coords=TRUE, \n            drop.shapes=TRUE) +\n  tm_layout(legend.show = FALSE,\n            title.position = c(\"center\", \"center\"), \n            title.size = 20) +\n  tm_borders(alpha = 0.5)\n\nWarning: The argument drop.shapes has been renamed to drop.units, and is\ntherefore deprecated\n\n\n\n\n\n\n\n\n\n\n\n4.6.3 By creating multiple stand-alone maps with tmap_arrange()\nIn this example, multiple small choropleth maps are created by creating multiple stand-alone maps with tmap_arrange().\n\nyoungmap &lt;- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"YOUNG\", \n              style = \"quantile\", \n              palette = \"Blues\")\n\nagedmap &lt;- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"AGED\", \n              style = \"quantile\", \n              palette = \"Blues\")\n\ntmap_arrange(youngmap, agedmap, asp=1, ncol=2)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#mappping-spatial-object-meeting-a-selection-criterion",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#mappping-spatial-object-meeting-a-selection-criterion",
    "title": "Hands-On Exercise 02: Thematic Mapping and Geovisualisation with R",
    "section": "4.7 Mappping Spatial Object Meeting a Selection Criterion",
    "text": "4.7 Mappping Spatial Object Meeting a Selection Criterion\nInstead of creating small multiple choropleth map, you can also use selection funtion to map spatial objects meeting the selection criterion.\n\ntm_shape(mpsz_pop2020[mpsz_pop2020$REGION_N==\"CENTRAL REGION\", ])+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(legend.outside = TRUE,\n            legend.height = 0.45, \n            legend.width = 5.0,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)\n\nWarning in pre_process_gt(x, interactive = interactive, orig_crs =\ngm$shape.orig_crs): legend.width controls the width of the legend within a map.\nPlease use legend.outside.size to control the width of the outside legend"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#all-about-tmap-package",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#all-about-tmap-package",
    "title": "Hands-On Exercise 02: Thematic Mapping and Geovisualisation with R",
    "section": "5.1 All about tmap package",
    "text": "5.1 All about tmap package\n\ntmap: Thematic Maps in R\ntmap\ntmap: get started!\ntmap: changes in version 2.0\ntmap: creating thematic maps in a flexible way (useR!2015)\nExploring and presenting maps with tmap (useR!2017)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#geosptial-data-wrangling",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#geosptial-data-wrangling",
    "title": "Hands-On Exercise 02: Thematic Mapping and Geovisualisation with R",
    "section": "5.2 Geosptial data wrangling",
    "text": "5.2 Geosptial data wrangling\n\nsf: Simple Features for R\nSimple Features for R: StandardizedSupport for Spatial Vector Data\nReading, Writing and Converting Simple Features"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#data-wrangling-1",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#data-wrangling-1",
    "title": "Hands-On Exercise 02: Thematic Mapping and Geovisualisation with R",
    "section": "5.3 Data wrangling",
    "text": "5.3 Data wrangling\n\ndplyr\nTidy data\ntidyr: Easily Tidy Data with ‘spread()’ and ‘gather()’ Functions"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html",
    "title": "Hands-On Exercise 03: 1st and 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "",
    "text": "Spatial Point Pattern Analysis is the evaluation of the pattern or distribution, of a set of points on a surface. The point can be location of:\n\nevents such as crime, traffic accident and disease onset, or\nbusiness services (coffee and fastfood outlets) or facilities such as childcare and eldercare.\n\nUsing appropriate functions of spatstat, this hands-on exercise aims to discover the spatial point processes of childecare centres in Singapore.\nThe specific questions we would like to answer are as follows:\n\nare the childcare centres in Singapore randomly distributed throughout the country?\nif the answer is not, then the next logical question is where are the locations with higher concentration of childcare centres?"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#importing-the-spatial-data",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#importing-the-spatial-data",
    "title": "Hands-On Exercise 03: 1st and 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "4.1 Importing the spatial data",
    "text": "4.1 Importing the spatial data\nImporting childcare centres data set.\n\nchildcare_sf &lt;- st_read(\"data/child-care-services-geojson.geojson\") %&gt;%\n  st_transform(crs = 3414)\n\nReading layer `child-care-services-geojson' from data source \n  `/Users/mingwei/Desktop/SMU/Y3S1/IS415/xXxPMWxXx/IS415-GAA/Hands-on_Ex/Hands-on_Ex03/data/child-care-services-geojson.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 1545 features and 2 fields\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6824 ymin: 1.248403 xmax: 103.9897 ymax: 1.462134\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\n\n\n\nsg_sf &lt;- st_read(dsn = \"data\", layer=\"CostalOutline\")\n\nReading layer `CostalOutline' from data source \n  `/Users/mingwei/Desktop/SMU/Y3S1/IS415/xXxPMWxXx/IS415-GAA/Hands-on_Ex/Hands-on_Ex03/data' \n  using driver `ESRI Shapefile'\nSimple feature collection with 60 features and 4 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 2663.926 ymin: 16357.98 xmax: 56047.79 ymax: 50244.03\nProjected CRS: SVY21\n\n\n\nmpsz_sf &lt;- st_read(dsn = \"data\", \n                layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `/Users/mingwei/Desktop/SMU/Y3S1/IS415/xXxPMWxXx/IS415-GAA/Hands-on_Ex/Hands-on_Ex03/data' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\nRetrieve the referencing system information\n\nst_crs(childcare_sf)\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\n\n\nst_crs(sg_sf)\n\nCoordinate Reference System:\n  User input: SVY21 \n  wkt:\nPROJCRS[\"SVY21\",\n    BASEGEOGCRS[\"SVY21[WGS84]\",\n        DATUM[\"World Geodetic System 1984\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]],\n            ID[\"EPSG\",6326]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"Degree\",0.0174532925199433]]],\n    CONVERSION[\"unnamed\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]]]\n\n\nAs ID was EPSG:9001, which does not match the SVY21(EPSG:3414) Projected CRS, let’s assign the correct EPSG code to sg_sf.\n\nsg_sf &lt;- st_set_crs(sg_sf,3414)\n\n\nst_crs(mpsz_sf)\n\nCoordinate Reference System:\n  User input: SVY21 \n  wkt:\nPROJCRS[\"SVY21\",\n    BASEGEOGCRS[\"SVY21[WGS84]\",\n        DATUM[\"World Geodetic System 1984\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]],\n            ID[\"EPSG\",6326]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"Degree\",0.0174532925199433]]],\n    CONVERSION[\"unnamed\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]]]\n\n\nSame for mpsz_sf\n\nmpsz_sf &lt;- st_set_crs(mpsz_sf,3414)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#mapping-geospatial-data",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#mapping-geospatial-data",
    "title": "Hands-On Exercise 03: 1st and 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "4.2 Mapping Geospatial Data",
    "text": "4.2 Mapping Geospatial Data\nTo plot a map to show their spatial patterns.\n\ntm_shape(sg_sf) +\n  tm_polygons() +\ntm_shape(mpsz_sf) +\n  tm_polygons() +\ntm_shape(childcare_sf) +\n  tm_dots()\n\n\n\n\n\n\n\n\nSet tmap_mode to interactive map\n\ntmap_mode('view')\ntm_shape(childcare_sf)+\n  tm_dots()\n\n\n\n\n\nSet the tmap_mode back to static map\n\ntmap_mode('plot')\n\n\n\n\n\n\n\nTip\n\n\n\nAlways remember to switch back to plot mode after the interactive map. This is because, each interactive mode will consume a connection. You should also avoid displaying ecessive numbers of interactive maps (i.e. not more than 10) in one RMarkdown document when publish on Netlify."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#converting-sf-data-frames-to-sps-spatial-class",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#converting-sf-data-frames-to-sps-spatial-class",
    "title": "Hands-On Exercise 03: 1st and 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "5.1 Converting sf data frames to sp’s Spatial* class",
    "text": "5.1 Converting sf data frames to sp’s Spatial* class\nUsing as_Spatial() of sf package to convert the three geospatial data from simple feature data frame to sp’s Spatial* class.\n\nchildcare &lt;- as_Spatial(childcare_sf)\nmpsz &lt;- as_Spatial(mpsz_sf)\nsg &lt;- as_Spatial(sg_sf)\n\nDisplay information of the three Spatial* classes.\n\nsummary(childcare)\n\nObject of class SpatialPointsDataFrame\nCoordinates:\n               min      max\ncoords.x1 11203.01 45404.24\ncoords.x2 25667.60 49300.88\ncoords.x3     0.00     0.00\nIs projected: TRUE \nproj4string :\n[+proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1\n+x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0\n+units=m +no_defs]\nNumber of points: 1545\nData attributes:\n     Name           Description       \n Length:1545        Length:1545       \n Class :character   Class :character  \n Mode  :character   Mode  :character  \n\n\n\nsummary(mpsz)\n\nObject of class SpatialPolygonsDataFrame\nCoordinates:\n        min      max\nx  2667.538 56396.44\ny 15748.721 50256.33\nIs projected: TRUE \nproj4string :\n[+proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1\n+x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0\n+units=m +no_defs]\nData attributes:\n    OBJECTID       SUBZONE_NO      SUBZONE_N          SUBZONE_C        \n Min.   :  1.0   Min.   : 1.000   Length:323         Length:323        \n 1st Qu.: 81.5   1st Qu.: 2.000   Class :character   Class :character  \n Median :162.0   Median : 4.000   Mode  :character   Mode  :character  \n Mean   :162.0   Mean   : 4.625                                        \n 3rd Qu.:242.5   3rd Qu.: 6.500                                        \n Max.   :323.0   Max.   :17.000                                        \n    CA_IND           PLN_AREA_N         PLN_AREA_C          REGION_N        \n Length:323         Length:323         Length:323         Length:323        \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n   REGION_C           INC_CRC            FMEL_UPD_D             X_ADDR     \n Length:323         Length:323         Min.   :2014-12-05   Min.   : 5093  \n Class :character   Class :character   1st Qu.:2014-12-05   1st Qu.:21864  \n Mode  :character   Mode  :character   Median :2014-12-05   Median :28465  \n                                       Mean   :2014-12-05   Mean   :27257  \n                                       3rd Qu.:2014-12-05   3rd Qu.:31674  \n                                       Max.   :2014-12-05   Max.   :50425  \n     Y_ADDR        SHAPE_Leng        SHAPE_Area      \n Min.   :19579   Min.   :  871.5   Min.   :   39438  \n 1st Qu.:31776   1st Qu.: 3709.6   1st Qu.:  628261  \n Median :35113   Median : 5211.9   Median : 1229894  \n Mean   :36106   Mean   : 6524.4   Mean   : 2420882  \n 3rd Qu.:39869   3rd Qu.: 6942.6   3rd Qu.: 2106483  \n Max.   :49553   Max.   :68083.9   Max.   :69748299  \n\n\n\nsummary(sg)\n\nObject of class SpatialPolygonsDataFrame\nCoordinates:\n        min      max\nx  2663.926 56047.79\ny 16357.981 50244.03\nIs projected: TRUE \nproj4string :\n[+proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1\n+x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0\n+units=m +no_defs]\nData attributes:\n    GDO_GID          MSLINK          MAPID    COSTAL_NAM       \n Min.   : 1.00   Min.   : 1.00   Min.   :0   Length:60         \n 1st Qu.:15.75   1st Qu.:17.75   1st Qu.:0   Class :character  \n Median :30.50   Median :33.50   Median :0   Mode  :character  \n Mean   :30.50   Mean   :33.77   Mean   :0                     \n 3rd Qu.:45.25   3rd Qu.:49.25   3rd Qu.:0                     \n Max.   :60.00   Max.   :67.00   Max.   :0"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#converting-the-spatial-class-into-generic-sp-format",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#converting-the-spatial-class-into-generic-sp-format",
    "title": "Hands-On Exercise 03: 1st and 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "5.2 Converting the Spatial* class into generic sp format",
    "text": "5.2 Converting the Spatial* class into generic sp format\nspatstat requires the analytical data in ppp object form. There is no direct way to convert a Spatial* classes into ppp object. We need to convert the Spatial classes* into Spatial object first.\n\nchildcare_sp &lt;- as(childcare, \"SpatialPoints\")\nsg_sp &lt;- as(sg, \"SpatialPolygons\")\n\nDisplay the sp objects properties.\n\nchildcare_sp\n\nclass       : SpatialPoints \nfeatures    : 1545 \nextent      : 11203.01, 45404.24, 25667.6, 49300.88  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \n\n\n\nsg_sp\n\nclass       : SpatialPolygons \nfeatures    : 60 \nextent      : 2663.926, 56047.79, 16357.98, 50244.03  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \n\n\n\n\n\n\n\n\nImportant\n\n\n\nThe sp package provides a framework for spatial data handling, and the Spatial* classes are the concrete implementations within that framework, designed to work with specific spatial data types (points, lines, polygons, grids).\nAs can see from the data that, Spatial* class will hold more structured information than a generic sp object. Have one more data attributes field, where sp object does not have."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#converting-the-generic-sp-format-into-spatstats-ppp-format",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#converting-the-generic-sp-format-into-spatstats-ppp-format",
    "title": "Hands-On Exercise 03: 1st and 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "5.3 Converting the generic sp format into spatstat’s ppp format",
    "text": "5.3 Converting the generic sp format into spatstat’s ppp format\nNow,we will use as.ppp() function of spatstat to convert the spatial data into spatstat’s ppp object format.\n\nchildcare_ppp &lt;- as.ppp(st_coordinates(childcare_sf), st_bbox(childcare_sf)) \nchildcare_ppp\n\nMarked planar point pattern: 1545 points\nmarks are numeric, of storage type  'double'\nwindow: rectangle = [11203.01, 45404.24] x [25667.6, 49300.88] units\n\n\nNow, let us plot childcare_ppp and examine the different.\n\nplot(childcare_ppp)\n\n\n\n\n\n\n\n\n\nplot(childcare_ppp)\n\n\n\n\n\n\n\n\nTo take a quick look at the summary statistics of the newly created ppp object.\n\nsummary(childcare_ppp)\n\nMarked planar point pattern:  1545 points\nAverage intensity 1.91145e-06 points per square unit\n\n*Pattern contains duplicated points*\n\nCoordinates are given to 11 decimal places\n\nmarks are numeric, of type 'double'\nSummary:\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n      0       0       0       0       0       0 \n\nWindow: rectangle = [11203.01, 45404.24] x [25667.6, 49300.88] units\n                    (34200 x 23630 units)\nWindow area = 808287000 square units\n\n\n\n\n\n\n\n\nNote\n\n\n\nNotice the warning message about duplicates. In spatial point patterns analysis an issue of significant is the presence of duplicates. The statistical methodology used for spatial point patterns processes is based largely on the assumption that process are simple, that is, that the points cannot be coincident."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#handling-duplicated-points",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#handling-duplicated-points",
    "title": "Hands-On Exercise 03: 1st and 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "5.4 Handling duplicated points",
    "text": "5.4 Handling duplicated points\nTo check the duplication in a ppp object.\n\nany(duplicated(childcare_ppp))\n\n[1] TRUE\n\n\nTo count the number of co-indicence point, we will use the multiplicity() function.\n\nmultiplicity(childcare_ppp)\n\n   1    2    3    4    5    6    7    8    9   10   11   12   13   14   15   16 \n   1    1    1    3    1    1    1    1    2    1    1    1    1    1    1    1 \n  17   18   19   20   21   22   23   24   25   26   27   28   29   30   31   32 \n   1    1    1    1    1    1    1    1    1    1    9    1    1    1    1    1 \n  33   34   35   36   37   38   39   40   41   42   43   44   45   46   47   48 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n  49   50   51   52   53   54   55   56   57   58   59   60   61   62   63   64 \n   1    1    1    1    1    1    2    1    1    3    1    1    1    1    1    1 \n  65   66   67   68   69   70   71   72   73   74   75   76   77   78   79   80 \n   1    1    1    1    1    2    1    1    1    1    1    2    1    1    1    1 \n  81   82   83   84   85   86   87   88   89   90   91   92   93   94   95   96 \n   1    1    1    3    1    1    1    1    1    1    1    1    1    1    1    1 \n  97   98   99  100  101  102  103  104  105  106  107  108  109  110  111  112 \n   1    1    1    1    1    1    1    1    2    1    1    1    1    1    1    1 \n 113  114  115  116  117  118  119  120  121  122  123  124  125  126  127  128 \n   1    1    1    1    1    1    2    1    1    1    3    1    1    1    2    1 \n 129  130  131  132  133  134  135  136  137  138  139  140  141  142  143  144 \n   1    1    1    1    1    2    1    1    1    1    1    1    1    1    3    2 \n 145  146  147  148  149  150  151  152  153  154  155  156  157  158  159  160 \n   1    2    1    1    1    2    2    3    1    5    1    5    1    1    1    2 \n 161  162  163  164  165  166  167  168  169  170  171  172  173  174  175  176 \n   1    1    1    1    2    1    1    1    1    1    1    2    1    1    1    1 \n 177  178  179  180  181  182  183  184  185  186  187  188  189  190  191  192 \n   1    4    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 193  194  195  196  197  198  199  200  201  202  203  204  205  206  207  208 \n   1    1    1    1    1    2    2    1    1    1    1    2    1    4    1    1 \n 209  210  211  212  213  214  215  216  217  218  219  220  221  222  223  224 \n   2    1    1    1    1    1    1    1    1    1    1    1    2    1    1    1 \n 225  226  227  228  229  230  231  232  233  234  235  236  237  238  239  240 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 241  242  243  244  245  246  247  248  249  250  251  252  253  254  255  256 \n   1    1    1    1    2    1    1    1    1    1    1    1    1    1    1    1 \n 257  258  259  260  261  262  263  264  265  266  267  268  269  270  271  272 \n   1    1    1    1    1    1    1    1    1    1    2    1    1    1    1    3 \n 273  274  275  276  277  278  279  280  281  282  283  284  285  286  287  288 \n   1    1    1    1    1    1    3    1    1    1    1    1    1    1    1    1 \n 289  290  291  292  293  294  295  296  297  298  299  300  301  302  303  304 \n   1    1    1    1    1    1    1    9    1    1    2    1    1    1    1    1 \n 305  306  307  308  309  310  311  312  313  314  315  316  317  318  319  320 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 321  322  323  324  325  326  327  328  329  330  331  332  333  334  335  336 \n   1    1    1    5    1    1    1    1    1    2    1    1    2    2    1    1 \n 337  338  339  340  341  342  343  344  345  346  347  348  349  350  351  352 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    2    2    1 \n 353  354  355  356  357  358  359  360  361  362  363  364  365  366  367  368 \n   1    1    1    1    9    1    1    1    1    1    1    1    1    1    1    1 \n 369  370  371  372  373  374  375  376  377  378  379  380  381  382  383  384 \n   1    3    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 385  386  387  388  389  390  391  392  393  394  395  396  397  398  399  400 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 401  402  403  404  405  406  407  408  409  410  411  412  413  414  415  416 \n   1    1    2    1    1    1    1    1    1    1    2    1    1    1    1    1 \n 417  418  419  420  421  422  423  424  425  426  427  428  429  430  431  432 \n   1    1    1    1    1    1    1    2    1    1    2    1    1    1    1    1 \n 433  434  435  436  437  438  439  440  441  442  443  444  445  446  447  448 \n   1    1    1    1    2    1    1    1    1    1    1    1    1    1    1    1 \n 449  450  451  452  453  454  455  456  457  458  459  460  461  462  463  464 \n   1    1    9    9    1    1    1    1    1    1    1    1    1    1    2    1 \n 465  466  467  468  469  470  471  472  473  474  475  476  477  478  479  480 \n   2    1    1    1    1    1    1    1    1    1    1    1    2    2    1    1 \n 481  482  483  484  485  486  487  488  489  490  491  492  493  494  495  496 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 497  498  499  500  501  502  503  504  505  506  507  508  509  510  511  512 \n   1    1    1    1    1    1    2    1    1    1    1    1    1    1    1    2 \n 513  514  515  516  517  518  519  520  521  522  523  524  525  526  527  528 \n   1    1    1    1    1    1    1    1    1    1    1    2    1    1    3    1 \n 529  530  531  532  533  534  535  536  537  538  539  540  541  542  543  544 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 545  546  547  548  549  550  551  552  553  554  555  556  557  558  559  560 \n   1    1    1    1    1    1    1    1    1    3    1    1    1    1    1    1 \n 561  562  563  564  565  566  567  568  569  570  571  572  573  574  575  576 \n   2    2    2    1    1    1    1    2    1    1    2    1    1    1    2    1 \n 577  578  579  580  581  582  583  584  585  586  587  588  589  590  591  592 \n   1    2    1    1    1    1    1    9    1    4    1    2    1    1    1    1 \n 593  594  595  596  597  598  599  600  601  602  603  604  605  606  607  608 \n   2    1    1    1    1    1    1    1    2    1    2    1    1    1    1    1 \n 609  610  611  612  613  614  615  616  617  618  619  620  621  622  623  624 \n   1    1    1    1    1    1    1    1    1    2    1    2    1    1    1    1 \n 625  626  627  628  629  630  631  632  633  634  635  636  637  638  639  640 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 641  642  643  644  645  646  647  648  649  650  651  652  653  654  655  656 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    4 \n 657  658  659  660  661  662  663  664  665  666  667  668  669  670  671  672 \n   1    1    1    1    1    1    1    3    1    1    1    1    1    1    1    1 \n 673  674  675  676  677  678  679  680  681  682  683  684  685  686  687  688 \n   1    1    1    1    1    4    1    1    1    1    1    4    1    1    1    1 \n 689  690  691  692  693  694  695  696  697  698  699  700  701  702  703  704 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 705  706  707  708  709  710  711  712  713  714  715  716  717  718  719  720 \n   1    1    2    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 721  722  723  724  725  726  727  728  729  730  731  732  733  734  735  736 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 737  738  739  740  741  742  743  744  745  746  747  748  749  750  751  752 \n   1    2    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 753  754  755  756  757  758  759  760  761  762  763  764  765  766  767  768 \n   1    1    1    1    1    2    1    1    1    1    1    1    1    1    1    1 \n 769  770  771  772  773  774  775  776  777  778  779  780  781  782  783  784 \n   1    1    1    1    1    1    1    1    1    4    1    1    1    1    1    1 \n 785  786  787  788  789  790  791  792  793  794  795  796  797  798  799  800 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 801  802  803  804  805  806  807  808  809  810  811  812  813  814  815  816 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 817  818  819  820  821  822  823  824  825  826  827  828  829  830  831  832 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 833  834  835  836  837  838  839  840  841  842  843  844  845  846  847  848 \n   1    1    1    1    1    1    1    2    1    1    1    1    1    1    1    1 \n 849  850  851  852  853  854  855  856  857  858  859  860  861  862  863  864 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 865  866  867  868  869  870  871  872  873  874  875  876  877  878  879  880 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    2 \n 881  882  883  884  885  886  887  888  889  890  891  892  893  894  895  896 \n   3    1    1    1    2    1    1    1    3    1    1    3    1    1    1    1 \n 897  898  899  900  901  902  903  904  905  906  907  908  909  910  911  912 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 913  914  915  916  917  918  919  920  921  922  923  924  925  926  927  928 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 929  930  931  932  933  934  935  936  937  938  939  940  941  942  943  944 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 945  946  947  948  949  950  951  952  953  954  955  956  957  958  959  960 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    2 \n 961  962  963  964  965  966  967  968  969  970  971  972  973  974  975  976 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 977  978  979  980  981  982  983  984  985  986  987  988  989  990  991  992 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 993  994  995  996  997  998  999 1000 1001 1002 1003 1004 1005 1006 1007 1008 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 \n   1    1    1    1    1    1    1    1    1    2    2    1    1    1    1    1 \n1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 \n   1    1    1    1    1    2    1    1    1    1    1    1    1    1    1    1 \n1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 \n   1    1    1    1    1    1    1    1    2    2    1    1    1    5    1    1 \n1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 \n   1    1    1    1    1    1    1    1    1    2    1    1    1    1    1    1 \n1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 \n   1    1    1    1    1    1    1    1    1    1    2    1    1    1    1    1 \n1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 \n   1    9    1    2    2    1    1    1    2    1    1    1    1    1    1    1 \n1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 \n   1    1    1    1    2    1    1    1    3    1    1    1    1    1    1    1 \n1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 1214 1215 1216 \n   9    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 \n   1    1    1    2    1    1    1    1    1    1    1    1    1    1    1    1 \n1233 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 1292 1293 1294 1295 1296 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    2 \n1297 1298 1299 1300 1301 1302 1303 1304 1305 1306 1307 1308 1309 1310 1311 1312 \n   1    1    1    2    1    2    1    1    1    2    2    2    1    1    1    1 \n1313 1314 1315 1316 1317 1318 1319 1320 1321 1322 1323 1324 1325 1326 1327 1328 \n   1    1    2    1    1    1    1    1    1    1    1    1    2    1    1    1 \n1329 1330 1331 1332 1333 1334 1335 1336 1337 1338 1339 1340 1341 1342 1343 1344 \n   1    1    1    1    3    1    1    1    1    1    1    1    1    1    1    1 \n1345 1346 1347 1348 1349 1350 1351 1352 1353 1354 1355 1356 1357 1358 1359 1360 \n   1    1    1    1    1    1    1    1    4    1    1    1    1    1    2    1 \n1361 1362 1363 1364 1365 1366 1367 1368 1369 1370 1371 1372 1373 1374 1375 1376 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1377 1378 1379 1380 1381 1382 1383 1384 1385 1386 1387 1388 1389 1390 1391 1392 \n   1    1    1    1    1    1    1    1    1    9    1    1    1    1    1    1 \n1393 1394 1395 1396 1397 1398 1399 1400 1401 1402 1403 1404 1405 1406 1407 1408 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1409 1410 1411 1412 1413 1414 1415 1416 1417 1418 1419 1420 1421 1422 1423 1424 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1425 1426 1427 1428 1429 1430 1431 1432 1433 1434 1435 1436 1437 1438 1439 1440 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    2    1 \n1441 1442 1443 1444 1445 1446 1447 1448 1449 1450 1451 1452 1453 1454 1455 1456 \n   1    2    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1457 1458 1459 1460 1461 1462 1463 1464 1465 1466 1467 1468 1469 1470 1471 1472 \n   1    1    1    1    1    1    1    1    1    1    2    1    1    1    1    1 \n1473 1474 1475 1476 1477 1478 1479 1480 1481 1482 1483 1484 1485 1486 1487 1488 \n   1    1    1    1    1    1    2    1    1    1    1    1    1    1    1    1 \n1489 1490 1491 1492 1493 1494 1495 1496 1497 1498 1499 1500 1501 1502 1503 1504 \n   1    1    1    1    1    1    1    1    1    1    5    1    1    1    1    1 \n1505 1506 1507 1508 1509 1510 1511 1512 1513 1514 1515 1516 1517 1518 1519 1520 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1521 1522 1523 1524 1525 1526 1527 1528 1529 1530 1531 1532 1533 1534 1535 1536 \n   1    1    1    1    1    2    1    1    1    1    2    1    1    1    1    3 \n1537 1538 1539 1540 1541 1542 1543 1544 1545 \n   1    1    1    1    1    1    2    1    1 \n\n\nTo check how many locations have more than one point event.\n\nsum(multiplicity(childcare_ppp) &gt; 1)\n\n[1] 128\n\n\nTo view the locations of these duplicate point events.\n\ntmap_mode('view')\ntm_shape(childcare) +\n  tm_dots(alpha=0.4, \n          size=0.05)\n\n\n\n\n\nChanging the tmap mode back to static map.\n\ntmap_mode('plot')\n\nTo spot the duplicate points from the map shown above.\n\n# Extract coordinates from the SpatialPointsDataFrame\ncoordinates_df &lt;- coordinates(childcare)\n\n# Find the duplicate rows based on coordinates\nduplicate_rows &lt;- multiplicity(coordinates_df) &gt; 1\n\n# Check which rows are duplicates\nduplicates &lt;- childcare[duplicate_rows,]\n\n\nsummary(duplicate_rows)\n\n   Mode   FALSE    TRUE \nlogical    1417     128 \n\n\n\ntmap_mode('view')\ntm_shape(childcare) +\n  tm_dots(alpha = 0.4, size = 0.05, col = \"blue\") +\n  tm_shape(duplicates) +\n  tm_dots(alpha = 0.6, size = 0.1, col = \"red\")\n\n\n\n\n\n\ntmap_mode('plot')\n\nThere are three ways to overcome this problem. The easiest way is to delete the duplicates. But, that will also mean that some useful point events will be lost.\nThe second solution is use jittering, which will add a small perturbation to the duplicate points so that they do not occupy the exact same space.\nThe third solution is to make each point “unique” and then attach the duplicates of the points to the patterns as marks, as attributes of the points. Then you would need analytical techniques that take into account these marks.\nThe code chunk below implements the jittering approach.\n\nchildcare_ppp_jit &lt;- rjitter(childcare_ppp, \n                             retry=TRUE, \n                             nsim=1, \n                             drop=TRUE)\n\nTo check any deplicated point in this geospatial data.\n\nsum(multiplicity(childcare_ppp_jit) &gt; 1)\n\n[1] 0\n\n\n\nany(duplicated(childcare_ppp_jit))\n\n[1] FALSE"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#creating-owin-object",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#creating-owin-object",
    "title": "Hands-On Exercise 03: 1st and 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "5.5 Creating owin object",
    "text": "5.5 Creating owin object\nWhen analysing spatial point patterns, it is a good practice to confine the analysis with a geographical area like Singapore boundary. In spatstat, an object called owin is specially designed to represent this polygonal region.\nTo covert sg SpatialPolygon object into owin object of spatstat.\n\nsg_owin &lt;- as.owin(sg_sf)\n\nThe ouput object can be displayed by using plot() function.\n\nplot(sg_owin)\n\n\n\n\n\n\n\n\n\nsummary(sg_owin)\n\nWindow: polygonal boundary\n50 separate polygons (1 hole)\n                 vertices         area relative.area\npolygon 1 (hole)       30     -7081.18     -9.76e-06\npolygon 2              55     82537.90      1.14e-04\npolygon 3              90    415092.00      5.72e-04\npolygon 4              49     16698.60      2.30e-05\npolygon 5              38     24249.20      3.34e-05\npolygon 6             976  23344700.00      3.22e-02\npolygon 7             721   1927950.00      2.66e-03\npolygon 8            1992   9992170.00      1.38e-02\npolygon 9             330   1118960.00      1.54e-03\npolygon 10            175    925904.00      1.28e-03\npolygon 11            115    928394.00      1.28e-03\npolygon 12             24      6352.39      8.76e-06\npolygon 13            190    202489.00      2.79e-04\npolygon 14             37     10170.50      1.40e-05\npolygon 15             25     16622.70      2.29e-05\npolygon 16             10      2145.07      2.96e-06\npolygon 17             66     16184.10      2.23e-05\npolygon 18           5195 636837000.00      8.78e-01\npolygon 19             76    312332.00      4.31e-04\npolygon 20            627  31891300.00      4.40e-02\npolygon 21             20     32842.00      4.53e-05\npolygon 22             42     55831.70      7.70e-05\npolygon 23             67   1313540.00      1.81e-03\npolygon 24            734   4690930.00      6.47e-03\npolygon 25             16      3194.60      4.40e-06\npolygon 26             15      4872.96      6.72e-06\npolygon 27             15      4464.20      6.15e-06\npolygon 28             14      5466.74      7.54e-06\npolygon 29             37      5261.94      7.25e-06\npolygon 30            111    662927.00      9.14e-04\npolygon 31             69     56313.40      7.76e-05\npolygon 32            143    145139.00      2.00e-04\npolygon 33            397   2488210.00      3.43e-03\npolygon 34             90    115991.00      1.60e-04\npolygon 35             98     62682.90      8.64e-05\npolygon 36            165    338736.00      4.67e-04\npolygon 37            130     94046.50      1.30e-04\npolygon 38             93    430642.00      5.94e-04\npolygon 39             16      2010.46      2.77e-06\npolygon 40            415   3253840.00      4.49e-03\npolygon 41             30     10838.20      1.49e-05\npolygon 42             53     34400.30      4.74e-05\npolygon 43             26      8347.58      1.15e-05\npolygon 44             74     58223.40      8.03e-05\npolygon 45            327   2169210.00      2.99e-03\npolygon 46            177    467446.00      6.44e-04\npolygon 47             46    699702.00      9.65e-04\npolygon 48              6     16841.00      2.32e-05\npolygon 49             13     70087.30      9.66e-05\npolygon 50              4      9459.63      1.30e-05\nenclosing rectangle: [2663.93, 56047.79] x [16357.98, 50244.03] units\n                     (53380 x 33890 units)\nWindow area = 725376000 square units\nFraction of frame area: 0.401"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#combining-point-events-object-and-owin-object",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#combining-point-events-object-and-owin-object",
    "title": "Hands-On Exercise 03: 1st and 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "5.6 Combining point events object and owin object",
    "text": "5.6 Combining point events object and owin object\nIn this last step of geospatial data wrangling, we will extract childcare events that are located within Singapore by using the code chunk below.\n\n\nchildcareSG_ppp = childcare_ppp[sg_owin]\n\nThe output object combined both the point and polygon feature in one ppp object class as shown below.\n\nsummary(childcareSG_ppp)\n\nMarked planar point pattern:  1545 points\nAverage intensity 2.129929e-06 points per square unit\n\n*Pattern contains duplicated points*\n\nCoordinates are given to 11 decimal places\n\nmarks are numeric, of type 'double'\nSummary:\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n      0       0       0       0       0       0 \n\nWindow: polygonal boundary\n50 separate polygons (1 hole)\n                 vertices         area relative.area\npolygon 1 (hole)       30     -7081.18     -9.76e-06\npolygon 2              55     82537.90      1.14e-04\npolygon 3              90    415092.00      5.72e-04\npolygon 4              49     16698.60      2.30e-05\npolygon 5              38     24249.20      3.34e-05\npolygon 6             976  23344700.00      3.22e-02\npolygon 7             721   1927950.00      2.66e-03\npolygon 8            1992   9992170.00      1.38e-02\npolygon 9             330   1118960.00      1.54e-03\npolygon 10            175    925904.00      1.28e-03\npolygon 11            115    928394.00      1.28e-03\npolygon 12             24      6352.39      8.76e-06\npolygon 13            190    202489.00      2.79e-04\npolygon 14             37     10170.50      1.40e-05\npolygon 15             25     16622.70      2.29e-05\npolygon 16             10      2145.07      2.96e-06\npolygon 17             66     16184.10      2.23e-05\npolygon 18           5195 636837000.00      8.78e-01\npolygon 19             76    312332.00      4.31e-04\npolygon 20            627  31891300.00      4.40e-02\npolygon 21             20     32842.00      4.53e-05\npolygon 22             42     55831.70      7.70e-05\npolygon 23             67   1313540.00      1.81e-03\npolygon 24            734   4690930.00      6.47e-03\npolygon 25             16      3194.60      4.40e-06\npolygon 26             15      4872.96      6.72e-06\npolygon 27             15      4464.20      6.15e-06\npolygon 28             14      5466.74      7.54e-06\npolygon 29             37      5261.94      7.25e-06\npolygon 30            111    662927.00      9.14e-04\npolygon 31             69     56313.40      7.76e-05\npolygon 32            143    145139.00      2.00e-04\npolygon 33            397   2488210.00      3.43e-03\npolygon 34             90    115991.00      1.60e-04\npolygon 35             98     62682.90      8.64e-05\npolygon 36            165    338736.00      4.67e-04\npolygon 37            130     94046.50      1.30e-04\npolygon 38             93    430642.00      5.94e-04\npolygon 39             16      2010.46      2.77e-06\npolygon 40            415   3253840.00      4.49e-03\npolygon 41             30     10838.20      1.49e-05\npolygon 42             53     34400.30      4.74e-05\npolygon 43             26      8347.58      1.15e-05\npolygon 44             74     58223.40      8.03e-05\npolygon 45            327   2169210.00      2.99e-03\npolygon 46            177    467446.00      6.44e-04\npolygon 47             46    699702.00      9.65e-04\npolygon 48              6     16841.00      2.32e-05\npolygon 49             13     70087.30      9.66e-05\npolygon 50              4      9459.63      1.30e-05\nenclosing rectangle: [2663.93, 56047.79] x [16357.98, 50244.03] units\n                     (53380 x 33890 units)\nWindow area = 725376000 square units\nFraction of frame area: 0.401\n\n\n\nplot(childcareSG_ppp)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#kernel-density-estimation",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#kernel-density-estimation",
    "title": "Hands-On Exercise 03: 1st and 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "6.1 Kernel Density Estimation",
    "text": "6.1 Kernel Density Estimation\nIn this section, I will learn how to compute the kernel density estimation (KDE) of childcare services in Singapore.\n\n6.1.1 Computing kernel density estimation using automatic bandwidth selection method\nThe code chunk below computes a kernel density by using the following configurations of density() of spatstat:\n\nbw.diggle() automatic bandwidth selection method. Other recommended methods are bw.CvL(), bw.scott() or bw.ppl().\nThe smoothing kernel used is gaussian, which is the default. Other smoothing methods are: “epanechnikov”, “quartic” or “disc”.\nThe intensity estimate is corrected for edge effect bias by using method described by Jones (1993) and Diggle (2010, equation 18.9). The default is FALSE.\n\n\nkde_childcareSG_bw &lt;- density(childcareSG_ppp,\n                              sigma=bw.diggle,\n                              edge=TRUE,\n                            kernel=\"gaussian\") \n\nTo plot the kernel density derived.\n\nplot(kde_childcareSG_bw)\n\n\n\n\n\n\n\n\nThe density values of the output range is too small for us to derive any practical insights. The output range is due to the default unit of measurement of svy21 being in meter. As such, the density values computed is in “number of points per square meter”.\nWe can check the bandwidth used to compute the kde layer by using the following code:\n\nbw &lt;- bw.diggle(childcareSG_ppp)\nbw\n\n   sigma \n298.4095 \n\n\n\n\n6.1.2 Rescalling KDE values\nThe function rescale() is used to convert the unit of measurement from metres to kilometres.\n\nchildcareSG_ppp.km &lt;- rescale(childcareSG_ppp, 1000, \"km\")\n\nNow re-run density() using the resale data set and plot the output kde map.\n\nkde_childcareSG.bw &lt;- density(childcareSG_ppp.km, sigma=bw.diggle, edge=TRUE, kernel=\"gaussian\")\nplot(kde_childcareSG.bw)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe graph legend are better and more readible and comprehensible."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#working-with-different-automatic-badwidth-methods",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#working-with-different-automatic-badwidth-methods",
    "title": "Hands-On Exercise 03: 1st and 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "6.2 Working with different automatic badwidth methods",
    "text": "6.2 Working with different automatic badwidth methods\nBesides bw.diggle(), there are three other spatstat functions can be used to determine the bandwidth, they are: bw.CvL(), bw.scott(), and bw.ppl().\n\nbw.CvL(childcareSG_ppp.km)\n\n   sigma \n4.543278 \n\n\n\nbw.scott(childcareSG_ppp.km)\n\n sigma.x  sigma.y \n2.224898 1.450966 \n\n\n\nbw.diggle(childcareSG_ppp.km)\n\n    sigma \n0.2984095 \n\n\nBaddeley et al. (2016) suggested the use of bw.ppl() algorithm because in their experience, the algorithm tends to produce the more appropriate values when the pattern consists predominantly tight clusters. However, they also insist that if the purpose is to detect a single tight cluster in the midst of random noise then bw.diggle() is the best.\nComparing the output of using bw.diggle and bw.ppl methods,\n\nkde_childcareSG.ppl &lt;- density(childcareSG_ppp.km, \n                               sigma=bw.ppl, \n                               edge=TRUE,\n                               kernel=\"gaussian\")\npar(mfrow=c(1,2))\nplot(kde_childcareSG.bw, main = \"bw.diggle\")\nplot(kde_childcareSG.ppl, main = \"bw.ppl\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#working-with-different-kernel-methods",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#working-with-different-kernel-methods",
    "title": "Hands-On Exercise 03: 1st and 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "6.3 Working with different kernel methods",
    "text": "6.3 Working with different kernel methods\nThe default kernel method used in density.ppp() is gaussian. There are other options, namely epanechnikov, quartic and dics.\nThe code chunk below will be used to compute three more kernel density estimations by using these three kernel functions.\n\npar(mfrow = c(2, 2))\npar(mar = c(3, 3, 2, 1))  # adjust the margin values to resolve margin issue\n\nplot(density(childcareSG_ppp.km, sigma = bw.ppl, edge = TRUE, kernel = \"gaussian\"), main = \"Gaussian\")\nplot(density(childcareSG_ppp.km, sigma = bw.ppl, edge = TRUE, kernel = \"epanechnikov\"), main = \"Epanechnikov\")\nplot(density(childcareSG_ppp.km, sigma = bw.ppl, edge = TRUE, kernel = \"quartic\"), main = \"Quartic\")\nplot(density(childcareSG_ppp.km, sigma = bw.ppl, edge = TRUE, kernel = \"disc\"), main = \"Disc\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#computing-kde-by-using-fixed-bandwidth",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#computing-kde-by-using-fixed-bandwidth",
    "title": "Hands-On Exercise 03: 1st and 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "7.1 Computing KDE by using fixed bandwidth",
    "text": "7.1 Computing KDE by using fixed bandwidth\nCompute a KDE layer by defining a bandwidth of 600m. The sigma value depends on the unit of measurement in the kde variable. In our case, childcareSG_ppp.km object is in kilometres, hence, 600m will be 0.6km.\n\nkde_childcareSG_600 &lt;- density(childcareSG_ppp.km, sigma=0.6, edge=TRUE, kernel=\"gaussian\")\nplot(kde_childcareSG_600)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#computing-kde-by-using-adaptive-bandwidth",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#computing-kde-by-using-adaptive-bandwidth",
    "title": "Hands-On Exercise 03: 1st and 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "7.2 Computing KDE by using adaptive bandwidth",
    "text": "7.2 Computing KDE by using adaptive bandwidth\nFixed bandwidth method is very sensitive to highly skew distribution of spatial point patterns over geographical units for example urban versus rural. One way to overcome this problem is by using adaptive bandwidth instead.\nThis section derive s the adaptive kernel density estimation by using density.adaptive() of spatstat.\n\nkde_childcareSG_adaptive &lt;- adaptive.density(childcareSG_ppp.km, method=\"kernel\")\nplot(kde_childcareSG_adaptive)\n\n\n\n\n\n\n\n\nCompare the fixed and adaptive kernel density estimation outputs.\n\npar(mfrow=c(1,2))\nplot(kde_childcareSG.bw, main = \"Fixed bandwidth\")\nplot(kde_childcareSG_adaptive, main = \"Adaptive bandwidth\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#converting-kde-output-into-grid-object",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#converting-kde-output-into-grid-object",
    "title": "Hands-On Exercise 03: 1st and 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "7.3 Converting KDE output into grid object",
    "text": "7.3 Converting KDE output into grid object\nThe result is the same, we just convert it so that it is suitable for mapping purposes\n\n#gridded_kde_childcareSG_bw &lt;- as.SpatialGridDataFrame.im(kde_childcareSG.bw)\n#spplot(gridded_kde_childcareSG_bw)\n\nTo resolve the error.\nError in as.SpatialGridDataFrame.im(kde_childcareSG.bw) : \n  could not find function \"as.SpatialGridDataFrame.im\"\n\nlibrary(spatial)\nlibrary(sp)\n\n\ngridded_kde_childcareSG_bw &lt;- as(kde_childcareSG.bw, \"SpatialGridDataFrame\")\n\nspplot(gridded_kde_childcareSG_bw)\n\n\n\n\n\n\n\n\n\n7.3.1 Converting gridded output into raster\nNext, we will convert the gridded kernal density objects into RasterLayer object by using raster() of rasterpackage.\n\nkde_childcareSG_bw_raster &lt;- raster(kde_childcareSG.bw)\n\nThen observe the properties of the raster layer,\n\nkde_childcareSG_bw_raster\n\nclass      : RasterLayer \ndimensions : 128, 128, 16384  (nrow, ncol, ncell)\nresolution : 0.4170614, 0.2647348  (x, y)\nextent     : 2.663926, 56.04779, 16.35798, 50.24403  (xmin, xmax, ymin, ymax)\ncrs        : NA \nsource     : memory\nnames      : layer \nvalues     : -8.489773e-15, 28.51831  (min, max)\n\n\n\n\n\n\n\n\nNote\n\n\n\nRaster layer does not have any CRS property, thus its value is NA after the grid (vector) is rasterised."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#visualising-the-output-in-tmap",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#visualising-the-output-in-tmap",
    "title": "Hands-On Exercise 03: 1st and 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "7.4 Visualising the output in tmap",
    "text": "7.4 Visualising the output in tmap\nFinally, we will display the raster in cartographic quality map using tmap package.\n\ntm_shape(kde_childcareSG_bw_raster) + \n  tm_raster(\"layer\", palette = \"viridis\") +\n  tm_layout(legend.position = c(\"right\", \"bottom\"), frame = FALSE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNotice that the raster values are encoded explicitly onto the raster pixel using the values in “v” field."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#comparing-spatial-point-patterns-using-kde",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#comparing-spatial-point-patterns-using-kde",
    "title": "Hands-On Exercise 03: 1st and 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "7.5 Comparing Spatial Point Patterns using KDE",
    "text": "7.5 Comparing Spatial Point Patterns using KDE\nHere we will compare KDE of childcare at Ponggol, Tampines, Chua Chu Kang and Jurong West planning areas.\n\n7.5.1 Extracting study area\nTo extract the target planning areas.\n\npg &lt;- mpsz_sf %&gt;%\n  filter(PLN_AREA_N == \"PUNGGOL\")\ntm &lt;- mpsz_sf %&gt;%\n  filter(PLN_AREA_N == \"TAMPINES\")\nck &lt;- mpsz_sf %&gt;%\n  filter(PLN_AREA_N == \"CHOA CHU KANG\")\njw &lt;- mpsz_sf %&gt;%\n  filter(PLN_AREA_N == \"JURONG WEST\")\n\nPlotting target planning areas.\n\npar(mfrow=c(2,2))\nplot(pg, main = \"Ponggol\")\n\n\n\n\n\n\n\n\n\nplot(tm, main = \"Tampines\")\n\n\n\n\n\n\n\n\n\nplot(ck, main = \"Choa Chu Kang\")\n\n\n\n\n\n\n\n\n\nplot(jw, main = \"Jurong West\")\n\n\n\n\n\n\n\n\n\n\n7.5.2 Creating owin object\nNow, we will convert these sf objects into owin objects that is required by spatstat.\n\npg_owin = as.owin(pg)\ntm_owin = as.owin(tm)\nck_owin = as.owin(ck)\njw_owin = as.owin(jw)\n\n\n\n7.5.3 Combining childcare points and the study area\nBy using the code chunk below, we are able to extract childcare that is within the specific region to do our analysis later on.\n\nchildcare_pg_ppp = childcare_ppp_jit[pg_owin]\nchildcare_tm_ppp = childcare_ppp_jit[tm_owin]\nchildcare_ck_ppp = childcare_ppp_jit[ck_owin]\nchildcare_jw_ppp = childcare_ppp_jit[jw_owin]\n\nNext, rescale.ppp() function is used to trasnform the unit of measurement from metre to kilometre.\n\nchildcare_pg_ppp.km = rescale.ppp(childcare_pg_ppp, 1000, \"km\")\nchildcare_tm_ppp.km = rescale.ppp(childcare_tm_ppp, 1000, \"km\")\nchildcare_ck_ppp.km = rescale.ppp(childcare_ck_ppp, 1000, \"km\")\nchildcare_jw_ppp.km = rescale.ppp(childcare_jw_ppp, 1000, \"km\")\n\nThe code chunk below is used to plot these four study areas and the locations of the childcare centres.\n\npar(mfrow=c(2,2))\npar(mar = c(1,3,1,3)) # to adjust the margins of the plot, bm,lm,tm,rm\nplot(childcare_pg_ppp.km, main=\"Punggol\")\nplot(childcare_tm_ppp.km, main=\"Tampines\")\nplot(childcare_ck_ppp.km, main=\"Choa Chu Kang\")\nplot(childcare_jw_ppp.km, main=\"Jurong West\")\n\n\n\n\n\n\n\n\n\n\n7.5.4 Computing KDE\nThe code chunk below will be used to compute the KDE of these four planning area. bw.diggle method is used to derive the bandwidth of each planning area.\n\npar(mfrow=c(2,2))\npar(mar = c(1,3,1,3))\nplot(density(childcare_pg_ppp.km, \n             sigma=bw.diggle, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Punggol\")\nplot(density(childcare_tm_ppp.km, \n             sigma=bw.diggle, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Tempines\")\nplot(density(childcare_ck_ppp.km, \n             sigma=bw.diggle, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Choa Chu Kang\")\nplot(density(childcare_jw_ppp.km, \n             sigma=bw.diggle, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Jurong West\")\n\n\n\n\n\n\n\n\n\n\n7.5.5 Computing fixed bandwidth KDE\nFor comparison purposes, we will use 250m as the bandwidth.\n\npar(mfrow=c(2,2))\npar(mar = c(1,3,1,3))\nplot(density(childcare_ck_ppp.km, \n             sigma=0.25, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Chou Chu Kang\")\nplot(density(childcare_jw_ppp.km, \n             sigma=0.25, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"JUrong West\")\nplot(density(childcare_pg_ppp.km, \n             sigma=0.25, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Punggol\")\nplot(density(childcare_tm_ppp.km, \n             sigma=0.25, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Tampines\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#testing-spatial-point-patterns-using-clark-and-evans-test",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#testing-spatial-point-patterns-using-clark-and-evans-test",
    "title": "Hands-On Exercise 03: 1st and 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "8.1 Testing spatial point patterns using Clark and Evans Test",
    "text": "8.1 Testing spatial point patterns using Clark and Evans Test\n\nclarkevans.test(childcareSG_ppp,\n                correction=\"none\",\n                clipregion=\"sg_owin\",\n                alternative=c(\"clustered\"),\n                nsim=99)\n\n\n    Clark-Evans test\n    No edge correction\n    Z-test\n\ndata:  childcareSG_ppp\nR = 0.55631, p-value &lt; 2.2e-16\nalternative hypothesis: clustered (R &lt; 1)\n\n\n\n\n\n\n\n\nTip\n\n\n\nR = 0.55631: Which is less than 1, which means the spatial distribution is clustered.\np-value &lt; 2.2e-16: This is an extremely small p-value, much smaller than any common significance level (e.g., 0.05 or 0.01). This means that we can reject the null hypothesis(H0)."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#clark-and-evans-test-choa-chu-kang-planning-area",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#clark-and-evans-test-choa-chu-kang-planning-area",
    "title": "Hands-On Exercise 03: 1st and 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "8.2 Clark and Evans Test: Choa Chu Kang planning area",
    "text": "8.2 Clark and Evans Test: Choa Chu Kang planning area\nIn the code chunk below, clarkevans.test() of spatstat is used to performs Clark-Evans test of aggregation for childcare centre in Choa Chu Kang planning area.\n\nclarkevans.test(childcare_ck_ppp,\n                correction=\"none\",\n                clipregion=NULL,\n                alternative=c(\"two.sided\"),\n                nsim=999)\n\n\n    Clark-Evans test\n    No edge correction\n    Z-test\n\ndata:  childcare_ck_ppp\nR = 0.92298, p-value = 0.2498\nalternative hypothesis: two-sided\n\n\n\n\n\n\n\n\nNote\n\n\n\nAs P &gt; 0.05, we cannot reject the null hypothesis that the childcare services are randomly distributed in Choa Chu Kang."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#clark-and-evans-test-tampines-planning-area",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#clark-and-evans-test-tampines-planning-area",
    "title": "Hands-On Exercise 03: 1st and 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "8.3 Clark and Evans Test: Tampines planning area",
    "text": "8.3 Clark and Evans Test: Tampines planning area\nIn the code chunk below, the similar test is used to analyse the spatial point patterns of childcare centre in Tampines planning area.\n\nclarkevans.test(childcare_tm_ppp,\n                correction=\"none\",\n                clipregion=NULL,\n                alternative=c(\"two.sided\"),\n                nsim=999)\n\n\n    Clark-Evans test\n    No edge correction\n    Z-test\n\ndata:  childcare_tm_ppp\nR = 0.80143, p-value = 0.0003387\nalternative hypothesis: two-sided\n\n\n\n\n\n\n\n\nNote\n\n\n\nAs P &lt; 0.05, we reject the null hypothesis that the childcare services are randomly distributed.\nThe pattern exhibits clustering in Tampines."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#analysing-spatial-point-process-using-g-function",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#analysing-spatial-point-process-using-g-function",
    "title": "Hands-On Exercise 03: 1st and 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "9.1 Analysing Spatial Point Process Using G-Function",
    "text": "9.1 Analysing Spatial Point Process Using G-Function\nThe G function measures the distribution of the distances from an arbitrary event to its nearest event. In this section, We will compute G-function estimation by using Gest() of spatstat package. We will also perform monta carlo simulation test using envelope() of spatstat package.\n\n9.1.1 Choa Chu Kang Planning Area\n\n9.1.1.1 Computing G-function estimation\nTo compute G-function using Gest() of spatat package.\n\nG_CK = Gest(childcare_ck_ppp, correction = \"border\")\nplot(G_CK, xlim=c(0,500))\n\n\n\n\n\n\n\n\n\n\n9.1.1.2 Performing Complete Spatial Randomness Test\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\nHo = The distribution of childcare services at Choa Chu Kang are randomly distributed.\nH1= The distribution of childcare services at Choa Chu Kang are not randomly distributed.\nThe null hypothesis will be rejected if p-value is smaller than alpha value of 0.001.\nMonte Carlo test with G-fucntion\n\nG_CK.csr &lt;- envelope(childcare_ck_ppp, Gest, nsim = 999)\n\nGenerating 999 simulations of CSR  ...\n1, 2, 3, ......10.........20.........30.........40.........50.........60..\n.......70.........80.........90.........100.........110.........120.........130\n.........140.........150.........160.........170.........180.........190........\n.200.........210.........220.........230.........240.........250.........260......\n...270.........280.........290.........300.........310.........320.........330....\n.....340.........350.........360.........370.........380.........390.........400..\n.......410.........420.........430.........440.........450.........460.........470\n.........480.........490.........500.........510.........520.........530........\n.540.........550.........560.........570.........580.........590.........600......\n...610.........620.........630.........640.........650.........660.........670....\n.....680.........690.........700.........710.........720.........730.........740..\n.......750.........760.........770.........780.........790.........800.........810\n.........820.........830.........840.........850.........860.........870........\n.880.........890.........900.........910.........920.........930.........940......\n...950.........960.........970.........980.........990........\n999.\n\nDone.\n\n\n\nplot(G_CK.csr)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInterpretation of the G-Function Plot\n\n\n\n\nThe observed G-function is plotted as the solid line.\nThe envelope(shaded area) represents the range of value that the G-function can take under Complete Spatial Randomness(CSR) based on the 999 simulated random patterns.\nIf the observed G-function lies outside the envelop, it means the pattern is not random.\n\nAbove the envelope indicates clustering (more points are closer to each other than expected under CSR)\nBelow the envelope suggests regularity or dispersion (points are more spaced out than expected under CSR)\n\n\nBased on the above plot, observed G-function lies within the envelope area suggests that the pattern is consistent with CSR.\n\n\n\n\n\n9.1.2 Tampines Planning Area\n\n9.1.2.1 Computing G-function estimation\n\nG_TM = Gest(childcare_tm_ppp, correction = \"best\")\nplot(G_TM)\n\n\n\n\n\n\n\n\n\n\n9.1.2.2 Performing Complete Spatial Randomness Test\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\nHo = The distribution of childcare services at Tampines are randomly distributed.\nH1= The distribution of childcare services at Tampines are not randomly distributed.\nThe null hypothesis will be rejected is p-value is smaller than alpha value of 0.001.\nThe code chunk below is used to perform the hypothesis testing.\n\nG_TM.csr &lt;- envelope(childcare_tm_ppp, Gest, correction = \"all\", nsim = 999)\n\nGenerating 999 simulations of CSR  ...\n1, 2, 3, ......10.........20.........30.........40.........50.........60..\n.......70.........80.........90.........100.........110.........120.........130\n.........140.........150.........160.........170.........180.........190........\n.200.........210.........220.........230.........240.........250.........260......\n...270.........280.........290.........300.........310.........320.........330....\n.....340.........350.........360.........370.........380.........390.........400..\n.......410.........420.........430.........440.........450.........460.........470\n.........480.........490.........500.........510.........520.........530........\n.540.........550.........560.........570.........580.........590.........600......\n...610.........620.........630.........640.........650.........660.........670....\n.....680.........690.........700.........710.........720.........730.........740..\n.......750.........760.........770.........780.........790.........800.........810\n.........820.........830.........840.........850.........860.........870........\n.880.........890.........900.........910.........920.........930.........940......\n...950.........960.........970.........980.........990........\n999.\n\nDone.\n\n\n\nplot(G_TM.csr)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nCompare with G_CK, we can see that the observed G-function are closer to the upper bound of envelope (but still below the upper-bound), suggest that the points in G_TM is more closer to each other compare with G_CK."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#analysing-spatial-point-process-using-f-function",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#analysing-spatial-point-process-using-f-function",
    "title": "Hands-On Exercise 03: 1st and 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "9.2 Analysing Spatial Point Process Using F-Function",
    "text": "9.2 Analysing Spatial Point Process Using F-Function\nThe F function estimates the empty space function F(r) or its hazard rate h(r) from a point pattern in a window of arbitrary shape. In this section, you will learn how to compute F-function estimation by using Fest() of spatstat package. You will also learn how to perform monta carlo simulation test using envelope() of spatstat package.\n\n9.2.1 Choa Chu Kang Planning Area\n\n9.2.1.1 Computing F-Function Estimation\nTo compute F-function using Fest() of spatat package.\n\nF_CK = Fest(childcare_ck_ppp)\nplot(F_CK)\n\n\n\n\n\n\n\n\n\n\n9.2.1.2 Performing Complete Spatial Randomness Test\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\nHo = The distribution of childcare services at Choa Chu Kang are randomly distributed.\nH1= The distribution of childcare services at Choa Chu Kang are not randomly distributed.\nThe null hypothesis will be rejected if p-value is smaller than alpha value of 0.001.\nMonte Carlo test with F-fucntion\n\nF_CK.csr &lt;- envelope(childcare_ck_ppp, Fest, nsim = 999)\n\nGenerating 999 simulations of CSR  ...\n1, 2, 3, ......10.........20.........30.........40.........50.........60..\n.......70.........80.........90.........100.........110.........120.........130\n.........140.........150.........160.........170.........180.........190........\n.200.........210.........220.........230.........240.........250.........260......\n...270.........280.........290.........300.........310.........320.........330....\n.....340.........350.........360.........370.........380.........390.........400..\n.......410.........420.........430.........440.........450.........460.........470\n.........480.........490.........500.........510.........520.........530........\n.540.........550.........560.........570.........580.........590.........600......\n...610.........620.........630.........640.........650.........660.........670....\n.....680.........690.........700.........710.........720.........730.........740..\n.......750.........760.........770.........780.........790.........800.........810\n.........820.........830.........840.........850.........860.........870........\n.880.........890.........900.........910.........920.........930.........940......\n...950.........960.........970.........980.........990........\n999.\n\nDone.\n\n\n\nplot(F_CK.csr)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInterpretation of F-Function Plot\n\n\n\n\nAbove the Envelope indicates regularity or dispersion.\nBelow the Envelope indicates clustering.\nWithin the Envelope suggests the pattern could be random. There is no significant envidence of clustering or dispersion.\n\n\n\n\n\n\n9.2.2 Tampines Planing Area\n\n9.2.2.1 Computing F-Function Estimation\nMonte Carlo test with F-fucntion\n\nF_TM = Fest(childcare_tm_ppp, correction = \"best\")\nplot(F_TM)\n\n\n\n\n\n\n\n\n\n\n9.2.2.2 Performing Complete Spatial Randomness Test\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\nHo = The distribution of childcare services at Tampines are randomly distributed.\nH1= The distribution of childcare services at Tampines are not randomly distributed.\nThe null hypothesis will be rejected is p-value is smaller than alpha value of 0.001.\nTo perform the hypothesis testing.\n\nF_TM.csr &lt;- envelope(childcare_tm_ppp, Fest, correction = \"all\", nsim = 999)\n\nGenerating 999 simulations of CSR  ...\n1, 2, 3, ......10.........20.........30.........40.........50.........60..\n.......70.........80.........90.........100.........110.........120.........130\n.........140.........150.........160.........170.........180.........190........\n.200.........210.........220.........230.........240.........250.........260......\n...270.........280.........290.........300.........310.........320.........330....\n.....340.........350.........360.........370.........380.........390.........400..\n.......410.........420.........430.........440.........450.........460.........470\n.........480.........490.........500.........510.........520.........530........\n.540.........550.........560.........570.........580.........590.........600......\n...610.........620.........630.........640.........650.........660.........670....\n.....680.........690.........700.........710.........720.........730.........740..\n.......750.........760.........770.........780.........790.........800.........810\n.........820.........830.........840.........850.........860.........870........\n.880.........890.........900.........910.........920.........930.........940......\n...950.........960.........970.........980.........990........\n999.\n\nDone.\n\n\n\nplot(F_TM.csr)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote that the observed F-Function is below the envelope, which suggest that clustering. So that we can reject the null hypothesis(randomly distributed). As the distribution of childcare services at Tampines are not randomly distributed."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#analysing-spatial-point-process-using-k-function",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#analysing-spatial-point-process-using-k-function",
    "title": "Hands-On Exercise 03: 1st and 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "9.3 Analysing Spatial Point Process Using K-Function",
    "text": "9.3 Analysing Spatial Point Process Using K-Function\nK-function measures the number of events found up to a given distance of any particular event. In this section, you will learn how to compute K-function estimates by using Kest() of spatstat package. You will also learn how to perform monta carlo simulation test using envelope() of spatstat package.\n\n9.3.1 Choa Chu Kan Planning Area\n\n9.3.1.1 Computing K-Function Estimate\n\nK_CK = Kest(childcare_ck_ppp, correction = \"Ripley\")\nplot(K_CK, . -r ~ r, ylab= \"K(d)-r\", xlab = \"d(m)\")\n\n\n\n\n\n\n\n\n\n\n9.3.1.2 Performing Complete Spatial Randomness Test\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\nHo = The distribution of childcare services at Choa Chu Kang are randomly distributed.\nH1= The distribution of childcare services at Choa Chu Kang are not randomly distributed.\nThe null hypothesis will be rejected if p-value is smaller than alpha value of 0.001.\nTo perform the hypothesis testing.\n\nK_CK.csr &lt;- envelope(childcare_ck_ppp, Kest, nsim = 99, rank = 1, glocal=TRUE)\n\nGenerating 99 simulations of CSR  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,\n21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,\n41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60,\n61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80,\n81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, \n99.\n\nDone.\n\n\n\nplot(K_CK.csr, . - r ~ r, xlab=\"d\", ylab=\"K(d)-r\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInterpretation of the K-Function Plot:\n\n\n\n\nAbove the Envelope indicates clustering.\n\nMore points are found within distance r than expected under CSR.\nSuggests spatial clustering at that distance scale.\n\nBelow the Envelope indicates regularity or dispersion.\n\nFewer points are found within distance r than expected under CSR.\nSuggests the point are more regularly spaced at that distance.\n\nWithin the Envelope suggests that the point pattern is consistent with randomness.\n\nSuggests that is no strong evidence for clustering or regularity.\n\n\nBased on the above plot, K_ck is still within the envelope which means the pattern is consistent with CSR. So we accept the null hypothesis, that the distribution is randomly distributed.\n\n\n\n\n\n9.3.2 Tampines Planning Area\n\n9.3.2.1 Computing K-Function Estimate\n\nK_TM = Kest(childcare_tm_ppp, correction = \"Ripley\")\nplot(K_TM, . -r ~ r, \n     ylab= \"K(d)-r\", xlab = \"d(m)\", \n     xlim=c(0,1000))\n\n\n\n\n\n\n\n\n\n\n9.3.2.2 Performing Complete Spatial Randomness Test\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\nHo = The distribution of childcare services at Tampines are randomly distributed.\nH1= The distribution of childcare services at Tampines are not randomly distributed.\nThe null hypothesis will be rejected if p-value is smaller than alpha value of 0.001.\nTo perform the hypothesis testing.\n\nK_TM.csr &lt;- envelope(childcare_tm_ppp, Kest, nsim = 99, rank = 1, glocal=TRUE)\n\nGenerating 99 simulations of CSR  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,\n21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,\n41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60,\n61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80,\n81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, \n99.\n\nDone.\n\n\n\nplot(K_TM.csr, . - r ~ r, \n     xlab=\"d\", ylab=\"K(d)-r\", xlim=c(0,500))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nBased on the plot, we can say that the distribution of childcare services at Tampines are not randomly distributed. As the observed K-Function is above envelope, which suggests clustering."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#analysing-spatial-point-process-using-l-function",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#analysing-spatial-point-process-using-l-function",
    "title": "Hands-On Exercise 03: 1st and 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "9.4 Analysing Spatial Point Process Using L-Function",
    "text": "9.4 Analysing Spatial Point Process Using L-Function\nTo compute L-function estimation by using Lest() of spatstat package. You will also learn how to perform monta carlo simulation test using envelope() of spatstat package.\n\n9.4.1 Choa Chu Kang Planning Area\n\n9.4.1.1 Computing L Function Estimation\n\nL_CK = Lest(childcare_ck_ppp, correction = \"Ripley\")\nplot(L_CK, . -r ~ r, \n     ylab= \"L(d)-r\", xlab = \"d(m)\")\n\n\n\n\n\n\n\n\n\n\n9.4.1.2 Performing Complete Spatial Randomness Test\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\nHo = The distribution of childcare services at Choa Chu Kang are randomly distributed.\nH1= The distribution of childcare services at Choa Chu Kang are not randomly distributed.\nThe null hypothesis will be rejected if p-value if smaller than alpha value of 0.001.\nTo perform the hypothesis testing.\n\nL_CK.csr &lt;- envelope(childcare_ck_ppp, Lest, nsim = 99, rank = 1, glocal=TRUE)\n\nGenerating 99 simulations of CSR  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,\n21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,\n41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60,\n61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80,\n81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, \n99.\n\nDone.\n\n\n\nplot(L_CK.csr, . - r ~ r, xlab=\"d\", ylab=\"L(d)-r\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInterpretation of the L-Function\n\n\n\n\nAbove the Envelope ( L(r) &gt; 0 ) indicates clustering.\n\nMore points are closer together than expected under randomness.\n\nBelow the Envelope ( L(r) &lt; 0 ) indicates regularity or dispersion.\n\nPoints are more evenly spaced than expected under randomness.\n\nWithin the envelope ( L(r) = 0 ) indicates randomness.\n\nNo evidence of clustering or regularity at any distance scale.\n\n\n\n\n\n\n\n9.4.2 Tampines Planning Area\n\n9.4.2.1 Computing L Function Estimation\n\nL_TM = Lest(childcare_tm_ppp, correction = \"Ripley\")\nplot(L_TM, . -r ~ r, \n     ylab= \"L(d)-r\", xlab = \"d(m)\", \n     xlim=c(0,1000))\n\n\n\n\n\n\n\n\n\n\n9.4.2.2 Performing Complete Spatial Randomness Test\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\nHo = The distribution of childcare services at Tampines are randomly distributed.\nH1= The distribution of childcare services at Tampines are not randomly distributed.\nThe null hypothesis will be rejected if p-value is smaller than alpha value of 0.001.\nTo perform the hypothesis testing.\n\nL_TM.csr &lt;- envelope(childcare_tm_ppp, Lest, nsim = 99, rank = 1, glocal=TRUE)\n\nGenerating 99 simulations of CSR  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,\n21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,\n41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60,\n61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80,\n81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, \n99.\n\nDone.\n\n\n\nplot(L_TM.csr, . - r ~ r, \n     xlab=\"d\", ylab=\"L(d)-r\", xlim=c(0,500))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nBased on the plot, we can say that most of the observed L-function value are above the envelope, which suggests clustering. Therefore, we know that the distribution of childcare services at Tampines are not randomly distributed."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html",
    "title": "Hands-On Exercise 05: Spatial Weights and Applications",
    "section": "",
    "text": "In this hands-on exercise, I will learn how to compute spatial weights using R and able to:\n\nimport geospatial data using appropriate function(s) of sf package,\nimport csv file using appropriate function of readr package,\nperform relational join using appropriate join function of dplyr package,\ncompute spatial weights using appropriate functions of spdep package, and\ncalculate spatially lagged variables using appropriate functions of spdep package."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#getting-started",
    "title": "Hands-On Exercise 05: Spatial Weights and Applications",
    "section": "2.1 Getting Started",
    "text": "2.1 Getting Started\nBefore get started,to ensure that spdep, sf, tmap and tidyverse packages of R are currently installed in R.\n\npacman::p_load(sf, spdep, tmap, tidyverse,knitr)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#import-shapefile-into-r-environment",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#import-shapefile-into-r-environment",
    "title": "Hands-On Exercise 05: Spatial Weights and Applications",
    "section": "3.1 Import Shapefile into R Environment",
    "text": "3.1 Import Shapefile into R Environment\nUsing st_read() of sf package to import Hunan shapefile into R. The imported shapefile will be simple features Object of sf.\n\nhunan &lt;- st_read(dsn = \"data/geospatial\",\n                 layer = \"Hunan\")\n\nReading layer `Hunan' from data source \n  `/Users/mingwei/Desktop/SMU/Y3S1/IS415/xXxPMWxXx/IS415-GAA/Hands-on_Ex/Hands-on_Ex05/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#import-csv-file-into-r-environment",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#import-csv-file-into-r-environment",
    "title": "Hands-On Exercise 05: Spatial Weights and Applications",
    "section": "3.2 Import CSV File into R Environment",
    "text": "3.2 Import CSV File into R Environment\nNext, import Hunan_2012.csv into R by using read_csv() of readr package. The output is R dataframe class.\n\nhunan2012 &lt;- read_csv(\"data/aspatial/Hunan_2012.csv\")\n\nRows: 88 Columns: 29\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (2): County, City\ndbl (27): avg_wage, deposite, FAI, Gov_Rev, Gov_Exp, GDP, GDPPC, GIO, Loan, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nTo the class type of hunan2012.\n\nclass(hunan2012)\n\n[1] \"spec_tbl_df\" \"tbl_df\"      \"tbl\"         \"data.frame\""
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#performing-relational-join",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#performing-relational-join",
    "title": "Hands-On Exercise 05: Spatial Weights and Applications",
    "section": "3.3 Performing Relational Join",
    "text": "3.3 Performing Relational Join\nTo update the attribute table of hunan’s SpatialPolygonsDataFrame with the attribute fields of hunan2012 dataframe. This is performed by using left_join() of dplyr package.\n\ntest &lt;- left_join(hunan,hunan2012)\n\nJoining with `by = join_by(County)`\n\n\n\ncolnames(test)\n\n [1] \"NAME_2\"      \"ID_3\"        \"NAME_3\"      \"ENGTYPE_3\"   \"Shape_Leng\" \n [6] \"Shape_Area\"  \"County\"      \"City\"        \"avg_wage\"    \"deposite\"   \n[11] \"FAI\"         \"Gov_Rev\"     \"Gov_Exp\"     \"GDP\"         \"GDPPC\"      \n[16] \"GIO\"         \"Loan\"        \"NIPCR\"       \"Bed\"         \"Emp\"        \n[21] \"EmpR\"        \"EmpRT\"       \"Pri_Stu\"     \"Sec_Stu\"     \"Household\"  \n[26] \"Household_R\" \"NOIP\"        \"Pop_R\"       \"RSCG\"        \"Pop_T\"      \n[31] \"Agri\"        \"Service\"     \"Disp_Inc\"    \"RORP\"        \"ROREmp\"     \n[36] \"geometry\"   \n\n\nCol 7 =&gt; County\nCol 15 =&gt; GDPPC\n\nhunan &lt;- left_join(hunan,hunan2012)%&gt;%\n  select(1:4, 7, 15)\n\nJoining with `by = join_by(County)`"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#computing-queen-contiguity-based-neighbours",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#computing-queen-contiguity-based-neighbours",
    "title": "Hands-On Exercise 05: Spatial Weights and Applications",
    "section": "5.1 Computing (QUEEN) Contiguity Based Neighbours",
    "text": "5.1 Computing (QUEEN) Contiguity Based Neighbours\nTo compute Queen contiguity weight matrix.\n\nwm_q &lt;- poly2nb(hunan, queen=TRUE)\nsummary(wm_q)\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 11 \n 2  2 12 16 24 14 11  4  2  1 \n2 least connected regions:\n30 65 with 1 link\n1 most connected region:\n85 with 11 links\n\n\nThe summary report above shows that there are 88 area units in Hunan. The most connected area unit has 11 neighbours. There are two area units with only one heighbours.\nFor each polygon in our polygon object, wm_q lists all neighboring polygons. For example, to see the neighbors for the first polygon in the object, type:\n\nwm_q[[1]]\n\n[1]  2  3  4 57 85\n\n\nPolygon 1 has 5 neighbors. The numbers represent the polygon IDs as stored in hunan SpatialPolygonsDataFrame class.\nWe can retrive the county name of Polygon ID=1 by using the code chunk below:\n\nhunan$County[1]\n\n[1] \"Anxiang\"\n\n\nThe output reveals that Polygon ID=1 is Anxiang county.\nTo reveal the county names of the five neighboring polygons, the code chunk will be used:\n\nhunan$NAME_3[c(2,3,4,57,85)]\n\n[1] \"Hanshou\" \"Jinshi\"  \"Li\"      \"Nan\"     \"Taoyuan\"\n\n\nTo retrieve the GDPPC of these five countries.\n\nnb1 &lt;- wm_q[[1]]\nnb1 &lt;- hunan$GDPPC[nb1]\nnb1\n\n[1] 20981 34592 24473 21311 22879\n\n\nThe above output shows the GDPPC of the five nearest neighbours based on Queen’s method.\nTo display the complete weight matrix by using str().\n\nstr(wm_q)\n\nList of 88\n $ : int [1:5] 2 3 4 57 85\n $ : int [1:5] 1 57 58 78 85\n $ : int [1:4] 1 4 5 85\n $ : int [1:4] 1 3 5 6\n $ : int [1:4] 3 4 6 85\n $ : int [1:5] 4 5 69 75 85\n $ : int [1:4] 67 71 74 84\n $ : int [1:7] 9 46 47 56 78 80 86\n $ : int [1:6] 8 66 68 78 84 86\n $ : int [1:8] 16 17 19 20 22 70 72 73\n $ : int [1:3] 14 17 72\n $ : int [1:5] 13 60 61 63 83\n $ : int [1:4] 12 15 60 83\n $ : int [1:3] 11 15 17\n $ : int [1:4] 13 14 17 83\n $ : int [1:5] 10 17 22 72 83\n $ : int [1:7] 10 11 14 15 16 72 83\n $ : int [1:5] 20 22 23 77 83\n $ : int [1:6] 10 20 21 73 74 86\n $ : int [1:7] 10 18 19 21 22 23 82\n $ : int [1:5] 19 20 35 82 86\n $ : int [1:5] 10 16 18 20 83\n $ : int [1:7] 18 20 38 41 77 79 82\n $ : int [1:5] 25 28 31 32 54\n $ : int [1:5] 24 28 31 33 81\n $ : int [1:4] 27 33 42 81\n $ : int [1:3] 26 29 42\n $ : int [1:5] 24 25 33 49 54\n $ : int [1:3] 27 37 42\n $ : int 33\n $ : int [1:8] 24 25 32 36 39 40 56 81\n $ : int [1:8] 24 31 50 54 55 56 75 85\n $ : int [1:5] 25 26 28 30 81\n $ : int [1:3] 36 45 80\n $ : int [1:6] 21 41 47 80 82 86\n $ : int [1:6] 31 34 40 45 56 80\n $ : int [1:4] 29 42 43 44\n $ : int [1:4] 23 44 77 79\n $ : int [1:5] 31 40 42 43 81\n $ : int [1:6] 31 36 39 43 45 79\n $ : int [1:6] 23 35 45 79 80 82\n $ : int [1:7] 26 27 29 37 39 43 81\n $ : int [1:6] 37 39 40 42 44 79\n $ : int [1:4] 37 38 43 79\n $ : int [1:6] 34 36 40 41 79 80\n $ : int [1:3] 8 47 86\n $ : int [1:5] 8 35 46 80 86\n $ : int [1:5] 50 51 52 53 55\n $ : int [1:4] 28 51 52 54\n $ : int [1:5] 32 48 52 54 55\n $ : int [1:3] 48 49 52\n $ : int [1:5] 48 49 50 51 54\n $ : int [1:3] 48 55 75\n $ : int [1:6] 24 28 32 49 50 52\n $ : int [1:5] 32 48 50 53 75\n $ : int [1:7] 8 31 32 36 78 80 85\n $ : int [1:6] 1 2 58 64 76 85\n $ : int [1:5] 2 57 68 76 78\n $ : int [1:4] 60 61 87 88\n $ : int [1:4] 12 13 59 61\n $ : int [1:7] 12 59 60 62 63 77 87\n $ : int [1:3] 61 77 87\n $ : int [1:4] 12 61 77 83\n $ : int [1:2] 57 76\n $ : int 76\n $ : int [1:5] 9 67 68 76 84\n $ : int [1:4] 7 66 76 84\n $ : int [1:5] 9 58 66 76 78\n $ : int [1:3] 6 75 85\n $ : int [1:3] 10 72 73\n $ : int [1:3] 7 73 74\n $ : int [1:5] 10 11 16 17 70\n $ : int [1:5] 10 19 70 71 74\n $ : int [1:6] 7 19 71 73 84 86\n $ : int [1:6] 6 32 53 55 69 85\n $ : int [1:7] 57 58 64 65 66 67 68\n $ : int [1:7] 18 23 38 61 62 63 83\n $ : int [1:7] 2 8 9 56 58 68 85\n $ : int [1:7] 23 38 40 41 43 44 45\n $ : int [1:8] 8 34 35 36 41 45 47 56\n $ : int [1:6] 25 26 31 33 39 42\n $ : int [1:5] 20 21 23 35 41\n $ : int [1:9] 12 13 15 16 17 18 22 63 77\n $ : int [1:6] 7 9 66 67 74 86\n $ : int [1:11] 1 2 3 5 6 32 56 57 69 75 ...\n $ : int [1:9] 8 9 19 21 35 46 47 74 84\n $ : int [1:4] 59 61 62 88\n $ : int [1:2] 59 87\n - attr(*, \"class\")= chr \"nb\"\n - attr(*, \"region.id\")= chr [1:88] \"1\" \"2\" \"3\" \"4\" ...\n - attr(*, \"call\")= language poly2nb(pl = hunan, queen = TRUE)\n - attr(*, \"type\")= chr \"queen\"\n - attr(*, \"sym\")= logi TRUE\n\n\nBe warned: The output might cut across several pages."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#creating-rook-contiguity-based-neighbours",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#creating-rook-contiguity-based-neighbours",
    "title": "Hands-On Exercise 05: Spatial Weights and Applications",
    "section": "5.2 Creating (ROOK) Contiguity Based Neighbours",
    "text": "5.2 Creating (ROOK) Contiguity Based Neighbours\nTo compute Rook contiguity weight matrix.\n\nwm_r &lt;- poly2nb(hunan, queen=FALSE)\nsummary(wm_r)\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 440 \nPercentage nonzero weights: 5.681818 \nAverage number of links: 5 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 10 \n 2  2 12 20 21 14 11  3  2  1 \n2 least connected regions:\n30 65 with 1 link\n1 most connected region:\n85 with 10 links\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe summary report above shows that there are 88 area units in Hunan. The most connect area unit has 10 neighbours. There are two area units with only one neighbours."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#visualising-contiguity-weights",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#visualising-contiguity-weights",
    "title": "Hands-On Exercise 05: Spatial Weights and Applications",
    "section": "5.3 Visualising Contiguity Weights",
    "text": "5.3 Visualising Contiguity Weights\nTo create a connectivity graph from polygons, we first need points, typically derived from polygon centroids. Using the sf package in R, we calculate these centroids before constructing the graph. However, we must extract the coordinates of these centroids into a separate data frame. Instead of directly applying st_centroid to the geometry column of the us.bound object, we use the map_dbl function from the purrr package. This function applies st_centroid to each element in the geometry column, returning a vector with centroid coordinates. We then access the longitude and latitude values by indexing the results. Longitude values are extracted by accessing the first value of each centroid.\n\nlongitude &lt;- map_dbl(hunan$geometry, ~st_centroid(.x)[[1]])\n\nDo the same for latitude with one key difference.\n\nlatitude &lt;- map_dbl(hunan$geometry, ~st_centroid(.x)[[2]])\n\nUse cbind to put longitude and latitude into the same object.\n\ncoords &lt;- cbind(longitude, latitude)\n\nCheck the coords.\n\nhead(coords)\n\n     longitude latitude\n[1,]  112.1531 29.44362\n[2,]  112.0372 28.86489\n[3,]  111.8917 29.47107\n[4,]  111.7031 29.74499\n[5,]  111.6138 29.49258\n[6,]  111.0341 29.79863\n\n\n\n5.3.1 Plotting Queen Contiguity Based Neighbours Map\n\nplot(hunan$geometry, border=\"lightgrey\")\nplot(wm_q, coords, pch = 19, cex = 0.6, add = TRUE, col= \"red\")\n\n\n\n\n\n\n\n\n\n\n5.3.2 plotting Rook Contiguity Based Neighbours Map\n\nplot(hunan$geometry, border=\"lightgrey\")\nplot(wm_r, coords, pch = 19, cex = 0.6, add = TRUE, col = \"red\")\n\n\n\n\n\n\n\n\n\n\n5.3.3 Plotting Both Queen and Rook Contiguity Based Neighbours Maps\n\npar(mfrow=c(1,2))\nplot(hunan$geometry, border=\"lightgrey\", main=\"Queen Contiguity\")\nplot(wm_q, coords, pch = 19, cex = 0.6, add = TRUE, col= \"red\")\nplot(hunan$geometry, border=\"lightgrey\", main=\"Rook Contiguity\")\nplot(wm_r, coords, pch = 19, cex = 0.6, add = TRUE, col = \"red\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#determine-the-cut-off-distance",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#determine-the-cut-off-distance",
    "title": "Hands-On Exercise 05: Spatial Weights and Applications",
    "section": "6.1 Determine the Cut-off Distance",
    "text": "6.1 Determine the Cut-off Distance\nFirstly, we need to determine the upper limit for distance band by using the steps below:\n\nReturn a matrix with the indices of points belonging to the set of the k nearest neighbours of each other by using knearneigh() of spdep.\nConvert the knn object returned by knearneigh() into a neighbours list of class nb with a list of integer vectors containing neighbour region number ids by using knn2nb().\nReturn the length of neighbour relationship edges by using nbdists() of spdep. The function returns in the units of the coordinates if the coordinates are projected, in km otherwise.\nRemove the list structure of the returned object by using unlist().\n\n\nk1 &lt;- knn2nb(knearneigh(coords))\nk1dists &lt;- unlist(nbdists(k1, coords, longlat = TRUE))\nsummary(k1dists)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  24.79   32.57   38.01   39.07   44.52   61.79 \n\n\nThe summary report shows that the largest first nearest neighbour distance is 61.79 km, so using this as the upper threshold gives certainty that all units will have at least one neighbour."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#computing-fixed-distance-weight-matrix",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#computing-fixed-distance-weight-matrix",
    "title": "Hands-On Exercise 05: Spatial Weights and Applications",
    "section": "6.2 Computing Fixed Distance Weight Matrix",
    "text": "6.2 Computing Fixed Distance Weight Matrix\nTo compute the distance weight matrix by using dnearneigh().\n\nwm_d62 &lt;- dnearneigh(coords, 0, 62, longlat = TRUE)\nwm_d62\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 324 \nPercentage nonzero weights: 4.183884 \nAverage number of links: 3.681818 \n\n\n\n\n\n\n\n\nNote\n\n\n\nThe average number of links: 3.681818 refers to the average number of neighbouring regions each region has. Calculated using Number of nonzero links(324) divided by Number of regions(88).\n\n\nUsing str() to display the content of wm_d62 weight matrix.\n\nstr(wm_d62)\n\nList of 88\n $ : int [1:5] 3 4 5 57 64\n $ : int [1:4] 57 58 78 85\n $ : int [1:4] 1 4 5 57\n $ : int [1:3] 1 3 5\n $ : int [1:4] 1 3 4 85\n $ : int 69\n $ : int [1:2] 67 84\n $ : int [1:4] 9 46 47 78\n $ : int [1:4] 8 46 68 84\n $ : int [1:4] 16 22 70 72\n $ : int [1:3] 14 17 72\n $ : int [1:5] 13 60 61 63 83\n $ : int [1:4] 12 15 60 83\n $ : int [1:2] 11 17\n $ : int 13\n $ : int [1:4] 10 17 22 83\n $ : int [1:3] 11 14 16\n $ : int [1:3] 20 22 63\n $ : int [1:5] 20 21 73 74 82\n $ : int [1:5] 18 19 21 22 82\n $ : int [1:6] 19 20 35 74 82 86\n $ : int [1:4] 10 16 18 20\n $ : int [1:3] 41 77 82\n $ : int [1:4] 25 28 31 54\n $ : int [1:4] 24 28 33 81\n $ : int [1:4] 27 33 42 81\n $ : int [1:2] 26 29\n $ : int [1:6] 24 25 33 49 52 54\n $ : int [1:2] 27 37\n $ : int 33\n $ : int [1:2] 24 36\n $ : int 50\n $ : int [1:5] 25 26 28 30 81\n $ : int [1:3] 36 45 80\n $ : int [1:6] 21 41 46 47 80 82\n $ : int [1:5] 31 34 45 56 80\n $ : int [1:2] 29 42\n $ : int [1:3] 44 77 79\n $ : int [1:4] 40 42 43 81\n $ : int [1:3] 39 45 79\n $ : int [1:5] 23 35 45 79 82\n $ : int [1:5] 26 37 39 43 81\n $ : int [1:3] 39 42 44\n $ : int [1:2] 38 43\n $ : int [1:6] 34 36 40 41 79 80\n $ : int [1:5] 8 9 35 47 86\n $ : int [1:5] 8 35 46 80 86\n $ : int [1:5] 50 51 52 53 55\n $ : int [1:4] 28 51 52 54\n $ : int [1:6] 32 48 51 52 54 55\n $ : int [1:4] 48 49 50 52\n $ : int [1:6] 28 48 49 50 51 54\n $ : int [1:2] 48 55\n $ : int [1:5] 24 28 49 50 52\n $ : int [1:4] 48 50 53 75\n $ : int 36\n $ : int [1:5] 1 2 3 58 64\n $ : int [1:5] 2 57 64 66 68\n $ : int [1:3] 60 87 88\n $ : int [1:4] 12 13 59 61\n $ : int [1:5] 12 60 62 63 87\n $ : int [1:4] 61 63 77 87\n $ : int [1:5] 12 18 61 62 83\n $ : int [1:4] 1 57 58 76\n $ : int 76\n $ : int [1:5] 58 67 68 76 84\n $ : int [1:2] 7 66\n $ : int [1:4] 9 58 66 84\n $ : int [1:2] 6 75\n $ : int [1:3] 10 72 73\n $ : int [1:2] 73 74\n $ : int [1:3] 10 11 70\n $ : int [1:4] 19 70 71 74\n $ : int [1:5] 19 21 71 73 86\n $ : int [1:2] 55 69\n $ : int [1:3] 64 65 66\n $ : int [1:3] 23 38 62\n $ : int [1:2] 2 8\n $ : int [1:4] 38 40 41 45\n $ : int [1:5] 34 35 36 45 47\n $ : int [1:5] 25 26 33 39 42\n $ : int [1:6] 19 20 21 23 35 41\n $ : int [1:4] 12 13 16 63\n $ : int [1:4] 7 9 66 68\n $ : int [1:2] 2 5\n $ : int [1:4] 21 46 47 74\n $ : int [1:4] 59 61 62 88\n $ : int [1:2] 59 87\n - attr(*, \"class\")= chr \"nb\"\n - attr(*, \"region.id\")= chr [1:88] \"1\" \"2\" \"3\" \"4\" ...\n - attr(*, \"call\")= language dnearneigh(x = coords, d1 = 0, d2 = 62, longlat = TRUE)\n - attr(*, \"dnn\")= num [1:2] 0 62\n - attr(*, \"bounds\")= chr [1:2] \"GE\" \"LE\"\n - attr(*, \"nbtype\")= chr \"distance\"\n - attr(*, \"sym\")= logi TRUE\n\n\nAnother way to display the structure of the weight matrix is to combine table() and card() of spdep.\n\ntable(hunan$County, card(wm_d62))\n\n               \n                1 2 3 4 5 6\n  Anhua         1 0 0 0 0 0\n  Anren         0 0 0 1 0 0\n  Anxiang       0 0 0 0 1 0\n  Baojing       0 0 0 0 1 0\n  Chaling       0 0 1 0 0 0\n  Changning     0 0 1 0 0 0\n  Changsha      0 0 0 1 0 0\n  Chengbu       0 1 0 0 0 0\n  Chenxi        0 0 0 1 0 0\n  Cili          0 1 0 0 0 0\n  Dao           0 0 0 1 0 0\n  Dongan        0 0 1 0 0 0\n  Dongkou       0 0 0 1 0 0\n  Fenghuang     0 0 0 1 0 0\n  Guidong       0 0 1 0 0 0\n  Guiyang       0 0 0 1 0 0\n  Guzhang       0 0 0 0 0 1\n  Hanshou       0 0 0 1 0 0\n  Hengdong      0 0 0 0 1 0\n  Hengnan       0 0 0 0 1 0\n  Hengshan      0 0 0 0 0 1\n  Hengyang      0 0 0 0 0 1\n  Hongjiang     0 0 0 0 1 0\n  Huarong       0 0 0 1 0 0\n  Huayuan       0 0 0 1 0 0\n  Huitong       0 0 0 1 0 0\n  Jiahe         0 0 0 0 1 0\n  Jianghua      0 0 1 0 0 0\n  Jiangyong     0 1 0 0 0 0\n  Jingzhou      0 1 0 0 0 0\n  Jinshi        0 0 0 1 0 0\n  Jishou        0 0 0 0 0 1\n  Lanshan       0 0 0 1 0 0\n  Leiyang       0 0 0 1 0 0\n  Lengshuijiang 0 0 1 0 0 0\n  Li            0 0 1 0 0 0\n  Lianyuan      0 0 0 0 1 0\n  Liling        0 1 0 0 0 0\n  Linli         0 0 0 1 0 0\n  Linwu         0 0 0 1 0 0\n  Linxiang      1 0 0 0 0 0\n  Liuyang       0 1 0 0 0 0\n  Longhui       0 0 1 0 0 0\n  Longshan      0 1 0 0 0 0\n  Luxi          0 0 0 0 1 0\n  Mayang        0 0 0 0 0 1\n  Miluo         0 0 0 0 1 0\n  Nan           0 0 0 0 1 0\n  Ningxiang     0 0 0 1 0 0\n  Ningyuan      0 0 0 0 1 0\n  Pingjiang     0 1 0 0 0 0\n  Qidong        0 0 1 0 0 0\n  Qiyang        0 0 1 0 0 0\n  Rucheng       0 1 0 0 0 0\n  Sangzhi       0 1 0 0 0 0\n  Shaodong      0 0 0 0 1 0\n  Shaoshan      0 0 0 0 1 0\n  Shaoyang      0 0 0 1 0 0\n  Shimen        1 0 0 0 0 0\n  Shuangfeng    0 0 0 0 0 1\n  Shuangpai     0 0 0 1 0 0\n  Suining       0 0 0 0 1 0\n  Taojiang      0 1 0 0 0 0\n  Taoyuan       0 1 0 0 0 0\n  Tongdao       0 1 0 0 0 0\n  Wangcheng     0 0 0 1 0 0\n  Wugang        0 0 1 0 0 0\n  Xiangtan      0 0 0 1 0 0\n  Xiangxiang    0 0 0 0 1 0\n  Xiangyin      0 0 0 1 0 0\n  Xinhua        0 0 0 0 1 0\n  Xinhuang      1 0 0 0 0 0\n  Xinning       0 1 0 0 0 0\n  Xinshao       0 0 0 0 0 1\n  Xintian       0 0 0 0 1 0\n  Xupu          0 1 0 0 0 0\n  Yanling       0 0 1 0 0 0\n  Yizhang       1 0 0 0 0 0\n  Yongshun      0 0 0 1 0 0\n  Yongxing      0 0 0 1 0 0\n  You           0 0 0 1 0 0\n  Yuanjiang     0 0 0 0 1 0\n  Yuanling      1 0 0 0 0 0\n  Yueyang       0 0 1 0 0 0\n  Zhijiang      0 0 0 0 1 0\n  Zhongfang     0 0 0 1 0 0\n  Zhuzhou       0 0 0 0 1 0\n  Zixing        0 0 1 0 0 0\n\n\nTo check the number of connected components exist within the neighbor list wm_d62. There are two field in n_comp, which are nc and comp.id.\n\nn_comp &lt;- n.comp.nb(wm_d62)\nn_comp$nc\n\n[1] 1\n\n\n\ntable(n_comp$comp.id)\n\n\n 1 \n88 \n\n\nThe above output means, there is 1 component with 88 regions in it.\n\n6.2.1 Plotting Fixed Distance Weight Matrix\n\nplot(hunan$geometry, border=\"lightgrey\")\nplot(wm_d62, coords, add=TRUE)\nplot(k1, coords, add=TRUE, col=\"red\", length=0.08)\n\n\n\n\n\n\n\n\nThe red lines show the links of 1st nearest neighbours and the black lines show the links of neighbours within the cut-off distance of 62km.\nTo plot both of them next to each other.\n\npar(mfrow=c(1,2))\nplot(hunan$geometry, border=\"lightgrey\", main=\"1st nearest neighbours\")\nplot(k1, coords, add=TRUE, col=\"red\", length=0.08)\nplot(hunan$geometry, border=\"lightgrey\", main=\"Distance link\")\nplot(wm_d62, coords, add=TRUE, pch = 19, cex = 0.6)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#computing-adaptive-distance-weight-matrix",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#computing-adaptive-distance-weight-matrix",
    "title": "Hands-On Exercise 05: Spatial Weights and Applications",
    "section": "6.3 Computing Adaptive Distance Weight Matrix",
    "text": "6.3 Computing Adaptive Distance Weight Matrix\nOne of the characteristics of fixed distance weight matrix is that more densely settled areas (usually the urban areas) tend to have more neighbours and the less densely settled areas (usually the rural counties) tend to have lesser neighbours. Having many neighbours smoothes the neighbour relationship across more neighbours.\nIt is possible to control the numbers of neighbours directly using k-nearest neighbours, either accepting asymmetric neighbours or imposing symmetry as shown in the code chunk below.\n\nknn6 &lt;- knn2nb(knearneigh(coords, k=6))\nknn6\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 528 \nPercentage nonzero weights: 6.818182 \nAverage number of links: 6 \nNon-symmetric neighbours list\n\n\nTo display the content of the matrix by using str().\n\nstr(knn6)\n\nList of 88\n $ : int [1:6] 2 3 4 5 57 64\n $ : int [1:6] 1 3 57 58 78 85\n $ : int [1:6] 1 2 4 5 57 85\n $ : int [1:6] 1 3 5 6 69 85\n $ : int [1:6] 1 3 4 6 69 85\n $ : int [1:6] 3 4 5 69 75 85\n $ : int [1:6] 9 66 67 71 74 84\n $ : int [1:6] 9 46 47 78 80 86\n $ : int [1:6] 8 46 66 68 84 86\n $ : int [1:6] 16 19 22 70 72 73\n $ : int [1:6] 10 14 16 17 70 72\n $ : int [1:6] 13 15 60 61 63 83\n $ : int [1:6] 12 15 60 61 63 83\n $ : int [1:6] 11 15 16 17 72 83\n $ : int [1:6] 12 13 14 17 60 83\n $ : int [1:6] 10 11 17 22 72 83\n $ : int [1:6] 10 11 14 16 72 83\n $ : int [1:6] 20 22 23 63 77 83\n $ : int [1:6] 10 20 21 73 74 82\n $ : int [1:6] 18 19 21 22 23 82\n $ : int [1:6] 19 20 35 74 82 86\n $ : int [1:6] 10 16 18 19 20 83\n $ : int [1:6] 18 20 41 77 79 82\n $ : int [1:6] 25 28 31 52 54 81\n $ : int [1:6] 24 28 31 33 54 81\n $ : int [1:6] 25 27 29 33 42 81\n $ : int [1:6] 26 29 30 37 42 81\n $ : int [1:6] 24 25 33 49 52 54\n $ : int [1:6] 26 27 37 42 43 81\n $ : int [1:6] 26 27 28 33 49 81\n $ : int [1:6] 24 25 36 39 40 54\n $ : int [1:6] 24 31 50 54 55 56\n $ : int [1:6] 25 26 28 30 49 81\n $ : int [1:6] 36 40 41 45 56 80\n $ : int [1:6] 21 41 46 47 80 82\n $ : int [1:6] 31 34 40 45 56 80\n $ : int [1:6] 26 27 29 42 43 44\n $ : int [1:6] 23 43 44 62 77 79\n $ : int [1:6] 25 40 42 43 44 81\n $ : int [1:6] 31 36 39 43 45 79\n $ : int [1:6] 23 35 45 79 80 82\n $ : int [1:6] 26 27 37 39 43 81\n $ : int [1:6] 37 39 40 42 44 79\n $ : int [1:6] 37 38 39 42 43 79\n $ : int [1:6] 34 36 40 41 79 80\n $ : int [1:6] 8 9 35 47 78 86\n $ : int [1:6] 8 21 35 46 80 86\n $ : int [1:6] 49 50 51 52 53 55\n $ : int [1:6] 28 33 48 51 52 54\n $ : int [1:6] 32 48 51 52 54 55\n $ : int [1:6] 28 48 49 50 52 54\n $ : int [1:6] 28 48 49 50 51 54\n $ : int [1:6] 48 50 51 52 55 75\n $ : int [1:6] 24 28 49 50 51 52\n $ : int [1:6] 32 48 50 52 53 75\n $ : int [1:6] 32 34 36 78 80 85\n $ : int [1:6] 1 2 3 58 64 68\n $ : int [1:6] 2 57 64 66 68 78\n $ : int [1:6] 12 13 60 61 87 88\n $ : int [1:6] 12 13 59 61 63 87\n $ : int [1:6] 12 13 60 62 63 87\n $ : int [1:6] 12 38 61 63 77 87\n $ : int [1:6] 12 18 60 61 62 83\n $ : int [1:6] 1 3 57 58 68 76\n $ : int [1:6] 58 64 66 67 68 76\n $ : int [1:6] 9 58 67 68 76 84\n $ : int [1:6] 7 65 66 68 76 84\n $ : int [1:6] 9 57 58 66 78 84\n $ : int [1:6] 4 5 6 32 75 85\n $ : int [1:6] 10 16 19 22 72 73\n $ : int [1:6] 7 19 73 74 84 86\n $ : int [1:6] 10 11 14 16 17 70\n $ : int [1:6] 10 19 21 70 71 74\n $ : int [1:6] 19 21 71 73 84 86\n $ : int [1:6] 6 32 50 53 55 69\n $ : int [1:6] 58 64 65 66 67 68\n $ : int [1:6] 18 23 38 61 62 63\n $ : int [1:6] 2 8 9 46 58 68\n $ : int [1:6] 38 40 41 43 44 45\n $ : int [1:6] 34 35 36 41 45 47\n $ : int [1:6] 25 26 28 33 39 42\n $ : int [1:6] 19 20 21 23 35 41\n $ : int [1:6] 12 13 15 16 22 63\n $ : int [1:6] 7 9 66 68 71 74\n $ : int [1:6] 2 3 4 5 56 69\n $ : int [1:6] 8 9 21 46 47 74\n $ : int [1:6] 59 60 61 62 63 88\n $ : int [1:6] 59 60 61 62 63 87\n - attr(*, \"region.id\")= chr [1:88] \"1\" \"2\" \"3\" \"4\" ...\n - attr(*, \"call\")= language knearneigh(x = coords, k = 6)\n - attr(*, \"sym\")= logi FALSE\n - attr(*, \"type\")= chr \"knn\"\n - attr(*, \"knn-k\")= num 6\n - attr(*, \"class\")= chr \"nb\"\n\n\n\n\n\n\n\n\nNote\n\n\n\nNotice that each county now has 6 neighbours.\n\n\n\n6.3.1 Plotting Distance Based Neighbours\nPlot the weight matrix.\n\nplot(hunan$geometry, border=\"lightgrey\")\nplot(knn6, coords, pch = 19, cex = 0.6, add = TRUE, col = \"red\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#spatial-lag-with-row-standardised-weights",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#spatial-lag-with-row-standardised-weights",
    "title": "Hands-On Exercise 05: Spatial Weights and Applications",
    "section": "9.1 Spatial Lag with Row-Standardised Weights",
    "text": "9.1 Spatial Lag with Row-Standardised Weights\nFinally, we’ll compute the average neighbor GDPPC value for each polygon. These values are often referred to as spatially lagged values.\n\nGDPPC.lag &lt;- lag.listw(rswm_q, hunan$GDPPC)\nGDPPC.lag\n\n [1] 24847.20 22724.80 24143.25 27737.50 27270.25 21248.80 43747.00 33582.71\n [9] 45651.17 32027.62 32671.00 20810.00 25711.50 30672.33 33457.75 31689.20\n[17] 20269.00 23901.60 25126.17 21903.43 22718.60 25918.80 20307.00 20023.80\n[25] 16576.80 18667.00 14394.67 19848.80 15516.33 20518.00 17572.00 15200.12\n[33] 18413.80 14419.33 24094.50 22019.83 12923.50 14756.00 13869.80 12296.67\n[41] 15775.17 14382.86 11566.33 13199.50 23412.00 39541.00 36186.60 16559.60\n[49] 20772.50 19471.20 19827.33 15466.80 12925.67 18577.17 14943.00 24913.00\n[57] 25093.00 24428.80 17003.00 21143.75 20435.00 17131.33 24569.75 23835.50\n[65] 26360.00 47383.40 55157.75 37058.00 21546.67 23348.67 42323.67 28938.60\n[73] 25880.80 47345.67 18711.33 29087.29 20748.29 35933.71 15439.71 29787.50\n[81] 18145.00 21617.00 29203.89 41363.67 22259.09 44939.56 16902.00 16930.00\n\n\nRecalled in the previous section, we retrieved the GDPPC of these five countries by using the code chunk below.\n\nnb1 &lt;- wm_q[[1]]\nnb1 &lt;- hunan$GDPPC[nb1]\nnb1\n\n[1] 20981 34592 24473 21311 22879\n\n\n\n\n\n\n\n\nNote\n\n\n\nSpatial lag with row-standardized weights allows we to compute the average value of GDPPC for the region’s neighbors, accounting for spatial relationships. This helps us in understanding spatial dependence and how regions are influenced by their neighbors’ characteristics.\n\n\nTo append the spatially lag GDPPC values onto hunan sf data frame.\n\nlag.list &lt;- list(hunan$NAME_3, lag.listw(rswm_q, hunan$GDPPC))\nlag.res &lt;- as.data.frame(lag.list)\ncolnames(lag.res) &lt;- c(\"NAME_3\", \"lag GDPPC\")\nhunan &lt;- left_join(hunan,lag.res)\n\nJoining with `by = join_by(NAME_3)`\n\n\n\nhead(hunan)\n\nSimple feature collection with 6 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 110.4922 ymin: 28.61762 xmax: 112.3013 ymax: 30.12812\nGeodetic CRS:  WGS 84\n   NAME_2  ID_3  NAME_3   ENGTYPE_3  County GDPPC lag GDPPC\n1 Changde 21098 Anxiang      County Anxiang 23667  24847.20\n2 Changde 21100 Hanshou      County Hanshou 20981  22724.80\n3 Changde 21101  Jinshi County City  Jinshi 34592  24143.25\n4 Changde 21102      Li      County      Li 24473  27737.50\n5 Changde 21103   Linli      County   Linli 25554  27270.25\n6 Changde 21104  Shimen      County  Shimen 27137  21248.80\n                        geometry\n1 POLYGON ((112.0625 29.75523...\n2 POLYGON ((112.2288 29.11684...\n3 POLYGON ((111.8927 29.6013,...\n4 POLYGON ((111.3731 29.94649...\n5 POLYGON ((111.6324 29.76288...\n6 POLYGON ((110.8825 30.11675...\n\n\nPlot both the GDPPC and spatial lag GDPPC for comparison.\n\ngdppc &lt;- qtm(hunan, \"GDPPC\")\nlag_gdppc &lt;- qtm(hunan, \"lag GDPPC\")\ntmap_arrange(gdppc, lag_gdppc, asp=1, ncol=2)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#spatial-lag-as-a-sum-of-neightboring-values",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#spatial-lag-as-a-sum-of-neightboring-values",
    "title": "Hands-On Exercise 05: Spatial Weights and Applications",
    "section": "9.2 Spatial Lag as a Sum of Neightboring Values",
    "text": "9.2 Spatial Lag as a Sum of Neightboring Values\nWe can calculate spatial lag as a sum of neighboring values by assigning binary weights. This requires us to go back to our neighbors list, then apply a function that will assign binary weights, then we use glist = in the nb2listw function to explicitly assign these weights.\nWe start by applying a function that will assign a value of 1 per each neighbor. This is done with lapply, which we have been using to manipulate the neighbors structure throughout the past notebooks. Basically it applies a function across each value in the neighbors structure.\n\nb_weights &lt;- lapply(wm_q, function(x) 0*x + 1)\nb_weights2 &lt;- nb2listw(wm_q, \n                       glist = b_weights, \n                       style = \"B\")\nb_weights2\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: B \nWeights constants summary:\n   n   nn  S0  S1    S2\nB 88 7744 448 896 10224\n\n\nWith the proper weights assigned, we can use lag.listw to compute a lag variable from our weight and GDPPC.\n\nlag_sum &lt;- list(hunan$NAME_3, lag.listw(b_weights2, hunan$GDPPC))\nlag.res &lt;- as.data.frame(lag_sum)\ncolnames(lag.res) &lt;- c(\"NAME_3\", \"lag_sum GDPPC\")\n\nCheck the result.\n\nlag_sum\n\n[[1]]\n [1] \"Anxiang\"       \"Hanshou\"       \"Jinshi\"        \"Li\"           \n [5] \"Linli\"         \"Shimen\"        \"Liuyang\"       \"Ningxiang\"    \n [9] \"Wangcheng\"     \"Anren\"         \"Guidong\"       \"Jiahe\"        \n[13] \"Linwu\"         \"Rucheng\"       \"Yizhang\"       \"Yongxing\"     \n[17] \"Zixing\"        \"Changning\"     \"Hengdong\"      \"Hengnan\"      \n[21] \"Hengshan\"      \"Leiyang\"       \"Qidong\"        \"Chenxi\"       \n[25] \"Zhongfang\"     \"Huitong\"       \"Jingzhou\"      \"Mayang\"       \n[29] \"Tongdao\"       \"Xinhuang\"      \"Xupu\"          \"Yuanling\"     \n[33] \"Zhijiang\"      \"Lengshuijiang\" \"Shuangfeng\"    \"Xinhua\"       \n[37] \"Chengbu\"       \"Dongan\"        \"Dongkou\"       \"Longhui\"      \n[41] \"Shaodong\"      \"Suining\"       \"Wugang\"        \"Xinning\"      \n[45] \"Xinshao\"       \"Shaoshan\"      \"Xiangxiang\"    \"Baojing\"      \n[49] \"Fenghuang\"     \"Guzhang\"       \"Huayuan\"       \"Jishou\"       \n[53] \"Longshan\"      \"Luxi\"          \"Yongshun\"      \"Anhua\"        \n[57] \"Nan\"           \"Yuanjiang\"     \"Jianghua\"      \"Lanshan\"      \n[61] \"Ningyuan\"      \"Shuangpai\"     \"Xintian\"       \"Huarong\"      \n[65] \"Linxiang\"      \"Miluo\"         \"Pingjiang\"     \"Xiangyin\"     \n[69] \"Cili\"          \"Chaling\"       \"Liling\"        \"Yanling\"      \n[73] \"You\"           \"Zhuzhou\"       \"Sangzhi\"       \"Yueyang\"      \n[77] \"Qiyang\"        \"Taojiang\"      \"Shaoyang\"      \"Lianyuan\"     \n[81] \"Hongjiang\"     \"Hengyang\"      \"Guiyang\"       \"Changsha\"     \n[85] \"Taoyuan\"       \"Xiangtan\"      \"Dao\"           \"Jiangyong\"    \n\n[[2]]\n [1] 124236 113624  96573 110950 109081 106244 174988 235079 273907 256221\n[11]  98013 104050 102846  92017 133831 158446 141883 119508 150757 153324\n[21] 113593 129594 142149 100119  82884  74668  43184  99244  46549  20518\n[31] 140576 121601  92069  43258 144567 132119  51694  59024  69349  73780\n[41]  94651 100680  69398  52798 140472 118623 180933  82798  83090  97356\n[51]  59482  77334  38777 111463  74715 174391 150558 122144  68012  84575\n[61] 143045  51394  98279  47671  26360 236917 220631 185290  64640  70046\n[71] 126971 144693 129404 284074 112268 203611 145238 251536 108078 238300\n[81] 108870 108085 262835 248182 244850 404456  67608  33860\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nSpatial lag refers to the effect or influence that the neighboring regions have on a given region. Instead of looking at a region in isolation, spatial lag helps us account for spatial dependencies by considering the neighboring regions’ values.\nSpatial lag as a sum refers the sum of the GDPPC values for all neighboring regions, calculated based on binary spatial weights.\n\n\n\nNext, we will append the lag_sum GDPPC field into hunan sf data frame.\n\nhunan &lt;- left_join(hunan, lag.res)\n\nJoining with `by = join_by(NAME_3)`\n\n\nPlot both GDPPC and Spatial Lag Sum GDPPC.\n\ngdppc &lt;- qtm(hunan, \"GDPPC\")\nlag_sum_gdppc &lt;- qtm(hunan, \"lag_sum GDPPC\")\ntmap_arrange(gdppc, lag_sum_gdppc, asp=1, ncol=2)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#spatial-window-average",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#spatial-window-average",
    "title": "Hands-On Exercise 05: Spatial Weights and Applications",
    "section": "9.3 Spatial Window Average",
    "text": "9.3 Spatial Window Average\nThe spatial window average uses row-standardized weights and includes the diagonal element. To do this in R, we need to go back to the neighbors structure and add the diagonal element before assigning weights.\nTo add the diagonal element to the neighbour list, we just need to use include.self() from spdep.\n\nwm_qs &lt;- include.self(wm_q)\nwm_qs\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 536 \nPercentage nonzero weights: 6.921488 \nAverage number of links: 6.090909 \n\n\n\n\n\n\n\n\nNote\n\n\n\nNotice that the Number of nonzero links, Percentage nonzero weights and Average number of links are 536, 6.921488 and 6.090909 respectively as compared to wm_q of 448, 5.785124 and 5.090909\n\n\nChecking neighbour list of area[1].\n\nwm_qs[[1]]\n\n[1]  1  2  3  4 57 85\n\n\nNotice now it has 6 neighbours instead of five.\nObtain weights with nb2listw().\n\nwm_qs &lt;- nb2listw(wm_qs)\nwm_qs\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 536 \nPercentage nonzero weights: 6.921488 \nAverage number of links: 6.090909 \n\nWeights style: W \nWeights constants summary:\n   n   nn S0       S1       S2\nW 88 7744 88 30.90265 357.5308\n\n\nTo create the lag variable from our weight structure and GDPPC variable.\n\nlag_w_avg_gpdpc &lt;- lag.listw(wm_qs, hunan$GDPPC)\nlag_w_avg_gpdpc\n\n [1] 24650.50 22434.17 26233.00 27084.60 26927.00 22230.17 47621.20 37160.12\n [9] 49224.71 29886.89 26627.50 22690.17 25366.40 25825.75 30329.00 32682.83\n[17] 25948.62 23987.67 25463.14 21904.38 23127.50 25949.83 20018.75 19524.17\n[25] 18955.00 17800.40 15883.00 18831.33 14832.50 17965.00 17159.89 16199.44\n[33] 18764.50 26878.75 23188.86 20788.14 12365.20 15985.00 13764.83 11907.43\n[41] 17128.14 14593.62 11644.29 12706.00 21712.29 43548.25 35049.00 16226.83\n[49] 19294.40 18156.00 19954.75 18145.17 12132.75 18419.29 14050.83 23619.75\n[57] 24552.71 24733.67 16762.60 20932.60 19467.75 18334.00 22541.00 26028.00\n[65] 29128.50 46569.00 47576.60 36545.50 20838.50 22531.00 42115.50 27619.00\n[73] 27611.33 44523.29 18127.43 28746.38 20734.50 33880.62 14716.38 28516.22\n[81] 18086.14 21244.50 29568.80 48119.71 22310.75 43151.60 17133.40 17009.33\n\n\nConvert the lag variable listw object into a data.frame by using as.data.frame().\n\nlag.list.wm_qs &lt;- list(hunan$NAME_3, lag.listw(wm_qs, hunan$GDPPC))\nlag_wm_qs.res &lt;- as.data.frame(lag.list.wm_qs)\ncolnames(lag_wm_qs.res) &lt;- c(\"NAME_3\", \"lag_window_avg GDPPC\")\n\n\n\n\n\n\n\nNote\n\n\n\nThe third command line on the code chunk above renames the field names of lag_wm_q1.res object into NAME_3 and lag_window_avg GDPPC respectively.\n\n\nTo append lag_window_avg GDPPC values onto hunan sf data.frame by using left_join() of dplyr package.\n\nhunan &lt;- left_join(hunan, lag_wm_qs.res)\n\nJoining with `by = join_by(NAME_3)`\n\n\nTo compare the values of lag GDPPC and Spatial window average, kable() of Knitr package is used to prepare a table using the code chunk below.\n\nhunan %&gt;%\n  select(\"County\", \n         \"lag GDPPC\", \n         \"lag_window_avg GDPPC\") %&gt;%\n  kable()\n\n\n\n\n\n\n\n\n\n\nCounty\nlag GDPPC\nlag_window_avg GDPPC\ngeometry\n\n\n\n\nAnxiang\n24847.20\n24650.50\nPOLYGON ((112.0625 29.75523…\n\n\nHanshou\n22724.80\n22434.17\nPOLYGON ((112.2288 29.11684…\n\n\nJinshi\n24143.25\n26233.00\nPOLYGON ((111.8927 29.6013,…\n\n\nLi\n27737.50\n27084.60\nPOLYGON ((111.3731 29.94649…\n\n\nLinli\n27270.25\n26927.00\nPOLYGON ((111.6324 29.76288…\n\n\nShimen\n21248.80\n22230.17\nPOLYGON ((110.8825 30.11675…\n\n\nLiuyang\n43747.00\n47621.20\nPOLYGON ((113.9905 28.5682,…\n\n\nNingxiang\n33582.71\n37160.12\nPOLYGON ((112.7181 28.38299…\n\n\nWangcheng\n45651.17\n49224.71\nPOLYGON ((112.7914 28.52688…\n\n\nAnren\n32027.62\n29886.89\nPOLYGON ((113.1757 26.82734…\n\n\nGuidong\n32671.00\n26627.50\nPOLYGON ((114.1799 26.20117…\n\n\nJiahe\n20810.00\n22690.17\nPOLYGON ((112.4425 25.74358…\n\n\nLinwu\n25711.50\n25366.40\nPOLYGON ((112.5914 25.55143…\n\n\nRucheng\n30672.33\n25825.75\nPOLYGON ((113.6759 25.87578…\n\n\nYizhang\n33457.75\n30329.00\nPOLYGON ((113.2621 25.68394…\n\n\nYongxing\n31689.20\n32682.83\nPOLYGON ((113.3169 26.41843…\n\n\nZixing\n20269.00\n25948.62\nPOLYGON ((113.7311 26.16259…\n\n\nChangning\n23901.60\n23987.67\nPOLYGON ((112.6144 26.60198…\n\n\nHengdong\n25126.17\n25463.14\nPOLYGON ((113.1056 27.21007…\n\n\nHengnan\n21903.43\n21904.38\nPOLYGON ((112.7599 26.98149…\n\n\nHengshan\n22718.60\n23127.50\nPOLYGON ((112.607 27.4689, …\n\n\nLeiyang\n25918.80\n25949.83\nPOLYGON ((112.9996 26.69276…\n\n\nQidong\n20307.00\n20018.75\nPOLYGON ((111.7818 27.0383,…\n\n\nChenxi\n20023.80\n19524.17\nPOLYGON ((110.2624 28.21778…\n\n\nZhongfang\n16576.80\n18955.00\nPOLYGON ((109.9431 27.72858…\n\n\nHuitong\n18667.00\n17800.40\nPOLYGON ((109.9419 27.10512…\n\n\nJingzhou\n14394.67\n15883.00\nPOLYGON ((109.8186 26.75842…\n\n\nMayang\n19848.80\n18831.33\nPOLYGON ((109.795 27.98008,…\n\n\nTongdao\n15516.33\n14832.50\nPOLYGON ((109.9294 26.46561…\n\n\nXinhuang\n20518.00\n17965.00\nPOLYGON ((109.227 27.43733,…\n\n\nXupu\n17572.00\n17159.89\nPOLYGON ((110.7189 28.30485…\n\n\nYuanling\n15200.12\n16199.44\nPOLYGON ((110.9652 28.99895…\n\n\nZhijiang\n18413.80\n18764.50\nPOLYGON ((109.8818 27.60661…\n\n\nLengshuijiang\n14419.33\n26878.75\nPOLYGON ((111.5307 27.81472…\n\n\nShuangfeng\n24094.50\n23188.86\nPOLYGON ((112.263 27.70421,…\n\n\nXinhua\n22019.83\n20788.14\nPOLYGON ((111.3345 28.19642…\n\n\nChengbu\n12923.50\n12365.20\nPOLYGON ((110.4455 26.69317…\n\n\nDongan\n14756.00\n15985.00\nPOLYGON ((111.4531 26.86812…\n\n\nDongkou\n13869.80\n13764.83\nPOLYGON ((110.6622 27.37305…\n\n\nLonghui\n12296.67\n11907.43\nPOLYGON ((110.985 27.65983,…\n\n\nShaodong\n15775.17\n17128.14\nPOLYGON ((111.9054 27.40254…\n\n\nSuining\n14382.86\n14593.62\nPOLYGON ((110.389 27.10006,…\n\n\nWugang\n11566.33\n11644.29\nPOLYGON ((110.9878 27.03345…\n\n\nXinning\n13199.50\n12706.00\nPOLYGON ((111.0736 26.84627…\n\n\nXinshao\n23412.00\n21712.29\nPOLYGON ((111.6013 27.58275…\n\n\nShaoshan\n39541.00\n43548.25\nPOLYGON ((112.5391 27.97742…\n\n\nXiangxiang\n36186.60\n35049.00\nPOLYGON ((112.4549 28.05783…\n\n\nBaojing\n16559.60\n16226.83\nPOLYGON ((109.7015 28.82844…\n\n\nFenghuang\n20772.50\n19294.40\nPOLYGON ((109.5239 28.19206…\n\n\nGuzhang\n19471.20\n18156.00\nPOLYGON ((109.8968 28.74034…\n\n\nHuayuan\n19827.33\n19954.75\nPOLYGON ((109.5647 28.61712…\n\n\nJishou\n15466.80\n18145.17\nPOLYGON ((109.8375 28.4696,…\n\n\nLongshan\n12925.67\n12132.75\nPOLYGON ((109.6337 29.62521…\n\n\nLuxi\n18577.17\n18419.29\nPOLYGON ((110.1067 28.41835…\n\n\nYongshun\n14943.00\n14050.83\nPOLYGON ((110.0003 29.29499…\n\n\nAnhua\n24913.00\n23619.75\nPOLYGON ((111.6034 28.63716…\n\n\nNan\n25093.00\n24552.71\nPOLYGON ((112.3232 29.46074…\n\n\nYuanjiang\n24428.80\n24733.67\nPOLYGON ((112.4391 29.1791,…\n\n\nJianghua\n17003.00\n16762.60\nPOLYGON ((111.6461 25.29661…\n\n\nLanshan\n21143.75\n20932.60\nPOLYGON ((112.2286 25.61123…\n\n\nNingyuan\n20435.00\n19467.75\nPOLYGON ((112.0715 26.09892…\n\n\nShuangpai\n17131.33\n18334.00\nPOLYGON ((111.8864 26.11957…\n\n\nXintian\n24569.75\n22541.00\nPOLYGON ((112.2578 26.0796,…\n\n\nHuarong\n23835.50\n26028.00\nPOLYGON ((112.9242 29.69134…\n\n\nLinxiang\n26360.00\n29128.50\nPOLYGON ((113.5502 29.67418…\n\n\nMiluo\n47383.40\n46569.00\nPOLYGON ((112.9902 29.02139…\n\n\nPingjiang\n55157.75\n47576.60\nPOLYGON ((113.8436 29.06152…\n\n\nXiangyin\n37058.00\n36545.50\nPOLYGON ((112.9173 28.98264…\n\n\nCili\n21546.67\n20838.50\nPOLYGON ((110.8822 29.69017…\n\n\nChaling\n23348.67\n22531.00\nPOLYGON ((113.7666 27.10573…\n\n\nLiling\n42323.67\n42115.50\nPOLYGON ((113.5673 27.94346…\n\n\nYanling\n28938.60\n27619.00\nPOLYGON ((113.9292 26.6154,…\n\n\nYou\n25880.80\n27611.33\nPOLYGON ((113.5879 27.41324…\n\n\nZhuzhou\n47345.67\n44523.29\nPOLYGON ((113.2493 28.02411…\n\n\nSangzhi\n18711.33\n18127.43\nPOLYGON ((110.556 29.40543,…\n\n\nYueyang\n29087.29\n28746.38\nPOLYGON ((113.343 29.61064,…\n\n\nQiyang\n20748.29\n20734.50\nPOLYGON ((111.5563 26.81318…\n\n\nTaojiang\n35933.71\n33880.62\nPOLYGON ((112.0508 28.67265…\n\n\nShaoyang\n15439.71\n14716.38\nPOLYGON ((111.5013 27.30207…\n\n\nLianyuan\n29787.50\n28516.22\nPOLYGON ((111.6789 28.02946…\n\n\nHongjiang\n18145.00\n18086.14\nPOLYGON ((110.1441 27.47513…\n\n\nHengyang\n21617.00\n21244.50\nPOLYGON ((112.7144 26.98613…\n\n\nGuiyang\n29203.89\n29568.80\nPOLYGON ((113.0811 26.04963…\n\n\nChangsha\n41363.67\n48119.71\nPOLYGON ((112.9421 28.03722…\n\n\nTaoyuan\n22259.09\n22310.75\nPOLYGON ((112.0612 29.32855…\n\n\nXiangtan\n44939.56\n43151.60\nPOLYGON ((113.0426 27.8942,…\n\n\nDao\n16902.00\n17133.40\nPOLYGON ((111.498 25.81679,…\n\n\nJiangyong\n16930.00\n17009.33\nPOLYGON ((111.3659 25.39472…\n\n\n\n\n\nqtm() of tmap package is used to plot the lag_gdppc and w_ave_gdppc maps next to each other for quick comparison.\n\nw_avg_gdppc &lt;- qtm(hunan, \"lag_window_avg GDPPC\")\ntmap_arrange(lag_gdppc, w_avg_gdppc, asp=1, ncol=2)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#spatial-window-sum",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#spatial-window-sum",
    "title": "Hands-On Exercise 05: Spatial Weights and Applications",
    "section": "9.4 Spatial Window Sum",
    "text": "9.4 Spatial Window Sum\nThe spatial window sum is the counter part of the window average, but without using row-standardized weights.\nTo add the diagonal element to the neighbour list, we just need to use include.self() from spdep.\n\nwm_qs &lt;- include.self(wm_q)\nwm_qs\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 536 \nPercentage nonzero weights: 6.921488 \nAverage number of links: 6.090909 \n\n\nNext, we will assign binary weights to the neighbour structure that includes the diagonal element.\n\nb_weights &lt;- lapply(wm_qs, function(x) 0*x + 1)\nb_weights[1]\n\n[[1]]\n[1] 1 1 1 1 1 1\n\n\nNotice it have 6 neighbours.\nUsing nb2listw() and glist() to explicitly assign weight values.\n\nb_weights2 &lt;- nb2listw(wm_qs, \n                       glist = b_weights, \n                       style = \"B\")\nb_weights2\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 536 \nPercentage nonzero weights: 6.921488 \nAverage number of links: 6.090909 \n\nWeights style: B \nWeights constants summary:\n   n   nn  S0   S1    S2\nB 88 7744 536 1072 14160\n\n\nCompute the lag variable with lag.listw().\n\nw_sum_gdppc &lt;- list(hunan$NAME_3, lag.listw(b_weights2, hunan$GDPPC))\nw_sum_gdppc\n\n[[1]]\n [1] \"Anxiang\"       \"Hanshou\"       \"Jinshi\"        \"Li\"           \n [5] \"Linli\"         \"Shimen\"        \"Liuyang\"       \"Ningxiang\"    \n [9] \"Wangcheng\"     \"Anren\"         \"Guidong\"       \"Jiahe\"        \n[13] \"Linwu\"         \"Rucheng\"       \"Yizhang\"       \"Yongxing\"     \n[17] \"Zixing\"        \"Changning\"     \"Hengdong\"      \"Hengnan\"      \n[21] \"Hengshan\"      \"Leiyang\"       \"Qidong\"        \"Chenxi\"       \n[25] \"Zhongfang\"     \"Huitong\"       \"Jingzhou\"      \"Mayang\"       \n[29] \"Tongdao\"       \"Xinhuang\"      \"Xupu\"          \"Yuanling\"     \n[33] \"Zhijiang\"      \"Lengshuijiang\" \"Shuangfeng\"    \"Xinhua\"       \n[37] \"Chengbu\"       \"Dongan\"        \"Dongkou\"       \"Longhui\"      \n[41] \"Shaodong\"      \"Suining\"       \"Wugang\"        \"Xinning\"      \n[45] \"Xinshao\"       \"Shaoshan\"      \"Xiangxiang\"    \"Baojing\"      \n[49] \"Fenghuang\"     \"Guzhang\"       \"Huayuan\"       \"Jishou\"       \n[53] \"Longshan\"      \"Luxi\"          \"Yongshun\"      \"Anhua\"        \n[57] \"Nan\"           \"Yuanjiang\"     \"Jianghua\"      \"Lanshan\"      \n[61] \"Ningyuan\"      \"Shuangpai\"     \"Xintian\"       \"Huarong\"      \n[65] \"Linxiang\"      \"Miluo\"         \"Pingjiang\"     \"Xiangyin\"     \n[69] \"Cili\"          \"Chaling\"       \"Liling\"        \"Yanling\"      \n[73] \"You\"           \"Zhuzhou\"       \"Sangzhi\"       \"Yueyang\"      \n[77] \"Qiyang\"        \"Taojiang\"      \"Shaoyang\"      \"Lianyuan\"     \n[81] \"Hongjiang\"     \"Hengyang\"      \"Guiyang\"       \"Changsha\"     \n[85] \"Taoyuan\"       \"Xiangtan\"      \"Dao\"           \"Jiangyong\"    \n\n[[2]]\n [1] 147903 134605 131165 135423 134635 133381 238106 297281 344573 268982\n[11] 106510 136141 126832 103303 151645 196097 207589 143926 178242 175235\n[21] 138765 155699 160150 117145 113730  89002  63532 112988  59330  35930\n[31] 154439 145795 112587 107515 162322 145517  61826  79925  82589  83352\n[41] 119897 116749  81510  63530 151986 174193 210294  97361  96472 108936\n[51]  79819 108871  48531 128935  84305 188958 171869 148402  83813 104663\n[61] 155742  73336 112705  78084  58257 279414 237883 219273  83354  90124\n[71] 168462 165714 165668 311663 126892 229971 165876 271045 117731 256646\n[81] 126603 127467 295688 336838 267729 431516  85667  51028\n\n\nConvert the lag variable listw object into a data.frame by using as.data.frame().\n\nw_sum_gdppc.res &lt;- as.data.frame(w_sum_gdppc)\ncolnames(w_sum_gdppc.res) &lt;- c(\"NAME_3\", \"w_sum GDPPC\")\n\nAppend the column.\n\nhunan &lt;- left_join(hunan, w_sum_gdppc.res)\n\nJoining with `by = join_by(NAME_3)`\n\n\nTo compare the values of lag GDPPC and Spatial window average, using kable() of Knitr package.\n\nhunan %&gt;%\n  select(\"County\", \"lag_sum GDPPC\", \"w_sum GDPPC\") %&gt;%\n  kable()\n\n\n\n\n\n\n\n\n\n\nCounty\nlag_sum GDPPC\nw_sum GDPPC\ngeometry\n\n\n\n\nAnxiang\n124236\n147903\nPOLYGON ((112.0625 29.75523…\n\n\nHanshou\n113624\n134605\nPOLYGON ((112.2288 29.11684…\n\n\nJinshi\n96573\n131165\nPOLYGON ((111.8927 29.6013,…\n\n\nLi\n110950\n135423\nPOLYGON ((111.3731 29.94649…\n\n\nLinli\n109081\n134635\nPOLYGON ((111.6324 29.76288…\n\n\nShimen\n106244\n133381\nPOLYGON ((110.8825 30.11675…\n\n\nLiuyang\n174988\n238106\nPOLYGON ((113.9905 28.5682,…\n\n\nNingxiang\n235079\n297281\nPOLYGON ((112.7181 28.38299…\n\n\nWangcheng\n273907\n344573\nPOLYGON ((112.7914 28.52688…\n\n\nAnren\n256221\n268982\nPOLYGON ((113.1757 26.82734…\n\n\nGuidong\n98013\n106510\nPOLYGON ((114.1799 26.20117…\n\n\nJiahe\n104050\n136141\nPOLYGON ((112.4425 25.74358…\n\n\nLinwu\n102846\n126832\nPOLYGON ((112.5914 25.55143…\n\n\nRucheng\n92017\n103303\nPOLYGON ((113.6759 25.87578…\n\n\nYizhang\n133831\n151645\nPOLYGON ((113.2621 25.68394…\n\n\nYongxing\n158446\n196097\nPOLYGON ((113.3169 26.41843…\n\n\nZixing\n141883\n207589\nPOLYGON ((113.7311 26.16259…\n\n\nChangning\n119508\n143926\nPOLYGON ((112.6144 26.60198…\n\n\nHengdong\n150757\n178242\nPOLYGON ((113.1056 27.21007…\n\n\nHengnan\n153324\n175235\nPOLYGON ((112.7599 26.98149…\n\n\nHengshan\n113593\n138765\nPOLYGON ((112.607 27.4689, …\n\n\nLeiyang\n129594\n155699\nPOLYGON ((112.9996 26.69276…\n\n\nQidong\n142149\n160150\nPOLYGON ((111.7818 27.0383,…\n\n\nChenxi\n100119\n117145\nPOLYGON ((110.2624 28.21778…\n\n\nZhongfang\n82884\n113730\nPOLYGON ((109.9431 27.72858…\n\n\nHuitong\n74668\n89002\nPOLYGON ((109.9419 27.10512…\n\n\nJingzhou\n43184\n63532\nPOLYGON ((109.8186 26.75842…\n\n\nMayang\n99244\n112988\nPOLYGON ((109.795 27.98008,…\n\n\nTongdao\n46549\n59330\nPOLYGON ((109.9294 26.46561…\n\n\nXinhuang\n20518\n35930\nPOLYGON ((109.227 27.43733,…\n\n\nXupu\n140576\n154439\nPOLYGON ((110.7189 28.30485…\n\n\nYuanling\n121601\n145795\nPOLYGON ((110.9652 28.99895…\n\n\nZhijiang\n92069\n112587\nPOLYGON ((109.8818 27.60661…\n\n\nLengshuijiang\n43258\n107515\nPOLYGON ((111.5307 27.81472…\n\n\nShuangfeng\n144567\n162322\nPOLYGON ((112.263 27.70421,…\n\n\nXinhua\n132119\n145517\nPOLYGON ((111.3345 28.19642…\n\n\nChengbu\n51694\n61826\nPOLYGON ((110.4455 26.69317…\n\n\nDongan\n59024\n79925\nPOLYGON ((111.4531 26.86812…\n\n\nDongkou\n69349\n82589\nPOLYGON ((110.6622 27.37305…\n\n\nLonghui\n73780\n83352\nPOLYGON ((110.985 27.65983,…\n\n\nShaodong\n94651\n119897\nPOLYGON ((111.9054 27.40254…\n\n\nSuining\n100680\n116749\nPOLYGON ((110.389 27.10006,…\n\n\nWugang\n69398\n81510\nPOLYGON ((110.9878 27.03345…\n\n\nXinning\n52798\n63530\nPOLYGON ((111.0736 26.84627…\n\n\nXinshao\n140472\n151986\nPOLYGON ((111.6013 27.58275…\n\n\nShaoshan\n118623\n174193\nPOLYGON ((112.5391 27.97742…\n\n\nXiangxiang\n180933\n210294\nPOLYGON ((112.4549 28.05783…\n\n\nBaojing\n82798\n97361\nPOLYGON ((109.7015 28.82844…\n\n\nFenghuang\n83090\n96472\nPOLYGON ((109.5239 28.19206…\n\n\nGuzhang\n97356\n108936\nPOLYGON ((109.8968 28.74034…\n\n\nHuayuan\n59482\n79819\nPOLYGON ((109.5647 28.61712…\n\n\nJishou\n77334\n108871\nPOLYGON ((109.8375 28.4696,…\n\n\nLongshan\n38777\n48531\nPOLYGON ((109.6337 29.62521…\n\n\nLuxi\n111463\n128935\nPOLYGON ((110.1067 28.41835…\n\n\nYongshun\n74715\n84305\nPOLYGON ((110.0003 29.29499…\n\n\nAnhua\n174391\n188958\nPOLYGON ((111.6034 28.63716…\n\n\nNan\n150558\n171869\nPOLYGON ((112.3232 29.46074…\n\n\nYuanjiang\n122144\n148402\nPOLYGON ((112.4391 29.1791,…\n\n\nJianghua\n68012\n83813\nPOLYGON ((111.6461 25.29661…\n\n\nLanshan\n84575\n104663\nPOLYGON ((112.2286 25.61123…\n\n\nNingyuan\n143045\n155742\nPOLYGON ((112.0715 26.09892…\n\n\nShuangpai\n51394\n73336\nPOLYGON ((111.8864 26.11957…\n\n\nXintian\n98279\n112705\nPOLYGON ((112.2578 26.0796,…\n\n\nHuarong\n47671\n78084\nPOLYGON ((112.9242 29.69134…\n\n\nLinxiang\n26360\n58257\nPOLYGON ((113.5502 29.67418…\n\n\nMiluo\n236917\n279414\nPOLYGON ((112.9902 29.02139…\n\n\nPingjiang\n220631\n237883\nPOLYGON ((113.8436 29.06152…\n\n\nXiangyin\n185290\n219273\nPOLYGON ((112.9173 28.98264…\n\n\nCili\n64640\n83354\nPOLYGON ((110.8822 29.69017…\n\n\nChaling\n70046\n90124\nPOLYGON ((113.7666 27.10573…\n\n\nLiling\n126971\n168462\nPOLYGON ((113.5673 27.94346…\n\n\nYanling\n144693\n165714\nPOLYGON ((113.9292 26.6154,…\n\n\nYou\n129404\n165668\nPOLYGON ((113.5879 27.41324…\n\n\nZhuzhou\n284074\n311663\nPOLYGON ((113.2493 28.02411…\n\n\nSangzhi\n112268\n126892\nPOLYGON ((110.556 29.40543,…\n\n\nYueyang\n203611\n229971\nPOLYGON ((113.343 29.61064,…\n\n\nQiyang\n145238\n165876\nPOLYGON ((111.5563 26.81318…\n\n\nTaojiang\n251536\n271045\nPOLYGON ((112.0508 28.67265…\n\n\nShaoyang\n108078\n117731\nPOLYGON ((111.5013 27.30207…\n\n\nLianyuan\n238300\n256646\nPOLYGON ((111.6789 28.02946…\n\n\nHongjiang\n108870\n126603\nPOLYGON ((110.1441 27.47513…\n\n\nHengyang\n108085\n127467\nPOLYGON ((112.7144 26.98613…\n\n\nGuiyang\n262835\n295688\nPOLYGON ((113.0811 26.04963…\n\n\nChangsha\n248182\n336838\nPOLYGON ((112.9421 28.03722…\n\n\nTaoyuan\n244850\n267729\nPOLYGON ((112.0612 29.32855…\n\n\nXiangtan\n404456\n431516\nPOLYGON ((113.0426 27.8942,…\n\n\nDao\n67608\n85667\nPOLYGON ((111.498 25.81679,…\n\n\nJiangyong\n33860\n51028\nPOLYGON ((111.3659 25.39472…\n\n\n\n\n\nUsing qtm() of tmap to plot the map.\n\nw_sum_gdppc &lt;- qtm(hunan, \"w_sum GDPPC\")\ntmap_arrange(lag_sum_gdppc, w_sum_gdppc, asp=1, ncol=2)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html",
    "title": "Hands-on Exercise 01:Geospatial Data Science with R",
    "section": "",
    "text": "Install and launching R packages\nThe code chunk below use p_load() of pacman package to check if tidyverse packages are installed in the computer. If they are, then they will be launched into R.\n\npacman::p_load(sf, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#getting-started",
    "title": "Hands-on Exercise 01:Geospatial Data Science with R",
    "section": "",
    "text": "Install and launching R packages\nThe code chunk below use p_load() of pacman package to check if tidyverse packages are installed in the computer. If they are, then they will be launched into R.\n\npacman::p_load(sf, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#importing-the-data",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#importing-the-data",
    "title": "Hands-on Exercise 01:Geospatial Data Science with R",
    "section": "1.4 Importing the data",
    "text": "1.4 Importing the data\nIn this section, you will learn how to import the following geospatial data into R by using st_read() of sfpackage:\n\nMP14_SUBZONE_WEB_PL, a polygon feature layer in ESRI shapefile format,\nCyclingPath, a line feature layer in ESRI shapefile format, and\nPreSchool, a point feature layer in kml file format.\n\n\n1.4.1 Importing polygon feature data in shapefile format\nThe code chunk below uses st_read() function of sf package to import MP14_SUBZONE_WEB_PL shapefile into R as a polygon feature data frame. Note that when the input geospatial data is in shapefile format, two arguments will be used, namely: dsn to define the data path and layer to provide the shapefile name. Also note that no extension such as .shp, .dbf, .prj and .shx are needed.\n\nmpsz = st_read(dsn = \"data/geospatial/MasterPlan2014SubzoneBoundaryWebSHP\", layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `/Users/mingwei/Desktop/SMU/Y3S1/IS415/xXxPMWxXx/IS415-GAA/Hands-on_Ex/Hands-on_Ex01/data/geospatial/MasterPlan2014SubzoneBoundaryWebSHP' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\n\n\n1.4.2 Importing polyline feature data in shapefile form\nThe code chunk below uses st_read() function of sf package to import CyclingPath shapefile into R as line feature data frame.\n\ncyclingpath = st_read(dsn = \"data/geospatial/CyclingPath_Jul2024\", layer = \"CyclingPathGazette\")\n\nReading layer `CyclingPathGazette' from data source \n  `/Users/mingwei/Desktop/SMU/Y3S1/IS415/xXxPMWxXx/IS415-GAA/Hands-on_Ex/Hands-on_Ex01/data/geospatial/CyclingPath_Jul2024' \n  using driver `ESRI Shapefile'\nSimple feature collection with 3138 features and 2 fields\nGeometry type: MULTILINESTRING\nDimension:     XY\nBounding box:  xmin: 11854.32 ymin: 28347.98 xmax: 42644.17 ymax: 48948.15\nProjected CRS: SVY21\n\n\n\n\n1.4.3 Importing GIS data in kml format\nThe PreSchoolsLocation is in kml format. The code chunk below will be used to import the kml into R. Notice that in the code chunk below, the complete path and the kml file extension were provided.\n\npreschool = st_read(\"data/geospatial/PreSchoolsLocation.kml\")\n\nReading layer `PRESCHOOLS_LOCATION' from data source \n  `/Users/mingwei/Desktop/SMU/Y3S1/IS415/xXxPMWxXx/IS415-GAA/Hands-on_Ex/Hands-on_Ex01/data/geospatial/PreSchoolsLocation.kml' \n  using driver `LIBKML'\nSimple feature collection with 2290 features and 16 fields\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6878 ymin: 1.247759 xmax: 103.9897 ymax: 1.462134\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\n\n\nThe message above reveals that preschool is a point feature data frame. There are a total of 2290 features and 16 fields. Different from the previous two simple feature data frame, preschool is in wgs84 coordinates system."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#checking-the-content-of-a-simple-feature-data-frame",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#checking-the-content-of-a-simple-feature-data-frame",
    "title": "Hands-on Exercise 01:Geospatial Data Science with R",
    "section": "1.5 Checking the Content of A Simple Feature Data Frame",
    "text": "1.5 Checking the Content of A Simple Feature Data Frame\nIn this sub-section, you will learn different ways to retrieve information related to the content of a simple feature data frame.\n\n1.5.1 Working with st_geometry()\nThe column in the sf data.frame that contains the geometries is a list, of class sfc. We can retrieve the geometry list-column in this case by mpsz$geom or mpsz[[1]], but the more general way uses st_geometry() as shown in the code chunk below.\n\nst_geometry(mpsz)\n\nGeometry set for 323 features \nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\nFirst 5 geometries:\n\n\nMULTIPOLYGON (((31495.56 30140.01, 31980.96 296...\n\n\nMULTIPOLYGON (((29092.28 30021.89, 29119.64 300...\n\n\nMULTIPOLYGON (((29932.33 29879.12, 29947.32 298...\n\n\nMULTIPOLYGON (((27131.28 30059.73, 27088.33 297...\n\n\nMULTIPOLYGON (((26451.03 30396.46, 26440.47 303...\n\n\n\n\n1.5.2 Working with glimpse()\nBeside the basic feature information, we also would like to learn more about the associated attribute information in the data frame. This is the time you will find glimpse() of dplyr. very handy as shown in the code chunk below.\n\nglimpse(mpsz)\n\nRows: 323\nColumns: 16\n$ OBJECTID   &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …\n$ SUBZONE_NO &lt;int&gt; 1, 1, 3, 8, 3, 7, 9, 2, 13, 7, 12, 6, 1, 5, 1, 1, 3, 2, 2, …\n$ SUBZONE_N  &lt;chr&gt; \"MARINA SOUTH\", \"PEARL'S HILL\", \"BOAT QUAY\", \"HENDERSON HIL…\n$ SUBZONE_C  &lt;chr&gt; \"MSSZ01\", \"OTSZ01\", \"SRSZ03\", \"BMSZ08\", \"BMSZ03\", \"BMSZ07\",…\n$ CA_IND     &lt;chr&gt; \"Y\", \"Y\", \"Y\", \"N\", \"N\", \"N\", \"N\", \"Y\", \"N\", \"N\", \"N\", \"N\",…\n$ PLN_AREA_N &lt;chr&gt; \"MARINA SOUTH\", \"OUTRAM\", \"SINGAPORE RIVER\", \"BUKIT MERAH\",…\n$ PLN_AREA_C &lt;chr&gt; \"MS\", \"OT\", \"SR\", \"BM\", \"BM\", \"BM\", \"BM\", \"SR\", \"QT\", \"QT\",…\n$ REGION_N   &lt;chr&gt; \"CENTRAL REGION\", \"CENTRAL REGION\", \"CENTRAL REGION\", \"CENT…\n$ REGION_C   &lt;chr&gt; \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\",…\n$ INC_CRC    &lt;chr&gt; \"5ED7EB253F99252E\", \"8C7149B9EB32EEFC\", \"C35FEFF02B13E0E5\",…\n$ FMEL_UPD_D &lt;date&gt; 2014-12-05, 2014-12-05, 2014-12-05, 2014-12-05, 2014-12-05…\n$ X_ADDR     &lt;dbl&gt; 31595.84, 28679.06, 29654.96, 26782.83, 26201.96, 25358.82,…\n$ Y_ADDR     &lt;dbl&gt; 29220.19, 29782.05, 29974.66, 29933.77, 30005.70, 29991.38,…\n$ SHAPE_Leng &lt;dbl&gt; 5267.381, 3506.107, 1740.926, 3313.625, 2825.594, 4428.913,…\n$ SHAPE_Area &lt;dbl&gt; 1630379.27, 559816.25, 160807.50, 595428.89, 387429.44, 103…\n$ geometry   &lt;MULTIPOLYGON [m]&gt; MULTIPOLYGON (((31495.56 30..., MULTIPOLYGON (…\n\n\nglimpse() report reveals the data type of each fields. For example FMEL-UPD_D field is in date data type and X_ADDR, Y_ADDR, SHAPE_L and SHAPE_AREA fields are all in double-precision values.\n\n\n1.5.3 Working with head()\nSometimes we would like to reveal complete information of a feature object, this is the job of head() of Base R\n\nhead(mpsz, n=5)\n\nSimple feature collection with 5 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 25867.68 ymin: 28369.47 xmax: 32362.39 ymax: 30435.54\nProjected CRS: SVY21\n  OBJECTID SUBZONE_NO      SUBZONE_N SUBZONE_C CA_IND      PLN_AREA_N\n1        1          1   MARINA SOUTH    MSSZ01      Y    MARINA SOUTH\n2        2          1   PEARL'S HILL    OTSZ01      Y          OUTRAM\n3        3          3      BOAT QUAY    SRSZ03      Y SINGAPORE RIVER\n4        4          8 HENDERSON HILL    BMSZ08      N     BUKIT MERAH\n5        5          3        REDHILL    BMSZ03      N     BUKIT MERAH\n  PLN_AREA_C       REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR\n1         MS CENTRAL REGION       CR 5ED7EB253F99252E 2014-12-05 31595.84\n2         OT CENTRAL REGION       CR 8C7149B9EB32EEFC 2014-12-05 28679.06\n3         SR CENTRAL REGION       CR C35FEFF02B13E0E5 2014-12-05 29654.96\n4         BM CENTRAL REGION       CR 3775D82C5DDBEFBD 2014-12-05 26782.83\n5         BM CENTRAL REGION       CR 85D9ABEF0A40678F 2014-12-05 26201.96\n    Y_ADDR SHAPE_Leng SHAPE_Area                       geometry\n1 29220.19   5267.381  1630379.3 MULTIPOLYGON (((31495.56 30...\n2 29782.05   3506.107   559816.2 MULTIPOLYGON (((29092.28 30...\n3 29974.66   1740.926   160807.5 MULTIPOLYGON (((29932.33 29...\n4 29933.77   3313.625   595428.9 MULTIPOLYGON (((27131.28 30...\n5 30005.70   2825.594   387429.4 MULTIPOLYGON (((26451.03 30...\n\n\nNote: One of the useful argument of head() is it allows user to select the numbers of record to display (i.e. the n argument)."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#plotting-the-geospatial-data",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#plotting-the-geospatial-data",
    "title": "Hands-on Exercise 01:Geospatial Data Science with R",
    "section": "1.6 Plotting the Geospatial Data",
    "text": "1.6 Plotting the Geospatial Data\nIn geospatial data science, by looking at the feature information is not enough. We are also interested to visualise the geospatial features. This is the time you will find plot() of R Graphic comes in very handy as shown in the code chunk below.\n\nplot(mpsz)\n\nWarning: plotting the first 9 out of 15 attributes; use max.plot = 15 to plot\nall\n\n\n\n\n\n\n\n\n\nThe default plot of an sf object is a multi-plot of all attributes, up to a reasonable maximum as shown above. We can, however, choose to plot only the geometry by using the code chunk below.\n\nplot(st_geometry(mpsz))\n\n\n\n\n\n\n\n\nAlternatively, we can also choose the plot the sf object by using a specific attribute as shown in the code chunk below.\n\nplot(mpsz[\"PLN_AREA_N\"])\n\n\n\n\n\n\n\n\nNote: plot() is mean for plotting the geospatial object for quick look. For high cartographic quality plot, other R package such as tmap should be used."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#working-with-projection",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#working-with-projection",
    "title": "Hands-on Exercise 01:Geospatial Data Science with R",
    "section": "1.7 Working with Projection",
    "text": "1.7 Working with Projection\nMap projection is an important property of a geospatial data. In order to perform geoprocessing using two geospatial data, we need to ensure that both geospatial data are projected using similar coordinate system.\nIn this section, you will learn how to project a simple feature data frame from one coordinate system to another coordinate system. The technical term of this process is called projection transformation.\n\n1.7.1 Assigning EPSG code to simple feature data frame\nOne of the common issue that can happen during importing geospatial data into R is that the coordinate system of the source data was either missing (such as due to missing .proj for ESRI shapefile) or wrongly assigned during the importing process.\nThis is an example the coordinate system of mpsz simple feature data frame by using st_crs() of sfpackage as shown in the code chunk below.\n\nst_crs(mpsz)\n\nCoordinate Reference System:\n  User input: SVY21 \n  wkt:\nPROJCRS[\"SVY21\",\n    BASEGEOGCRS[\"SVY21[WGS84]\",\n        DATUM[\"World Geodetic System 1984\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]],\n            ID[\"EPSG\",6326]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"Degree\",0.0174532925199433]]],\n    CONVERSION[\"unnamed\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]]]\n\n\nAlthough mpsz data frame is projected in svy21 but when we read until the end of the print, it indicates that the EPSG is 9001. This is a wrong EPSG code because the correct EPSG code for svy21 should be 3414.\nIn order to assign the correct EPSG code to mpsz data frame, st_set_crs() of sf package is used as shown in the code chunk below.\n\nmpsz3414 &lt;- st_set_crs(mpsz,3414)\n\nWarning: st_crs&lt;- : replacing crs does not reproject data; use st_transform for\nthat\n\n\nCheck the CSR again by using the code chunk below\n\nst_crs(mpsz3414)\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\n\nThe EPSG code is 3414 now.\n\n\n1.7.2 Transforming the projection of preschool from wgs84 to svy21.\nIn geospatial analytics, it is very common for us to transform the original data from geographic coordinate system to projected coordinate system. This is because geographic coordinate system is not appropriate if the analysis need to use distance or/and area measurements.\nLet us take preschool simple feature data frame as an example. The print below reveals that it is in wgs84 coordinate system.\nGeometry set for 2290 features \nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6878 ymin: 1.247759 xmax: 103.9897 ymax: 1.462134\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\nFirst 5 geometries:\nThis is a scenario that st_set_crs() is not appropriate and st_transform() of sf package should be used. This is because we need to reproject preschool from one coordinate system to another coordinate system mathemetically.\nLet us perform the projection transformation by using the code chunk below.\n\npreschool3414 &lt;- st_transform(preschool,crs = 3414)\nhead(preschool3414, n=1)\n\nSimple feature collection with 1 feature and 16 fields\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 25089.46 ymin: 31299.16 xmax: 25089.46 ymax: 31299.16\nz_range:       zmin: 0 zmax: 0\nProjected CRS: SVY21 / Singapore TM\n   Name\n1 kml_1\n                                                                                                                                                                                                                                                                                                                                                                                                  description\n1 &lt;center&gt;&lt;table&gt;&lt;tr&gt;&lt;th colspan='2' align='center'&gt;&lt;em&gt;Attributes&lt;/em&gt;&lt;/th&gt;&lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt;\\n&lt;th&gt;CENTRE_NAME&lt;/th&gt;\\n&lt;td&gt;CHILDREN'S COVE PRESCHOOL PTE.LTD.&lt;/td&gt;\\n&lt;/tr&gt;&lt;tr bgcolor=\"\"&gt;\\n&lt;th&gt;CENTRE_CODE&lt;/th&gt;\\n&lt;td&gt;PT9390&lt;/td&gt;\\n&lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt;\\n&lt;th&gt;INC_CRC&lt;/th&gt;\\n&lt;td&gt;498CC9FE48CC94D4&lt;/td&gt;\\n&lt;/tr&gt;&lt;tr bgcolor=\"\"&gt;\\n&lt;th&gt;FMEL_UPD_D&lt;/th&gt;\\n&lt;td&gt;20211201093631&lt;/td&gt;\\n&lt;/tr&gt;&lt;/table&gt;&lt;/center&gt;\n  timestamp begin  end altitudeMode tessellate extrude visibility drawOrder\n1      &lt;NA&gt;  &lt;NA&gt; &lt;NA&gt;         &lt;NA&gt;         -1       0         -1        NA\n  icon                        CENTRE_NAME CENTRE_CODE          INC_CRC\n1 &lt;NA&gt; CHILDREN'S COVE PRESCHOOL PTE.LTD.      PT9390 498CC9FE48CC94D4\n      FMEL_UPD_D snippet                      geometry\n1 20211201093631         POINT Z (25089.46 31299.16 0)\n\n\nNote: In practice, we need find out the appropriate project coordinate system to use before performing the projection transformation."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#importing-and-converting-an-aspatial-data",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#importing-and-converting-an-aspatial-data",
    "title": "Hands-on Exercise 01:Geospatial Data Science with R",
    "section": "1.8 Importing and Converting An Aspatial Data",
    "text": "1.8 Importing and Converting An Aspatial Data\nIn practice, it is not unusual that we will come across data such as listing of Inside Airbnb. We call this kind of data aspatial data. This is because it is not a geospatial data but among the data fields, there are two fields that capture the x- and y-coordinates of the data points.\nIn this section, you will learn how to import an aspatial data into R environment and save it as a tibble data frame. Next, you will convert it into a simple feature data frame.\nFor the purpose of this exercise, the listings.csv data downloaded from AirBnb will be used.\n\n1.8.1 Importing the aspatial data\nSince listings data set is in csv file format, we will use read_csv() of readr package to import listing.csv as shown the code chunk below. The output R object is called listings and it is a tibble data frame.\n\nlistings &lt;- read_csv(\"/Users/mingwei/Desktop/SMU/Y3S1/IS415/xXxPMWxXx/IS415-GAA/Hands-on_Ex/Hands-on_Ex01/data//aspatial/listings.csv\")\n\nRows: 3540 Columns: 18\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr   (6): name, host_name, neighbourhood_group, neighbourhood, room_type, l...\ndbl  (11): id, host_id, latitude, longitude, price, minimum_nights, number_o...\ndate  (1): last_review\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nAfter importing the data file into R, it is important for us to examine if the data file has been imported correctly.\nThe code chunk below shows list() of Base R instead of glimpse() is used to do the job.\n\nlist(listings) \n\n[[1]]\n# A tibble: 3,540 × 18\n       id name      host_id host_name neighbourhood_group neighbourhood latitude\n    &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;               &lt;chr&gt;            &lt;dbl&gt;\n 1  71609 Ensuite …  367042 Belinda   East Region         Tampines          1.35\n 2  71896 B&B  Roo…  367042 Belinda   East Region         Tampines          1.35\n 3  71903 Room 2-n…  367042 Belinda   East Region         Tampines          1.35\n 4 275343 10min wa… 1439258 Kay       Central Region      Bukit Merah       1.29\n 5 275344 15 mins … 1439258 Kay       Central Region      Bukit Merah       1.29\n 6 289234 Booking …  367042 Belinda   East Region         Tampines          1.34\n 7 294281 5 mins w… 1521514 Elizabeth Central Region      Newton            1.31\n 8 324945 Comforta… 1439258 Kay       Central Region      Bukit Merah       1.29\n 9 330095 Relaxing… 1439258 Kay       Central Region      Bukit Merah       1.29\n10 344803 Budget s…  367042 Belinda   East Region         Tampines          1.35\n# ℹ 3,530 more rows\n# ℹ 11 more variables: longitude &lt;dbl&gt;, room_type &lt;chr&gt;, price &lt;dbl&gt;,\n#   minimum_nights &lt;dbl&gt;, number_of_reviews &lt;dbl&gt;, last_review &lt;date&gt;,\n#   reviews_per_month &lt;dbl&gt;, calculated_host_listings_count &lt;dbl&gt;,\n#   availability_365 &lt;dbl&gt;, number_of_reviews_ltm &lt;dbl&gt;, license &lt;chr&gt;\n\n\nThe output reveals that listing tibble data frame consists of 3,540 rows and 18 columns. Two useful fields we are going to use in the next phase are latitude and longitude. Note that they are in decimal degree format. As a best guess, we will assume that the data is in wgs84 Geographic Coordinate System.\n\n\n1.8.2 Creating a simple feature data frame from an aspatial data frame\nThe code chunk below converts listing data frame into a simple feature data frame by using st_as_sf()of sf packages\n\nlistings_sf &lt;- st_as_sf(listings, \n                       coords = c(\"longitude\", \"latitude\"),\n                       crs=4326) %&gt;%\n  st_transform(crs = 3414)\n\nThings to learn from the arguments above:\n\ncoords argument requires you to provide the column name of the x-coordinates first then followed by the column name of the y-coordinates.\ncrs argument requires you to provide the coordinates system in epsg format. EPSG: 4326 is wgs84 Geographic Coordinate System and EPSG: 3414 is Singapore SVY21 Projected Coordinate System. You can search for other country’s epsg code by referring to epsg.io.\n%&gt;% is used to nest st_transform() to transform the newly created simple feature data frame into svy21 projected coordinates system.\n\nLet us examine the content of this newly created simple feature data frame.\n\nglimpse(listings_sf)\n\nRows: 3,540\nColumns: 17\n$ id                             &lt;dbl&gt; 71609, 71896, 71903, 275343, 275344, 28…\n$ name                           &lt;chr&gt; \"Ensuite Room (Room 1 & 2) near EXPO\", …\n$ host_id                        &lt;dbl&gt; 367042, 367042, 367042, 1439258, 143925…\n$ host_name                      &lt;chr&gt; \"Belinda\", \"Belinda\", \"Belinda\", \"Kay\",…\n$ neighbourhood_group            &lt;chr&gt; \"East Region\", \"East Region\", \"East Reg…\n$ neighbourhood                  &lt;chr&gt; \"Tampines\", \"Tampines\", \"Tampines\", \"Bu…\n$ room_type                      &lt;chr&gt; \"Private room\", \"Private room\", \"Privat…\n$ price                          &lt;dbl&gt; NA, 80, 80, 50, 50, NA, 85, 65, 45, 54,…\n$ minimum_nights                 &lt;dbl&gt; 92, 92, 92, 180, 180, 92, 92, 180, 180,…\n$ number_of_reviews              &lt;dbl&gt; 19, 24, 46, 20, 16, 12, 131, 17, 5, 60,…\n$ last_review                    &lt;date&gt; 2020-01-17, 2019-10-13, 2020-01-09, 20…\n$ reviews_per_month              &lt;dbl&gt; 0.12, 0.15, 0.29, 0.15, 0.11, 0.08, 0.8…\n$ calculated_host_listings_count &lt;dbl&gt; 6, 6, 6, 49, 49, 6, 7, 49, 49, 6, 7, 7,…\n$ availability_365               &lt;dbl&gt; 89, 148, 90, 62, 0, 88, 365, 0, 0, 365,…\n$ number_of_reviews_ltm          &lt;dbl&gt; 0, 0, 0, 0, 2, 0, 0, 1, 1, 1, 0, 0, 0, …\n$ license                        &lt;chr&gt; NA, NA, NA, \"S0399\", \"S0399\", NA, NA, \"…\n$ geometry                       &lt;POINT [m]&gt; POINT (41972.5 36390.05), POINT (…\n\n\nTable above shows the content of listing_sf. Notice that a new column called geometry has been added into the data frame. On the other hand, the longitude and latitude columns have been dropped from the data frame."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#geoprocessing-with-sf-package",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#geoprocessing-with-sf-package",
    "title": "Hands-on Exercise 01:Geospatial Data Science with R",
    "section": "1.9 Geoprocessing with sf package",
    "text": "1.9 Geoprocessing with sf package\nBesides providing functions to handling (i.e. importing, exporting, assigning projection, transforming projection etc) geospatial data, sf package also offers a wide range of geoprocessing (also known as GIS analysis) functions.\nIn this section, you will learn how to perform two commonly used geoprocessing functions, namely buffering and point in polygon count.\n\n1.9.1 Buffering\nThe scenario:\nThe authority is planning to upgrade the exiting cycling path. To do so, they need to acquire 5 metres of reserved land on the both sides of the current cycling path. You are tasked to determine the extend of the land need to be acquired and their total area.\nThe solution:\nFirstly, st_buffer() of sf package is used to compute the 5-meter buffers around cycling paths\n\nbuffer_cycling &lt;- st_buffer(cyclingpath, dist=5, nQuadSegs = 30)\n\nThis is followed by calculating the area of the buffers as shown in the code chunk below.\n\nbuffer_cycling$AREA &lt;- st_area(buffer_cycling)\n\nLastly, sum() of Base R will be used to derive the total land involved\n\nsum(buffer_cycling$AREA)\n\n2218855 [m^2]\n\n\n\n\n1.9.2 Point-in-polygon count\nThe scenario:\nA pre-school service group want to find out the numbers of pre-schools in each Planning Subzone.\nThe solution:\nThe code chunk below performs two operations at one go. Firstly, identify pre-schools located inside each Planning Subzone by using st_intersects(). Next, length() of Base R is used to calculate numbers of pre-schools that fall inside each planning subzone.\n\nmpsz3414$`PreSch Count`&lt;- lengths(st_intersects(mpsz3414, preschool3414))\n\nWarning: Do not confuse with st_intersection().\nCan check the summary statistics of the newly derived PreSch Count field by using summary() as shown in the code chunk below.\n\nsummary(mpsz3414$`PreSch Count`)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   0.00    0.00    4.00    7.09   10.00   72.00 \n\n\nTo list the planning subzone with the most number of pre-school, the top_n() of dplyr package is used as shown in the code chunk below.\n\ntop_n(mpsz3414, 1, `PreSch Count`)\n\nSimple feature collection with 1 feature and 16 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 39655.33 ymin: 35966 xmax: 42940.57 ymax: 38622.37\nProjected CRS: SVY21 / Singapore TM\n  OBJECTID SUBZONE_NO     SUBZONE_N SUBZONE_C CA_IND PLN_AREA_N PLN_AREA_C\n1      189          2 TAMPINES EAST    TMSZ02      N   TAMPINES         TM\n     REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR   Y_ADDR SHAPE_Leng\n1 EAST REGION       ER 21658EAAF84F4D8D 2014-12-05 41122.55 37392.39   10180.62\n  SHAPE_Area                       geometry PreSch Count\n1    4339824 MULTIPOLYGON (((42196.76 38...           72\n\n\nCalculate the density of pre-school by planning subzone.\nFirstly, the code chunk below uses st_area() of sf package to derive the area of each planning subzone.\n\nmpsz3414$Area &lt;- mpsz3414 %&gt;%\n  st_area()\n\nNext, mutate() of dplyr package is used to compute the density by using the code chunk below.\n\nmpsz3414 &lt;- mpsz3414 %&gt;%\n  mutate(`PreSch Density` = `PreSch Count`/Area * 1000000)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#exploratory-data-analysis-eda",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#exploratory-data-analysis-eda",
    "title": "Hands-on Exercise 01:Geospatial Data Science with R",
    "section": "1.10 Exploratory Data Analysis (EDA)",
    "text": "1.10 Exploratory Data Analysis (EDA)\nIn practice, many geospatial analytics start with Exploratory Data Analysis. In this section, you will learn how to use appropriate ggplot2 functions to create functional and yet truthful statistical graphs for EDA purposes.\nFirstly, we will plot a histogram to reveal the distribution of PreSch Density. Conventionally, hist() of R Graphics will be used as shown in the code chunk below.\n\nhist(mpsz3414$`PreSch Density`)\n\n\n\n\n\n\n\n\nAlthough the syntax is very easy to use however the output is far from meeting publication quality. Furthermore, the function has limited room for further customisation.\nIn the code chunk below, appropriate ggplot2 functions will be used.\n\nggplot(data=mpsz3414, \n       aes(x= as.numeric(`PreSch Density`)))+\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\") +\n  labs(title = \"Are pre-school even distributed in Singapore?\",\n       subtitle= \"There are many planning sub-zones with a single pre-school, on the other hand, \\nthere are two planning sub-zones with at least 20 pre-schools\",\n      x = \"Pre-school density (per km sq)\",\n      y = \"Frequency\")\n\n\n\n\n\n\n\n\nUsing ggplot2 method, plot a scatterplot showing the relationship between Pre-school Density and Pre-school Count.\n\nggplot(data=mpsz3414, \n       aes(y = `PreSch Count`, \n           x= as.numeric(`PreSch Density`)))+\n  geom_point(color=\"black\", \n             fill=\"light blue\") +\n  xlim(0, 40) +\n  ylim(0, 40) +\n  labs(title = \"\",\n      x = \"Pre-school density (per km sq)\",\n      y = \"Pre-school count\")\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`)."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This website for IS415(AY24/25 T1) course work submission\n-Mingwei"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04/In-class_Ex04.html",
    "href": "In-class_Ex/In-class_Ex04/In-class_Ex04.html",
    "title": "In Class Exercise 04: Spatio-Temporal Point Patterns Analysis",
    "section": "",
    "text": "A spatio-temporal point process (also called space-time) is a random collection of points, where each point represents the time and location of an event. Examples of events include incidence of disease, sightings or births of a species, or the occurrences of fires, earthquakes, lightning strikes, tsunamis, or volcanic eruptions.\n\n\n\n\n\n\nNote\n\n\n\nSpatio-temporal are recorded in three-dimension: longitude, latitude and time."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#importing-study-area",
    "href": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#importing-study-area",
    "title": "In Class Exercise 04: Spatio-Temporal Point Patterns Analysis",
    "section": "4.1 Importing study area",
    "text": "4.1 Importing study area\n\nkbb &lt;- st_read(dsn = \"data/rawdata\", \n               layer = \"Kepulauan_Bangka_Belitung\") %&gt;%\n  st_transform(crs = 32748)\n\nReading layer `Kepulauan_Bangka_Belitung' from data source \n  `/Users/mingwei/Desktop/SMU/Y3S1/IS415/xXxPMWxXx/IS415-GAA/In-class_Ex/In-class_Ex04/data/rawdata' \n  using driver `ESRI Shapefile'\nSimple feature collection with 298 features and 27 fields\nGeometry type: POLYGON\nDimension:     XYZ\nBounding box:  xmin: 105.1085 ymin: -3.116593 xmax: 106.8488 ymax: -1.501603\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\n\n\n\nkbb_sf &lt;- st_read(dsn = \"data/rawdata\", \n               layer = \"Kepulauan_Bangka_Belitung\") %&gt;%\n  st_union() %&gt;%\n  st_zm(drop = TRUE, what - \"ZM\") %&gt;%\n  st_transform(crs = 32748)\n\nReading layer `Kepulauan_Bangka_Belitung' from data source \n  `/Users/mingwei/Desktop/SMU/Y3S1/IS415/xXxPMWxXx/IS415-GAA/In-class_Ex/In-class_Ex04/data/rawdata' \n  using driver `ESRI Shapefile'\nSimple feature collection with 298 features and 27 fields\nGeometry type: POLYGON\nDimension:     XYZ\nBounding box:  xmin: 105.1085 ymin: -3.116593 xmax: 106.8488 ymax: -1.501603\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\n\n\n\n\n\n\n\n\nNote\n\n\n\nDropping ZM coordinate to avoid the error when we converting OWIN, as OWIN only need x and y coordinate."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#converting-owin",
    "href": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#converting-owin",
    "title": "In Class Exercise 04: Spatio-Temporal Point Patterns Analysis",
    "section": "4.2 Converting OWIN",
    "text": "4.2 Converting OWIN\nTo convert kbb_sf into OWIN object.\n\nkbb_owin &lt;- as.owin(kbb_sf)\nplot(kbb_owin)\n\n\n\n\n\n\n\n\nTo check the data is correct.\n\nclass(kbb_owin)\n\n[1] \"owin\""
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#importing-and-preparing-forest-fire-data",
    "href": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#importing-and-preparing-forest-fire-data",
    "title": "In Class Exercise 04: Spatio-Temporal Point Patterns Analysis",
    "section": "4.3 Importing and Preparing Forest Fire data",
    "text": "4.3 Importing and Preparing Forest Fire data\nst_as_sf is change from aspatial to spatial data with the projection 4326(WGS84).( it will create a geometry field, based on longitude and latitude field. Note, must be longitude follow by latitude).\nThen transform the CRS to 32748(UTM 48) same as the kbb_owin.\n\nfire_sf &lt;- read_csv(\"data/rawdata/forestfires.csv\") %&gt;%\n  st_as_sf(coords = c(\"longitude\" , \"latitude\"),\n           crs = 4326) %&gt;%\n  st_transform(crs = 32748)\n\nTo convert data type of acq_date to numeric using lubridate packages(included in tidyverse).\n\nfire_sf &lt;- fire_sf %&gt;%\n  mutate(DayofYear = yday(acq_date)) %&gt;%\n  mutate(Month_num = month(acq_date)) %&gt;%\n  mutate(Month_fac = month(acq_date,\n                          label = TRUE,\n                          abbr = FALSE))"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#overview-of-the-plot",
    "href": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#overview-of-the-plot",
    "title": "In Class Exercise 04: Spatio-Temporal Point Patterns Analysis",
    "section": "5.1 Overview of the plot",
    "text": "5.1 Overview of the plot\n\ntm_shape(kbb_sf) +\n  tm_polygons() +\n  tm_shape(fire_sf) +\n  tm_dots()"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#visualizing-geographic-distribution-of-forest-fires-by-month",
    "href": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#visualizing-geographic-distribution-of-forest-fires-by-month",
    "title": "In Class Exercise 04: Spatio-Temporal Point Patterns Analysis",
    "section": "5.2 Visualizing geographic distribution of forest fires by month",
    "text": "5.2 Visualizing geographic distribution of forest fires by month\n\ntm_shape(kbb_sf) +\n  tm_polygons() +\n  tm_shape(fire_sf) +\n  tm_dots(size = 0.1) +\n  tm_facets(by=\"Month_fac\",\n            free.coords = FALSE, # To avoid the zoom issue of the display map\n            drop.units = TRUE)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#extracting-forest-fires-by-month",
    "href": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#extracting-forest-fires-by-month",
    "title": "In Class Exercise 04: Spatio-Temporal Point Patterns Analysis",
    "section": "6.1 Extracting forest fires by month",
    "text": "6.1 Extracting forest fires by month\nTo remove unwanted fields from fire_sf data frame. This because as.ppp only need the mark field and geometry field from the input sf data frame.\n\nfire_month &lt;- fire_sf %&gt;%\n  select(Month_num)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#creating-ppp",
    "href": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#creating-ppp",
    "title": "In Class Exercise 04: Spatio-Temporal Point Patterns Analysis",
    "section": "6.2 Creating ppp",
    "text": "6.2 Creating ppp\n\nfire_month_ppp &lt;-as.ppp(fire_month)\nfire_month_ppp\n\nMarked planar point pattern: 741 points\nmarks are numeric, of storage type  'double'\nwindow: rectangle = [521564.1, 695791] x [9658137, 9828767] units\n\n\nTo check if there is duplicated point.\n\nany(duplicated(fire_month_ppp))\n\n[1] FALSE"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#including-owin-object",
    "href": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#including-owin-object",
    "title": "In Class Exercise 04: Spatio-Temporal Point Patterns Analysis",
    "section": "6.3 Including Owin object",
    "text": "6.3 Including Owin object\n\nfire_month_owin &lt;- fire_month_ppp[kbb_owin]\nsummary(fire_month_owin)\n\nMarked planar point pattern:  741 points\nAverage intensity 6.424519e-08 points per square unit\n\nCoordinates are given to 10 decimal places\n\nmarks are numeric, of type 'double'\nSummary:\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.000   8.000   9.000   8.579  10.000  12.000 \n\nWindow: polygonal boundary\n2 separate polygons (no holes)\n           vertices        area relative.area\npolygon 1     47493 11533600000      1.00e+00\npolygon 2       256      306427      2.66e-05\nenclosing rectangle: [512066.8, 705559.4] x [9655398, 9834006] units\n                     (193500 x 178600 units)\nWindow area = 11533900000 square units\nFraction of frame area: 0.334"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#computing-spatio-temporal-kde",
    "href": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#computing-spatio-temporal-kde",
    "title": "In Class Exercise 04: Spatio-Temporal Point Patterns Analysis",
    "section": "6.4 Computing Spatio-temporal KDE",
    "text": "6.4 Computing Spatio-temporal KDE\n\nst_kde &lt;- spattemp.density(fire_month_owin)\nsummary(st_kde)\n\nSpatiotemporal Kernel Density Estimate\n\nBandwidths\n  h = 15102.47 (spatial)\n  lambda = 0.0304 (temporal)\n\nNo. of observations\n  741 \n\nSpatial bound\n  Type: polygonal\n  2D enclosure: [512066.8, 705559.4] x [9655398, 9834006]\n\nTemporal bound\n  [1, 12]\n\nEvaluation\n  128 x 128 x 12 trivariate lattice\n  Density range: [1.233458e-27, 8.202976e-10]\n\n\nPlot from July to Dec because from previous plot by month, we can see that the majority of fire happen after July.\n\ntims &lt;- c(7,8,9,10,11,12)\npar(mfcol=c(2,3))\nfor(i in tims){\n  plot(st_kde, i ,\n       override.par=FALSE,\n       fix.range=TRUE,\n       main=paste(\"KDE at month\",i))\n}"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#extracting-by-day",
    "href": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#extracting-by-day",
    "title": "In Class Exercise 04: Spatio-Temporal Point Patterns Analysis",
    "section": "7.1 Extracting by day",
    "text": "7.1 Extracting by day\n\nfire_day &lt;- fire_sf %&gt;%\n  select(DayofYear)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#create-ppp",
    "href": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#create-ppp",
    "title": "In Class Exercise 04: Spatio-Temporal Point Patterns Analysis",
    "section": "7.2 Create ppp",
    "text": "7.2 Create ppp\n\nfire_day_ppp &lt;-as.ppp(fire_day)\nfire_day_ppp\n\nMarked planar point pattern: 741 points\nmarks are numeric, of storage type  'double'\nwindow: rectangle = [521564.1, 695791] x [9658137, 9828767] units"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#including-owin-object-1",
    "href": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#including-owin-object-1",
    "title": "In Class Exercise 04: Spatio-Temporal Point Patterns Analysis",
    "section": "7.3 Including Owin object",
    "text": "7.3 Including Owin object\n\nfire_day_owin &lt;- fire_day_ppp[kbb_owin]\nsummary(fire_day_owin)\n\nMarked planar point pattern:  741 points\nAverage intensity 6.424519e-08 points per square unit\n\nCoordinates are given to 10 decimal places\n\nmarks are numeric, of type 'double'\nSummary:\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   10.0   213.0   258.0   245.9   287.0   352.0 \n\nWindow: polygonal boundary\n2 separate polygons (no holes)\n           vertices        area relative.area\npolygon 1     47493 11533600000      1.00e+00\npolygon 2       256      306427      2.66e-05\nenclosing rectangle: [512066.8, 705559.4] x [9655398, 9834006] units\n                     (193500 x 178600 units)\nWindow area = 11533900000 square units\nFraction of frame area: 0.334"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#computing-spatio-temporal-kde-1",
    "href": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#computing-spatio-temporal-kde-1",
    "title": "In Class Exercise 04: Spatio-Temporal Point Patterns Analysis",
    "section": "7.4 Computing Spatio-temporal KDE",
    "text": "7.4 Computing Spatio-temporal KDE\n\n#kde_yday &lt;- spattemp.density(fire_day_owin)\n#summary(kde_yday)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05/In-class_Ex05.html",
    "href": "In-class_Ex/In-class_Ex05/In-class_Ex05.html",
    "title": "In Class Exercise 05: Spatial Weights and Applications",
    "section": "",
    "text": "Note\n\n\n\nGeographically referenced attribute =&gt; attribute that have a geometry attached to it. e.g polygon\nTwo type of relationship:\n\nAdjacency base =&gt; sharing a common boundary\n\nBinary Matrix =&gt; 0, 1; If the polygon is adjacency then it will be 1, else it will be 0.\n\nDistance base =&gt; easy for lines but difficult for polygon.\n\nWhere is the start point for polygon? =&gt; define a centre point for polygon.\nLimitation : To define the centre point for polygon, as each polygon have different size, so if using fixed distance, then sometime it would not be effective.\nTwo type of matrices:\n\nBinary =&gt; if the target is within the distance, then it will be 1\nInverse Weight =&gt; The closer to the target the higher the value will be.\n\n\n\nContiguity Neighbours:\n\nRooks Case (Commonly used) =&gt; sharing common edge.\nBishops Case =&gt; share the boundary at the corner.\nQueen Case (Commonly used) =&gt; sharing both edge and corner.\n\nLagged Contiguity(Only for adjacency base):\n\nDifferent contiguity neighbours case used will affect the lagged contiguity.\n1st order neighbour =&gt; immediate neighbor of the target location.\n2nd order neighbour =&gt; immediate neighbor of the 1st order neighbor.\nNoted: if the shape is hexagon, then different contiguity neighbours case would not affect the lagged contiguity."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#loading-the-package",
    "href": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#loading-the-package",
    "title": "In Class Exercise 05: Spatial Weights and Applications",
    "section": "2.1 Loading the package",
    "text": "2.1 Loading the package\n\npacman::p_load(sf, spdep, tmap, tidyverse,knitr, GWmodel)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#preparing-the-data",
    "href": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#preparing-the-data",
    "title": "In Class Exercise 05: Spatial Weights and Applications",
    "section": "2.2 Preparing the Data",
    "text": "2.2 Preparing the Data\nTo import Hunan shapefile.\nNote: Good to add _sf in naming the variable.\n\nhunan_sf &lt;- st_read(dsn = \"data/geospatial\",\n                 layer = \"Hunan\")\n\nTo import 2012 hunan GPD data from CSV.\n\nhunan2012 &lt;- read_csv(\"data/aspatial/Hunan_2012.csv\")\n\nJoin hunan_sf and hunan2012.\nNote: In order to join the data set, need to check whether there is a common field from the two data set.\n\nhunan_sf &lt;- left_join(hunan_sf,hunan2012) %&gt;%\n  select(1:3, 7, 15, 16, 31, 32)\n\nWrite the object into RDS format.\nNote: Good to write the data set into RDS, so that we can just read the data set each time and don’t need to run the above code again every time.\n\nwrite_rds(hunan_sf,\"data/rds/hunan_sf.rds\")\n\nTo read the data set from RDS file.\n\n# By adding echo: false =&gt; the code chunk would not be shown\nhunan_sf &lt;- read_rds(\"data/rds/hunan_sf.rds\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#converting-to-spatialpolygondataframe",
    "href": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#converting-to-spatialpolygondataframe",
    "title": "In Class Exercise 05: Spatial Weights and Applications",
    "section": "2.3 Converting to SpatialPolygonDataFrame",
    "text": "2.3 Converting to SpatialPolygonDataFrame\nAs for GWmodel cannot work well with sf object. Therefore, we need to convert sf object into sp object.\n\nhunan_sp &lt;- hunan_sf %&gt;%\n  as_Spatial()"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#determine-adaptive-bandwidth",
    "href": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#determine-adaptive-bandwidth",
    "title": "In Class Exercise 05: Spatial Weights and Applications",
    "section": "3.1 Determine adaptive bandwidth",
    "text": "3.1 Determine adaptive bandwidth\n\nbw_AIC &lt;- bw.gwr(GDPPC ~ 1,\n                 data = hunan_sp,\n                 approach = \"AIC\",\n                 adaptive = TRUE,\n                 kernel = \"bisquare\",\n                 longlat = T) # note because we never do projection, by having this means it will help to project the longlat and the unit will be in km.\n\nAdaptive bandwidth (number of nearest neighbours): 62 AICc value: 1923.156 \nAdaptive bandwidth (number of nearest neighbours): 46 AICc value: 1920.469 \nAdaptive bandwidth (number of nearest neighbours): 36 AICc value: 1917.324 \nAdaptive bandwidth (number of nearest neighbours): 29 AICc value: 1916.661 \nAdaptive bandwidth (number of nearest neighbours): 26 AICc value: 1914.897 \nAdaptive bandwidth (number of nearest neighbours): 22 AICc value: 1914.045 \nAdaptive bandwidth (number of nearest neighbours): 22 AICc value: 1914.045 \n\n\n\nbw_AIC\n\n[1] 22\n\n\n\nbw_CV &lt;- bw.gwr(GDPPC ~ 1,\n                 data = hunan_sp,\n                 approach = \"CV\",\n                 adaptive = TRUE,\n                 kernel = \"bisquare\",\n                 longlat = T) \n\nAdaptive bandwidth: 62 CV score: 15515442343 \nAdaptive bandwidth: 46 CV score: 14937956887 \nAdaptive bandwidth: 36 CV score: 14408561608 \nAdaptive bandwidth: 29 CV score: 14198527496 \nAdaptive bandwidth: 26 CV score: 13898800611 \nAdaptive bandwidth: 22 CV score: 13662299974 \nAdaptive bandwidth: 22 CV score: 13662299974 \n\n\n\nbw_CV\n\n[1] 22\n\n\n\n\n\n\n\n\nNote\n\n\n\nNotice that even we use different approach, both recommend the same number of neightbours =&gt; 22"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#fixed-bandwidth",
    "href": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#fixed-bandwidth",
    "title": "In Class Exercise 05: Spatial Weights and Applications",
    "section": "3.2 Fixed bandwidth",
    "text": "3.2 Fixed bandwidth\nBy using fixed bandwidth, it will recommend the fixed distance.\n\nbw_fixed_CV &lt;- bw.gwr(GDPPC ~ 1,\n                 data = hunan_sp,\n                 approach = \"CV\",\n                 adaptive = FALSE, # calculate fixed distance\n                 kernel = \"bisquare\",\n                 longlat = T) \n\nFixed bandwidth: 357.4897 CV score: 16265191728 \nFixed bandwidth: 220.985 CV score: 14954930931 \nFixed bandwidth: 136.6204 CV score: 14134185837 \nFixed bandwidth: 84.48025 CV score: 13693362460 \nFixed bandwidth: 52.25585 CV score: Inf \nFixed bandwidth: 104.396 CV score: 13891052305 \nFixed bandwidth: 72.17162 CV score: 13577893677 \nFixed bandwidth: 64.56447 CV score: 14681160609 \nFixed bandwidth: 76.8731 CV score: 13444716890 \nFixed bandwidth: 79.77877 CV score: 13503296834 \nFixed bandwidth: 75.07729 CV score: 13452450771 \nFixed bandwidth: 77.98296 CV score: 13457916138 \nFixed bandwidth: 76.18716 CV score: 13442911302 \nFixed bandwidth: 75.76323 CV score: 13444600639 \nFixed bandwidth: 76.44916 CV score: 13442994078 \nFixed bandwidth: 76.02523 CV score: 13443285248 \nFixed bandwidth: 76.28724 CV score: 13442844774 \nFixed bandwidth: 76.34909 CV score: 13442864995 \nFixed bandwidth: 76.24901 CV score: 13442855596 \nFixed bandwidth: 76.31086 CV score: 13442847019 \nFixed bandwidth: 76.27264 CV score: 13442846793 \nFixed bandwidth: 76.29626 CV score: 13442844829 \nFixed bandwidth: 76.28166 CV score: 13442845238 \nFixed bandwidth: 76.29068 CV score: 13442844678 \nFixed bandwidth: 76.29281 CV score: 13442844691 \nFixed bandwidth: 76.28937 CV score: 13442844698 \nFixed bandwidth: 76.2915 CV score: 13442844676 \nFixed bandwidth: 76.292 CV score: 13442844679 \nFixed bandwidth: 76.29119 CV score: 13442844676 \nFixed bandwidth: 76.29099 CV score: 13442844676 \nFixed bandwidth: 76.29131 CV score: 13442844676 \nFixed bandwidth: 76.29138 CV score: 13442844676 \nFixed bandwidth: 76.29126 CV score: 13442844676 \nFixed bandwidth: 76.29123 CV score: 13442844676 \n\nbw_fixed_AIC &lt;- bw.gwr(GDPPC ~ 1,\n                 data = hunan_sp,\n                 approach = \"AIC\",\n                 adaptive = FALSE, # calculate fixed distance\n                 kernel = \"bisquare\",\n                 longlat = T) \n\nFixed bandwidth: 357.4897 AICc value: 1927.631 \nFixed bandwidth: 220.985 AICc value: 1921.547 \nFixed bandwidth: 136.6204 AICc value: 1919.993 \nFixed bandwidth: 84.48025 AICc value: 1940.603 \nFixed bandwidth: 168.8448 AICc value: 1919.457 \nFixed bandwidth: 188.7606 AICc value: 1920.007 \nFixed bandwidth: 156.5362 AICc value: 1919.41 \nFixed bandwidth: 148.929 AICc value: 1919.527 \nFixed bandwidth: 161.2377 AICc value: 1919.392 \nFixed bandwidth: 164.1433 AICc value: 1919.403 \nFixed bandwidth: 159.4419 AICc value: 1919.393 \nFixed bandwidth: 162.3475 AICc value: 1919.394 \nFixed bandwidth: 160.5517 AICc value: 1919.391 \n\n\nThe unit is in km.\nNotice that the different is very huge.\n\nbw_fixed_CV\n\n[1] 76.29126\n\nbw_fixed_AIC\n\n[1] 160.5517"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#computing-geographically-weighted-summary-statistics",
    "href": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#computing-geographically-weighted-summary-statistics",
    "title": "In Class Exercise 05: Spatial Weights and Applications",
    "section": "3.2 Computing geographically weighted summary statistics",
    "text": "3.2 Computing geographically weighted summary statistics\n\ngwstat &lt;- gwss(data = hunan_sp,\n               vars = \"GDPPC\",\n               bw = bw_AIC,\n               kernel = \"bisquare\",\n               adaptive = TRUE,\n               longlat = T)\n\n\n\n\n\n\n\nNote\n\n\n\nFor bw, if the adaptive field need to be the same, as TRUE is number of neightbour and FALSE is the distance.\ngwss =&gt; the calculation will include itself."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#preparing-the-output-data",
    "href": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#preparing-the-output-data",
    "title": "In Class Exercise 05: Spatial Weights and Applications",
    "section": "3.3 Preparing the output data",
    "text": "3.3 Preparing the output data\nExtract SDF object from gwstat into data frame.\n\ngwstat_df &lt;- as.data.frame(gwstat$SDF)\n\n\n\n\n\n\n\nWarning\n\n\n\nDo not apply sorting when extracting into the data frame. It will affect the data when we doing cbind.\n\n\ncbind append the gwstat_df into hunan_sf.\n\nhunan_gstat &lt;- cbind(hunan_sf,gwstat_df)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#visualising-geographically-weighted-summary-statistics",
    "href": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#visualising-geographically-weighted-summary-statistics",
    "title": "In Class Exercise 05: Spatial Weights and Applications",
    "section": "3.4 Visualising geographically weighted summary statistics",
    "text": "3.4 Visualising geographically weighted summary statistics\n\ntm_shape(hunan_gstat) +\n  tm_fill(\"GDPPC_LM\",\n          n = 5,\n          style = \"quantile\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Distribution of geographically weighted mean\",\n            main.title.position = \"center\",\n            main.title.size = 2.0,\n            legend.text.size = 1.2,\n            legend.height = 1.50,\n            legend.width = 1.50,\n            frame = TRUE)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02/data/geospatial/MPSZNoSeaSHP/MPSZ-2019.html",
    "href": "In-class_Ex/In-class_Ex02/data/geospatial/MPSZNoSeaSHP/MPSZ-2019.html",
    "title": "IS415-GAA",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n        0 0     false"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex01/data/MPSZ-2019.html",
    "href": "In-class_Ex/In-class_Ex01/data/MPSZ-2019.html",
    "title": "IS415-GAA",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n        0 0     false"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex06/In-class_Ex06.html",
    "href": "In-class_Ex/In-class_Ex06/In-class_Ex06.html",
    "title": "In-class Exercise 06: Global/Local Measures of Spatial Autocorrelation",
    "section": "",
    "text": "In Class Notes\n\n\n\n\nInstead of interested in whether the distribution are normally distributed, but rather focus more on whether the data set are randomly distributed.\n\nIf we can reject that the data set is not randomly distributed, then we can infer whether it is clustered or dispersed.\n\nHow to identify outlier?\n\nE.g. if the neighbours are all with high GDPPC, then most likely it will be the outlier. However, we need to perform statistical analysis to confirm it. =&gt; using LMSA\n\nNeed to be able to reject the null hypothesis(it is not randomly distributed), then we can infer whether there is spatial autocorrelation. (Use term such as weak/strong spatial autocorrelation)\n\nPositive Spatial autocorrelation =&gt; clustering and little or no outliers.\nNegative Spatial autocorrelation =&gt; means there will be more outliers.\n\nGMSA:\n\nMoran’I =&gt; Make use of mean(itself and it’s neighbours) to calculate the I value. To check how differ in the study area as a whole.\nGeary C =&gt; compare itself and it’s neighbours. To check the difference from immediate neighbours.\n\nThe lowest value will be 0, but the highest value can be inf.\nWould not be smaller than 0.\n\n\nLMSA:\n\nGetis-Ord Gi/Gi* =&gt; must use distance matrix and all positive value.\n\nUse to identify clusters.\nG* =&gt; consider itself.\nG =&gt; do not consider itself.\nNote: not to identify outliers, but clusters.\n\nLISA =&gt; to identify clusters or outliers.\n\nClusters:\n\nHH =&gt; High value area ,surrounded by high value neighbours\nLL =&gt; Low value area, surrounded by low value neighbours.\n\nOutliers:\n\nHL =&gt; High value area ,surrounded by low value neighbours\nLH =&gt; Low value area, surrounded by high value neighbours.\n\n\n\nTips on choosing color for the plot.\n\nIf there are positive and negative value =&gt; choose the diverging color scheme.\nFor nominal data =&gt; try use distinct color."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex06/In-class_Ex06.html#import-the-data",
    "href": "In-class_Ex/In-class_Ex06/In-class_Ex06.html#import-the-data",
    "title": "In-class Exercise 06: Global/Local Measures of Spatial Autocorrelation",
    "section": "1.1 Import the data",
    "text": "1.1 Import the data\n\nHunan province administrative boundary layer at county level. This is a geospatial data set in ESRI shapefile format.\nHunan_2012.csv: This csv file contains selected Hunan’s local development indicators in 2012.\n\n\nhunan_sf &lt;- st_read(dsn = \"data/geospatial\", \n                 layer = \"Hunan\")\n\nReading layer `Hunan' from data source \n  `/Users/mingwei/Desktop/SMU/Y3S1/IS415/xXxPMWxXx/IS415-GAA/In-class_Ex/In-class_Ex06/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n\n\nUsing read_csv function from tidyverse package.\n\nhunan2012 &lt;- read_csv(\"data/aspatial/Hunan_2012.csv\")\n\nUsing left_join() of dplyr package to update the attribute table of hunan_sf with the attribute fields of hunan2012 dataframe.\n\nhunan_sf_GDPPC &lt;- left_join(hunan_sf,hunan2012) %&gt;%\n  dplyr::select(1:4, 7, 15)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex06/In-class_Ex06.html#deriving-queens-contiguity-weight-sfdep-methods",
    "href": "In-class_Ex/In-class_Ex06/In-class_Ex06.html#deriving-queens-contiguity-weight-sfdep-methods",
    "title": "In-class Exercise 06: Global/Local Measures of Spatial Autocorrelation",
    "section": "2.1 Deriving Queen’s contiguity weight: sfdep methods",
    "text": "2.1 Deriving Queen’s contiguity weight: sfdep methods\n\nwm_q &lt;- hunan_sf_GDPPC %&gt;%\n  mutate(nb = st_contiguity(geometry),\n         wt = st_weights(nb,\n                         style = \"W\"),\n         .before = 1) # the new column will be add to the front."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex06/In-class_Ex06.html#computing-global-morani",
    "href": "In-class_Ex/In-class_Ex06/In-class_Ex06.html#computing-global-morani",
    "title": "In-class Exercise 06: Global/Local Measures of Spatial Autocorrelation",
    "section": "2.2 Computing Global Moran’I",
    "text": "2.2 Computing Global Moran’I\n\nmoranI &lt;- global_moran(wm_q$GDPPC,\n                       wm_q$nb,\n                       wm_q$wt)\n\nglimpse(moranI)\n\nList of 2\n $ I: num 0.301\n $ K: num 7.64\n\n\nk =&gt; the average neighbour in the data."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex06/In-class_Ex06.html#performing-global-morans-i-test",
    "href": "In-class_Ex/In-class_Ex06/In-class_Ex06.html#performing-global-morans-i-test",
    "title": "In-class Exercise 06: Global/Local Measures of Spatial Autocorrelation",
    "section": "2.3 Performing Global Moran’s I Test",
    "text": "2.3 Performing Global Moran’s I Test\n\nglobal_moran_test(wm_q$GDPPC,\n                       wm_q$nb,\n                       wm_q$wt)\n\n\n    Moran I test under randomisation\n\ndata:  x  \nweights: listw    \n\nMoran I statistic standard deviate = 4.7351, p-value = 1.095e-06\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.300749970      -0.011494253       0.004348351 \n\n\n\n\n\n\n\n\nNote\n\n\n\n\np-value = 1.095e-06(&lt; 0.05), indicate that the null hypothesis can be rejected.\nI = 0.3007, indicates weak positive spatial autocorrelation, meaning that areas with similar values of GDPPC are geographically clustered together."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex06/In-class_Ex06.html#performing-global-morani-permutation-test",
    "href": "In-class_Ex/In-class_Ex06/In-class_Ex06.html#performing-global-morani-permutation-test",
    "title": "In-class Exercise 06: Global/Local Measures of Spatial Autocorrelation",
    "section": "2.4 Performing Global Moran’I permutation test",
    "text": "2.4 Performing Global Moran’I permutation test\nIn practice, Monte carlo simulation should be used to perform the statistical test. For sfdep, it is supported by global_moran_perm().\nTo ensure that the computation is reproducible. Make use of set.seed() before perform permutation.\n\nset.seed(1234)\n\n\nglobal_moran_perm(wm_q$GDPPC,\n                       wm_q$nb,\n                       wm_q$wt,\n                  nsim = 99)\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 100 \n\nstatistic = 0.30075, observed rank = 100, p-value &lt; 2.2e-16\nalternative hypothesis: two.sided\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nNotice that the I value is still around 0.3."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex06/In-class_Ex06.html#visualising-local-morani",
    "href": "In-class_Ex/In-class_Ex06/In-class_Ex06.html#visualising-local-morani",
    "title": "In-class Exercise 06: Global/Local Measures of Spatial Autocorrelation",
    "section": "3.1 Visualising local Moran’I",
    "text": "3.1 Visualising local Moran’I\n\ntmap_mode(\"plot\")\ntm_shape(lisa) +\n  tm_fill(\"ii\") +\n  tm_borders(alpha = 0.5) +\n  tm_view(set.zoom.limits = c(6,8)) +\n  tm_layout(\n    main.title = \"local Moran'I of GDPPC\",\n    main.title.size = 2\n  )"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex06/In-class_Ex06.html#visalising-local-morans-i-and-p-value",
    "href": "In-class_Ex/In-class_Ex06/In-class_Ex06.html#visalising-local-morans-i-and-p-value",
    "title": "In-class Exercise 06: Global/Local Measures of Spatial Autocorrelation",
    "section": "3.2 Visalising local Moran’s I and p-value",
    "text": "3.2 Visalising local Moran’s I and p-value\n\ntmap_mode(\"plot\")\nmap1 &lt;- tm_shape(lisa) +\n  tm_fill(\"ii\") +\n  tm_borders(alpha = 0.5) +\n  tm_view(set.zoom.limits = c(6,8)) +\n  tm_layout(\n    main.title = \"local Moran'I of GDPPC\",\n    main.title.size = 1\n  )\n\n\nmap2 &lt;- tm_shape(lisa) + \n  tm_fill(\"p_ii\", breaks = c(0, 0.001, 0.01, 0.05, 1),\n          labels = c(\"0.001\", \"0.01\", \"0.05\", \"Not sig\")) +\n  tm_borders(alpha = 0.5) +\n  tm_layout(\n    main.title = \"p-value of local Moran's I\",\n    main.title.size = 0.8\n  )\n \ntmap_arrange(map1, \n             map2, \n             ncol=2)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex06/In-class_Ex06.html#visualising-lisa-map",
    "href": "In-class_Ex/In-class_Ex06/In-class_Ex06.html#visualising-lisa-map",
    "title": "In-class Exercise 06: Global/Local Measures of Spatial Autocorrelation",
    "section": "3.3 Visualising LISA map",
    "text": "3.3 Visualising LISA map\nIn lisa, there are 3 fields contain the LISA categories. They are mean, median and pysal. In general , classification in mean will be used.\n\nlisa_sig &lt;- lisa %&gt;%\n  filter(p_ii &lt; 0.05)\ntmap_mode(\"plot\")\ntm_shape(lisa) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\ntm_shape(lisa_sig) +\n  tm_fill(\"mean\") +\n  tm_borders(alpha = 0.4)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex06/In-class_Ex06.html#visalising-local-gi-and-p-value",
    "href": "In-class_Ex/In-class_Ex06/In-class_Ex06.html#visalising-local-gi-and-p-value",
    "title": "In-class Exercise 06: Global/Local Measures of Spatial Autocorrelation",
    "section": "4.1 Visalising local Gi* and p-value",
    "text": "4.1 Visalising local Gi* and p-value\n\ntmap_mode(\"plot\")\nmap1 &lt;- tm_shape(HCSA) +\n  tm_fill(\"gi_star\") + \n  tm_borders(alpha = 0.5) +\n  tm_view(set.zoom.limits = c(6,8)) +\n  tm_layout(main.title = \"Gi* of GDPPC\",\n            main.title.size = 0.8)\n\nmap2 &lt;- tm_shape(HCSA) +\n  tm_fill(\"p_value\",\n          breaks = c(0, 0.001, 0.01, 0.05, 1),\n              labels = c(\"0.001\", \"0.01\", \"0.05\", \"Not sig\")) + \n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"p-value of Gi*\",\n            main.title.size = 0.8)\n\ntmap_arrange(map1, map2, ncol = 2)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex06/In-class_Ex06.html#visualising-hot-spot-and-cold-spot-area",
    "href": "In-class_Ex/In-class_Ex06/In-class_Ex06.html#visualising-hot-spot-and-cold-spot-area",
    "title": "In-class Exercise 06: Global/Local Measures of Spatial Autocorrelation",
    "section": "4.1 Visualising hot spot and cold spot area",
    "text": "4.1 Visualising hot spot and cold spot area\n\nHCSA_sig &lt;- HCSA %&gt;%\n  filter(p_sim &lt; 0.05)\ntmap_mode(\"plot\")\ntm_shape(HCSA) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\ntm_shape(HCSA_sig) +\n  tm_fill(\"gi_star\") +\n  tm_borders(alpha = 0.4)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html",
    "title": "Take Home Exercise 02",
    "section": "",
    "text": "Drug abuse is associated with significant negative health, financial and social consequences. Yet, illicit drug consumption remains highly prevalent and continues to be a growing problem worldwide. In 2021, 1 in 17 people aged 15–64 in the world had used a drug in the past 12 months. Notwithstanding population growth, the estimated number of drug users grew from 240 million in 2011 to 296 million in 2021.\nThe geopolitics of Thailand which is near the Golden Triangle of Indochina, the largest drug production site in Asia, and the constant transportation infrastructure development made Thailand became market and transit routes for drug trafficking to the third countries.\nIn Thailand, drug abuse is one of the major social issue. There are about 2.7 million youths using drugs in Thailand. Among youths aged between 15 and 19 years, there are about 300,000 who have needs for drug treatment. Most of Thai youths involved with drugs are vocational-school students, which nearly doubles in number compared to secondary-school students.\n\n\nThis take-home exercise will focus on determining:\n\nif the key indicators of drug abuse of Thailand are independent from space.\nIf the indicators of drug abuse is indeed spatial dependent, then, identify where are the clusters and outliers, and the hotspots.\nLast, investigate how the observation above evolve over time.\n\n\n\n\n\nUse sf and tidyverse to prepare the geospatial data layer:\n\na study area layer of province level(including Bangkok).\na drug abuse indicators layer within the study area.\n\nPerform Global Spatial Autocorrelation Analysis using sfdep.\nPerform Local Spatial Autocorrelation Analysis using sfdep.\nAnalysis the spatial patterns.\n\n\n\n\n\nThailand Drug Offenses [2017-2022] at Kaggle.\nThailand - Subnational Administrative Boundaries at HDX. You are required to use the province boundary data set."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#the-objective",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#the-objective",
    "title": "Take Home Exercise 02",
    "section": "",
    "text": "This take-home exercise will focus on determining:\n\nif the key indicators of drug abuse of Thailand are independent from space.\nIf the indicators of drug abuse is indeed spatial dependent, then, identify where are the clusters and outliers, and the hotspots.\nLast, investigate how the observation above evolve over time."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#the-task",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#the-task",
    "title": "Take Home Exercise 02",
    "section": "",
    "text": "Use sf and tidyverse to prepare the geospatial data layer:\n\na study area layer of province level(including Bangkok).\na drug abuse indicators layer within the study area.\n\nPerform Global Spatial Autocorrelation Analysis using sfdep.\nPerform Local Spatial Autocorrelation Analysis using sfdep.\nAnalysis the spatial patterns."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#the-data",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#the-data",
    "title": "Take Home Exercise 02",
    "section": "",
    "text": "Thailand Drug Offenses [2017-2022] at Kaggle.\nThailand - Subnational Administrative Boundaries at HDX. You are required to use the province boundary data set."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#load-r-packages",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#load-r-packages",
    "title": "Take Home Exercise 02",
    "section": "2.1 Load R Packages",
    "text": "2.1 Load R Packages\n\nsf is use for importing and handling geospatial data in R,\ntidyverse is mainly use for wrangling attribute data in R,\nsfped, builds on the great shoulders of spdep package for spatial dependence. sfdep creates an sf and tidyverse friendly interface\ntmap will be used to prepare cartographic quality choropleth map.\ngridExtra, will be used to arrange grid-based plot, such as ggplot.\nplotly, will be used to plot interactive graphs.\nKendall, computes the Kendall rank correlation and Mann-Kendall trend test.\n\n\npacman::p_load(sf, sfdep, tmap, tidyverse, gridExtra, plotly,Kendall)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#load-the-data",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#load-the-data",
    "title": "Take Home Exercise 02",
    "section": "2.2 Load the Data",
    "text": "2.2 Load the Data\n\n2.2.1 Geospatial Data\nLoading the Thailand_Subnational_Administrative1_Boundaries dataset, which contain the province boundary including Bandkok of Thailand. Using st_read() of sf package.\n\nthailand_sf &lt;- st_read(dsn = \"data/geospatial\", \n                 layer = \"Thailand_Subnational_Administrative1_Boundaries\")\n\nReading layer `Thailand_Subnational_Administrative1_Boundaries' from data source `/Users/mingwei/Desktop/SMU/Y3S1/IS415/xXxPMWxXx/IS415-GAA/Take-home_Ex/Take-home_Ex02/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 77 features and 16 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 97.34336 ymin: 5.613038 xmax: 105.637 ymax: 20.46507\nGeodetic CRS:  WGS 84\n\n\nThere are 77 geographic features, which aligns with expectations. This includes 76 provinces at the first-order administrative level and the capital city, Bangkok.\nNext, lets visualise thailand_sf using tmap package.\n\ntmap_mode(\"plot\")\ntm_shape(thailand_sf)+\n  tm_fill(col=\"white\")+\n  tm_borders(col = \"black\", lwd=0.5, alpha=0.5)+\n  tm_layout(\n    main.title = \"Thailand Provinces (including Bandkok)\",\n    main.title.size = 1.5,\n    main.title.position = \"center\",\n    legend.show = FALSE,\n     frame = FALSE)\n\n\n\n\n\n\n\n\n\n\n2.2.2 Aspatial Data\nLoad Thailand drug offenses[2017 - 2022] dataset, using read_csv() from readr package.\n\ndrug &lt;- read_csv(\"data/aspatial/thai_drug_offenses_2017_2022.csv\")"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#remove-columns",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#remove-columns",
    "title": "Take Home Exercise 02",
    "section": "3.1 Remove Columns",
    "text": "3.1 Remove Columns\nAs there some columns that would not be useful in our analysis. Therefore, it is good practice by removing those column.\nBefore we removing the column. Lets list down the column for the two data set.\n\ncolnames(thailand_sf)\n\n [1] \"Shape_Leng\" \"Shape_Area\" \"ADM1_EN\"    \"ADM1_TH\"    \"ADM1_PCODE\"\n [6] \"ADM1_REF\"   \"ADM1ALT1EN\" \"ADM1ALT2EN\" \"ADM1ALT1TH\" \"ADM1ALT2TH\"\n[11] \"ADM0_EN\"    \"ADM0_TH\"    \"ADM0_PCODE\" \"date\"       \"validOn\"   \n[16] \"validTo\"    \"geometry\"  \n\n\n\ncolnames(drug)\n\n[1] \"fiscal_year\"            \"types_of_drug_offenses\" \"no_cases\"              \n[4] \"province_th\"            \"province_en\"           \n\n\nBased on the columns list on the two dataset and after browsing the data, the following fields I will keep:\n\nthailand_sf\n\nADM1_EN (common field with the drug dataset) =&gt; column 3\ngeometry =&gt; column 17\n\ndrug\n\nfiscal_year =&gt; column 1\ntypes_of_drug_offenses =&gt; column 2\nno_cases =&gt; column 3\nprovince_en =&gt; column 5\n\n\n\nthailand_sf &lt;- thailand_sf %&gt;%\n  select(\"province_en\" = 3,17) # change the ADM1_EN to province_en\n\n\ndrug &lt;- drug %&gt;%\n  select(1:3,5)\n\n\nsum(apply(drug, 1, function(x) any(is.na(x))))\n\n[1] 0"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#checking-the-common-field",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#checking-the-common-field",
    "title": "Take Home Exercise 02",
    "section": "3.2 Checking the common field",
    "text": "3.2 Checking the common field\nSince we need to join the two datasets, it’s important to ensure that all the values in the common column match correctly.\n\ncheck_match &lt;- setdiff(thailand_sf$province_en, drug$province_en)\ncheck_match # print the mismatch field in thailand_sf\n\n[1] \"Lop Buri\"  \"Bueng Kan\"\n\ncheck_match &lt;- setdiff(drug$province_en, thailand_sf$province_en)\ncheck_match # print the mismatch field in drug\n\n[1] \"Loburi\"  \"buogkan\"\n\n\nI observe that two provinces have names in different formats (“Lop Buri” and “Bueng Kan”). Let’s modify the values in the drug dataset to match the naming convention used in the thailand_sf dataset.\n\ndrug &lt;- drug %&gt;%\n  mutate(province_en = case_when(\n    province_en == \"Loburi\" ~ \"Lop Buri\",\n    province_en == \"buogkan\" ~ \"Bueng Kan\",\n    TRUE ~ province_en\n  ))\n\nLets check, if there still have any mismatch.\n\ncheck_match &lt;- setdiff(thailand_sf$province_en, drug$province_en)\ncheck_match\n\ncharacter(0)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#checking-types-of-drug-offenses",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#checking-types-of-drug-offenses",
    "title": "Take Home Exercise 02",
    "section": "3.3 Checking Types of Drug Offenses",
    "text": "3.3 Checking Types of Drug Offenses\nAs in the dataset, there are many different types of drug offenses. In order to get better insight. I decide to further check in the column and see which are the type I should consider.\n\nunique_types &lt;- unique(drug$types_of_drug_offenses)\nprint(unique_types)\n\n [1] \"drug_use_cases\"                                        \n [2] \"suspects_in_drug_use_cases\"                            \n [3] \"possession_cases\"                                      \n [4] \"suspects_in_possession_cases\"                          \n [5] \"possession_with_intent_to_distribute_cases\"            \n [6] \"suspects_in_possession_with_intent_to_distribute_cases\"\n [7] \"trafficking_cases\"                                     \n [8] \"suspects_in_trafficking_cases\"                         \n [9] \"production_cases\"                                      \n[10] \"suspects_in_production_cases\"                          \n[11] \"import_cases\"                                          \n[12] \"suspects_in_import_cases\"                              \n[13] \"export_cases\"                                          \n[14] \"suspects_in_export_cases\"                              \n[15] \"conspiracy_cases\"                                      \n[16] \"suspects_in_conspiracy_cases\"                          \n\n\nTo get a better sense of the field, and decide which type I should focus on. I will group the type using the code chunk.\n\ndrug_counts &lt;- drug %&gt;%\n  group_by(types_of_drug_offenses) %&gt;%\n  summarise(total_cases = sum(no_cases, na.rm = TRUE),, .groups = 'drop')\n\nTo visualise the data.\n\nggplot(drug_counts, aes(x = types_of_drug_offenses, y = total_cases, fill = types_of_drug_offenses)) +\n  geom_bar(stat = \"identity\") +  # Use stat = \"identity\" for summarized data\n  labs(title = \"Total Cases of Drug Offenses by Type\",\n       x = \"Types of Drug Offenses\",\n       y = \"Total Cases\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotate x-axis labels for better visibility\n\n\n\n\n\n\n\n\nBased on the plot, we can observe that drug use cases and suspects in possession cases have been the most common offenses over the years. However, since possession cases may not accurately reflect the severity of drug abuse in Thailand, my analysis will focus on drug use cases. This approach provides more reliable data on the prevalence and severity of drug abuse in each province, as actual drug use offers clearer indicators of abuse trends. By narrowing the scope to drug use cases, I aim to draw more precise conclusions about the drug abuse situation in Thailand from 2017 to 2022.\nThe following code chunk filters the dataset to retain only the drug_use_cases, ensuring that the analysis focuses solely on actual drug use incidents.\n\ndrug_use_cases &lt;- drug %&gt;%\n  filter(types_of_drug_offenses == \"drug_use_cases\")\n\nSince, there is no mismatch in the province_en column, we can combine both data frame using left_join() of dplyr package.\n\ndrug_use_cases &lt;- left_join(thailand_sf, drug_use_cases)\n\n\nwrite_rds(drug_use_cases,\"data/rds/drug_use_cases.rds\")\n\nLets visualising the drug use case over the years.\n\ntmap_mode(\"plot\")\ntm_shape(drug_use_cases) +\n  tm_fill(\"no_cases\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"total_cases\") +\n  tm_layout(main.title = \"Distribution of drug use cases\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_facets(by = \"fiscal_year\", free.coords = FALSE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#global-morans-i-test",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#global-morans-i-test",
    "title": "Take Home Exercise 02",
    "section": "4.1 Global Moran’s I Test",
    "text": "4.1 Global Moran’s I Test\nI decided to use Queen contiguity for the neighbors, as it shares both edges and corners, providing a more comprehensive result in helping me determine whether the drug use cases are independent of their locations.\nI’ve observed that Phuket(67) is an island, which means that st_contiguity cannot identify any polygon neighbors for it. Therefore, I will set allow_zero to be TRUE.\n\n\n\n\n\n\n201720182019202020212022\n\n\n\n\nClick to expand/collapse code\n# Deriving Queen’s contiguity weights\nwm_q_2017 &lt;- drug_use_cases %&gt;%\n  filter(fiscal_year == 2017) %&gt;%\n  mutate(nb = st_contiguity(geometry),\n         wt = st_weights(nb,\n                         style = \"W\", allow_zero = TRUE),\n         .before = 1)\n\n\n\n\nClick to expand/collapse code\nglobal_moran_test(wm_q_2017$no_cases,\n                       wm_q_2017$nb,\n                       wm_q_2017$wt,\n                       zero.policy = TRUE,)\n\n\n\n    Moran I test under randomisation\n\ndata:  x  \nweights: listw  \nn reduced by no-neighbour observations  \n\nMoran I statistic standard deviate = 1.5049, p-value = 0.06618\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.077263424      -0.013333333       0.003624223 \n\n\nThe results show an I value of 0.077, is close to zero, and a p-value of 0.06618, which exceeds the alpha level of 0.05. Therefore, we do not have sufficient statistical evidence to reject the null hypothesis, indicating that the spatial distribution of drug use cases in Thailand was random in 2017.\nTo strengthen the finding, we can run Monte carlo simulation to perform the statistical test.\n\n\nClick to expand/collapse code\nset.seed(1234) # To ensure reproducible\nmoran_mc_2017_res = global_moran_perm(wm_q_2017$no_cases,\n                       wm_q_2017$nb,\n                       wm_q_2017$wt,\n                        nsim=999,\n                       zero.policy = TRUE,)\nmoran_mc_2017_res\n\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.077263, observed rank = 922, p-value = 0.156\nalternative hypothesis: two.sided\n\n\nFrom the simulation results, we can observed that the p-value have increased, which further indicate that we cannot reject the null hypothesis.\nLets check the summary statistics using summary() function.\n\n\nClick to expand/collapse code\nsummary(moran_mc_2017_res$res)\n\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-0.14280 -0.05100 -0.02104 -0.01239  0.02079  0.20105 \n\n\nTo visualise the monte-carlo simulation, we can plot a histogram. The statistic I value for 2017 was 0.07 which fall under the random section of the simulation result. As such, we cannot reject the null hypothesis.\n\n\nClick to expand/collapse code\nggplot() + \n  aes(moran_mc_2017_res$res[1:999]) + \n  geom_histogram(colour=\"black\", fill=\"grey\") +\n  geom_vline(xintercept = 0, color = \"red\", linetype = \"solid\", size = 1) +\n  labs(title = \"Histogram of Simulated Moran's I For Thailand Drug Use Cases 2017\",\n       x = \"Simulated Moran's I\",\n       y = \"Frequency\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\nClick to expand/collapse code\n# Deriving Queen’s contiguity weights\nwm_q_2018 &lt;- drug_use_cases %&gt;%\n  filter(fiscal_year == 2018) %&gt;%\n  mutate(nb = st_contiguity(geometry),\n         wt = st_weights(nb,\n                         style = \"W\", allow_zero = TRUE),\n         .before = 1)\n\n\n\n\nClick to expand/collapse code\nglobal_moran_test(wm_q_2018$no_cases,\n                       wm_q_2018$nb,\n                       wm_q_2018$wt,\n                       zero.policy = TRUE,)\n\n\n\n    Moran I test under randomisation\n\ndata:  x  \nweights: listw  \nn reduced by no-neighbour observations  \n\nMoran I statistic standard deviate = 1.673, p-value = 0.04716\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.091283835      -0.013333333       0.003910294 \n\n\nFrom the above result with I = 0.09 and the p-value = 0.4716 is smaller than alpha value of 0.05. Hence, we have statistical evidence to reject the null hypothesis and since the I value is positive but close to 0, so we can infer that the spatial distribution shows a weak sign of clustering in 2018.\n\n\nClick to expand/collapse code\nset.seed(1234) # To ensure reproducible\nmoran_mc_2018_res = global_moran_perm(wm_q_2018$no_cases,\n                       wm_q_2018$nb,\n                       wm_q_2018$wt,\n                        nsim=999,\n                       zero.policy = TRUE,)\nmoran_mc_2018_res\n\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.091284, observed rank = 944, p-value = 0.112\nalternative hypothesis: two.sided\n\n\nHowever, based on the simulation, we can observed that the p-value has become larger than the alpha value of 0.05. Hence, based on the simulation result, we do not have statistical evidence to reject the null hypothesis that the spatial distribution of drug use case in Thailand are resemble random distribution for Year 2018.\nTo check the summary statistics.\n\n\nClick to expand/collapse code\nsummary(moran_mc_2018_res$res)\n\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-0.14516 -0.05257 -0.01834 -0.01137  0.02357  0.26485 \n\n\nBased on the histogram, the result similar to Year 2017, the statistic I value still fall under the random section(0.09). As such, we cannot reject the null hypothesis.\n\n\nClick to expand/collapse code\nggplot() + \n  aes(moran_mc_2018_res$res[1:999]) + \n  geom_histogram(colour=\"black\", fill=\"grey\") +\n  geom_vline(xintercept = 0, color = \"red\", linetype = \"solid\", size = 1) +\n  labs(title = \"Histogram of Simulated Moran's I For Thailand Drug Use Cases 2018\",\n       x = \"Simulated Moran's I\",\n       y = \"Frequency\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\nClick to expand/collapse code\n# Deriving Queen’s contiguity weights\nwm_q_2019 &lt;- drug_use_cases %&gt;%\n  filter(fiscal_year == 2019) %&gt;%\n  mutate(nb = st_contiguity(geometry),\n         wt = st_weights(nb,\n                         style = \"W\", allow_zero = TRUE),\n         .before = 1)\n\n\n\n\nClick to expand/collapse code\nglobal_moran_test(wm_q_2019$no_cases,\n                       wm_q_2019$nb,\n                       wm_q_2019$wt,\n                       zero.policy = TRUE,)\n\n\n\n    Moran I test under randomisation\n\ndata:  x  \nweights: listw  \nn reduced by no-neighbour observations  \n\nMoran I statistic standard deviate = 2.1385, p-value = 0.01624\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.138046280      -0.013333333       0.005010889 \n\n\n\n\nClick to expand/collapse code\nset.seed(1234) # To ensure reproducible\nmoran_mc_2019_res = global_moran_perm(wm_q_2019$no_cases,\n                       wm_q_2019$nb,\n                       wm_q_2019$wt,\n                        nsim=999,\n                       zero.policy = TRUE,)\nmoran_mc_2019_res\n\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.13805, observed rank = 972, p-value = 0.056\nalternative hypothesis: two.sided\n\n\nBased on the Moran I test and simulation result, we can observed that the p-value have become larger than 0.05 in the statistical test. Hence, we do not have enough statistical evidence to reject the null hypothesis that the spatial distribution of drug use case in Thailand are resemble random distribution for Year 2019. However, notice that the p-value is actually very close to the alpha value of 0.05, so based on this, we can infer that even we do not have enough statistical evidence to reject the null hypothesis, but it also give us insight that the drug use case in 2019, is probably higher than Year 2017 and 2018.\nCheck the summary statistics.\n\n\nClick to expand/collapse code\nsummary(moran_mc_2019_res$res)\n\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-0.15737 -0.05923 -0.01686 -0.01095  0.02767  0.31845 \n\n\nBased on the histogram, we can observed that the statistic I value of 0.13 fall under the random section but very close to the alpha-value section.\n\n\nClick to expand/collapse code\nggplot() + \n  aes(moran_mc_2019_res$res[1:999]) + \n  geom_histogram(colour=\"black\", fill=\"grey\") +\n  geom_vline(xintercept = 0, color = \"red\", linetype = \"solid\", size = 1) +\n  labs(title = \"Histogram of Simulated Moran's I For Thailand Drug Use Cases 2019\",\n       x = \"Simulated Moran's I\",\n       y = \"Frequency\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\nClick to expand/collapse code\n# Deriving Queen’s contiguity weights\nwm_q_2020 &lt;- drug_use_cases %&gt;%\n  filter(fiscal_year == 2020) %&gt;%\n  mutate(nb = st_contiguity(geometry),\n         wt = st_weights(nb,\n                         style = \"W\", allow_zero = TRUE),\n         .before = 1)\n\n\n\n\nClick to expand/collapse code\nglobal_moran_test(wm_q_2020$no_cases,\n                       wm_q_2020$nb,\n                       wm_q_2020$wt,\n                       zero.policy = TRUE,)\n\n\n\n    Moran I test under randomisation\n\ndata:  x  \nweights: listw  \nn reduced by no-neighbour observations  \n\nMoran I statistic standard deviate = 1.382, p-value = 0.08349\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.087091664      -0.013333333       0.005280586 \n\n\n\n\nClick to expand/collapse code\nset.seed(1234) # To ensure reproducible\nmoran_mc_2020_res = global_moran_perm(wm_q_2020$no_cases,\n                       wm_q_2020$nb,\n                       wm_q_2020$wt,\n                        nsim=999,\n                       zero.policy = TRUE,)\nmoran_mc_2020_res\n\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.087092, observed rank = 924, p-value = 0.152\nalternative hypothesis: two.sided\n\n\nBased on the Moran I test and simulation result, we can observed that the p-value have become larger than 0.05 in the statistical test. Hence, we do not have enough statistical evidence to reject the null hypothesis that the spatial distribution of drug use case in Thailand are resemble random distribution for Year 2020.\nThe summary of the simulation statistics.\n\n\nClick to expand/collapse code\nsummary(moran_mc_2020_res$res)\n\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-0.15929 -0.06243 -0.02218 -0.01433  0.02699  0.33025 \n\n\nBased on the histogram of the simulation, shows that the statistic I value of 0.08, fall in the random section.\n\n\nClick to expand/collapse code\nggplot() + \n  aes(moran_mc_2020_res$res[1:999]) + \n  geom_histogram(colour=\"black\", fill=\"grey\") +\n  geom_vline(xintercept = 0, color = \"red\", linetype = \"solid\", size = 1) +\n  labs(title = \"Histogram of Simulated Moran's I For Thailand Drug Use Cases 2020\",\n       x = \"Simulated Moran's I\",\n       y = \"Frequency\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\nClick to expand/collapse code\n# Deriving Queen’s contiguity weights\nwm_q_2021 &lt;- drug_use_cases %&gt;%\n  filter(fiscal_year == 2021) %&gt;%\n  mutate(nb = st_contiguity(geometry),\n         wt = st_weights(nb,\n                         style = \"W\", allow_zero = TRUE),\n         .before = 1)\n\n\n\n\nClick to expand/collapse code\nglobal_moran_test(wm_q_2021$no_cases,\n                       wm_q_2021$nb,\n                       wm_q_2021$wt,\n                       zero.policy = TRUE,)\n\n\n\n    Moran I test under randomisation\n\ndata:  x  \nweights: listw  \nn reduced by no-neighbour observations  \n\nMoran I statistic standard deviate = 2.862, p-value = 0.002105\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n       0.20376109       -0.01333333        0.00575372 \n\n\n\n\nClick to expand/collapse code\nset.seed(1234) # To ensure reproducible\nmoran_mc_2021_res = global_moran_perm(wm_q_2021$no_cases,\n                       wm_q_2021$nb,\n                       wm_q_2021$wt,\n                        nsim=999,\n                       zero.policy = TRUE,)\nmoran_mc_2021_res\n\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.20376, observed rank = 993, p-value = 0.014\nalternative hypothesis: two.sided\n\n\nBased on the Moran I test and simulation result, we can observed that the p-value is still smaller than 0.05 in the statistical test. Hence, we have enough statistical evidence to reject the null hypothesis that the spatial distribution of drug use case in Thailand are not resemble random distribution for Year 2021. Since I is positive value of 0.2, hence we can infer that the spatial distribution of drag use in Thailand in 2021 shows sign of weak clustering.\nTo check the simulation statisitcs.\n\n\nClick to expand/collapse code\nsummary(moran_mc_2021_res$res)\n\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-0.23519 -0.06340 -0.01754 -0.01333  0.03027  0.29476 \n\n\nBased on the histogram, our statistic I value of 0.20376 falls under the alpha-value range. Hence, we can reject the null hypothesis and can infer there is a weak sign of spatial autocorrelation in the drug use case across different locations.\n\n\nClick to expand/collapse code\nggplot() + \n  aes(moran_mc_2021_res$res[1:999]) + \n  geom_histogram(colour=\"black\", fill=\"grey\") +\n  geom_vline(xintercept = 0, color = \"red\", linetype = \"solid\", size = 1) +\n  labs(title = \"Histogram of Simulated Moran's I For Thailand Drug Use Cases 2021\",\n       x = \"Simulated Moran's I\",\n       y = \"Frequency\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\nClick to expand/collapse code\n# Deriving Queen’s contiguity weights\nwm_q_2022 &lt;- drug_use_cases %&gt;%\n  filter(fiscal_year == 2022) %&gt;%\n  mutate(nb = st_contiguity(geometry),\n         wt = st_weights(nb,\n                         style = \"W\", allow_zero = TRUE),\n         .before = 1)\n\n\n\n\nClick to expand/collapse code\nglobal_moran_test(wm_q_2022$no_cases,\n                       wm_q_2022$nb,\n                       wm_q_2022$wt,\n                       zero.policy = TRUE,)\n\n\n\n    Moran I test under randomisation\n\ndata:  x  \nweights: listw  \nn reduced by no-neighbour observations  \n\nMoran I statistic standard deviate = 2.7688, p-value = 0.002813\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.197931897      -0.013333333       0.005822019 \n\n\n\n\nClick to expand/collapse code\nset.seed(1234) # To ensure reproducible\nmoran_mc_2022_res = global_moran_perm(wm_q_2022$no_cases,\n                       wm_q_2022$nb,\n                       wm_q_2022$wt,\n                        nsim=999,\n                       zero.policy = TRUE,)\nmoran_mc_2022_res\n\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.19793, observed rank = 994, p-value = 0.012\nalternative hypothesis: two.sided\n\n\nSimilar to Year 2021, we can observed that the p-value is still smaller than 0.05 in the statistical test. Hence, we have enough statistical evidence to reject the null hypothesis that the spatial distribution of drug use case in Thailand are not resemble random distribution for Year 2022 and shows sign of weak clustering.\nLets check the summary statistics.\n\n\nClick to expand/collapse code\nsummary(moran_mc_2022_res$res[1:999])\n\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-0.21825 -0.05910 -0.01462 -0.01072  0.03411  0.30446 \n\n\nTo visualise the monte-carlo simulation, we can plot a histogram. The statistic I value for 2022 was 0.19 which fall under the alpha-value of the simulation result. As such, we can infer there is a weak sign of spatial autocorrelation in the drug use case across different locations.\n\n\nClick to expand/collapse code\nggplot() + \n  aes(moran_mc_2022_res$res[1:999]) + \n  geom_histogram(colour=\"black\", fill=\"grey\") +\n  geom_vline(xintercept = 0, color = \"red\", linetype = \"solid\", size = 1) +\n  labs(title = \"Histogram of Simulated Moran's I For Thailand Drug Use Cases 2022\",\n       x = \"Simulated Moran's I\",\n       y = \"Frequency\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\nwrite_rds(wm_q_2017,\"data/rds/wm_q_2017.rds\")\nwrite_rds(wm_q_2018,\"data/rds/wm_q_2018.rds\")\nwrite_rds(wm_q_2019,\"data/rds/wm_q_2019.rds\")\nwrite_rds(wm_q_2020,\"data/rds/wm_q_2020.rds\")\nwrite_rds(wm_q_2021,\"data/rds/wm_q_2021.rds\")\nwrite_rds(wm_q_2022,\"data/rds/wm_q_2022.rds\")"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#global-morans-i-test-overall-insight",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#global-morans-i-test-overall-insight",
    "title": "Take Home Exercise 02",
    "section": "4.2 Global Moran’s I Test Overall Insight",
    "text": "4.2 Global Moran’s I Test Overall Insight\n\n\n\n\n\n\nOverall Observation\n\n\n\n\nOverall, we can observed that from the Year 2017 to 2019, the trend are increasing in term of I value and decreasing in p-value. But the p-value over the three years are still larger than the alpha value of 0.05. Hence, we do not have enough statistical evidence to reject the null hypothesis that the spatial distribution of drug use case in Thailand are resemble random distribution.\nIn the Year 2020, the trend broke, with the I value decreasing instead of continuing to increase, and the p-value increasing instead of decreasing. This suggests a stronger indication of randomness in the distribution. I believe this change is due to COVID-19, which resulted in a decrease in recorded drug use cases.\nIn the Year 2021 - 2022, there are enough statistical evidence to reject the null hypothesis and we can infer that the spatial distribution of drug use cases shows sign of weak clustering.\nBased on the histograms of the simulation results, we can observe that the distribution of the simulated Moran’s I values is predominantly skewed to the negative side, indicating that the data points exhibit a dispersed spatial pattern."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#global-gearys-c",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#global-gearys-c",
    "title": "Take Home Exercise 02",
    "section": "4.3 Global Geary’s C",
    "text": "4.3 Global Geary’s C\nIn order to gain different insight into the spatial pattern of the drug use cases in Thailand. I will perform Global Geary’s C as it focus more on local differences and more sensitive to local variability than Moran’s I. Hence, it will be able to help to detect local variability and help in understand how strongly individual provinces deviate from the overall pattern in neighboring regions.\n\n201720182019202020212022\n\n\nTo performs Geary’s C test for spatial autocorrelation using global_c_test() from sfdep package.\n\n\nClick to expand/collapse code\nglobal_c_test(wm_q_2017$no_cases,\n              wm_q_2017$nb,\n              wm_q_2017$wt,\n              randomization = TRUE,\n              allow_zero=TRUE)\n\n\n\n    Geary C test under randomisation\n\ndata:  x \nweights: listw  \nn reduced by no-neighbour observations \n\nGeary C statistic standard deviate = -0.18872, p-value = 0.5748\nalternative hypothesis: Expectation greater than statistic\nsample estimates:\nGeary C statistic       Expectation          Variance \n        1.0256614         1.0000000         0.0184905 \n\n\nTo performs permutation test for Geary’s C statistic by using global_c_perm() from sfdep packages.\n\n\nClick to expand/collapse code\nset.seed(1234)\ngeary_mc_2017_res = global_c_perm(wm_q_2017$no_cases,\n              wm_q_2017$nb,\n              wm_q_2017$wt,\n              nsim = 999,\n              allow_zero = TRUE,)\ngeary_mc_2017_res\n\n\n\n    Monte-Carlo simulation of Geary C\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 1000 \n\nstatistic = 1.0257, observed rank = 629, p-value = 0.629\nalternative hypothesis: greater\n\n\nBased on the above actual and simulation statistic, with C = 1.0257, the drug use cases exhibits a slight tendency toward spatial dispersion, but the result is very close to 1, indicating near-randomness. The large simulated p-value(0.629) suggests that the spatial distribution is likely random, and there is no significant evidence of spatial autocorrelation in 2017.\nTo visvualising the Monte Carlo Geary’s C.\n\n\nClick to expand/collapse code\nsummary(geary_mc_2017_res$res)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.3162  0.8991  0.9790  0.9845  1.0755  1.4784 \n\n\n\n\nClick to expand/collapse code\nggplot() + \n  aes(geary_mc_2017_res$res[1:999]) + \n  geom_histogram(colour=\"black\", fill=\"grey\") +\n  geom_vline(xintercept = 1, color = \"red\", linetype = \"solid\", size = 1) +\n  labs(title = \"Histogram of Simulated  Geary C Values For Thailand Drug Use Cases 2017\",\n       x = \"Simulated  Geary C\",\n       y = \"Frequency\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nTo performs Geary’s C test for spatial autocorrelation using global_c_test() from sfdep package.\n\n\nClick to expand/collapse code\nglobal_c_test(wm_q_2018$no_cases,\n              wm_q_2018$nb,\n              wm_q_2018$wt,\n              randomization = TRUE,\n              allow_zero=TRUE)\n\n\n\n    Geary C test under randomisation\n\ndata:  x \nweights: listw  \nn reduced by no-neighbour observations \n\nGeary C statistic standard deviate = -0.026269, p-value = 0.5105\nalternative hypothesis: Expectation greater than statistic\nsample estimates:\nGeary C statistic       Expectation          Variance \n       1.00342511        1.00000000        0.01700099 \n\n\nTo performs permutation test for Geary’s C statistic by using global_c_perm() from sfdep packages.\n\n\nClick to expand/collapse code\nset.seed(1234)\ngeary_mc_2018_res = global_c_perm(wm_q_2018$no_cases,\n              wm_q_2018$nb,\n              wm_q_2018$wt,\n              nsim = 999,\n              allow_zero = TRUE,)\ngeary_mc_2018_res\n\n\n\n    Monte-Carlo simulation of Geary C\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 1000 \n\nstatistic = 1.0034, observed rank = 582, p-value = 0.582\nalternative hypothesis: greater\n\n\nBased on the above actual and simulation statistic, with C = 1.0034 and simulated p-value are very similar to Year 2017. Hence, we ca infer there is no significant evidence of spatial autocorrelation in 2018 as well.\nTo visvualising the Monte Carlo Geary’s C.\n\n\nClick to expand/collapse code\nsummary(geary_mc_2018_res$res)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.3658  0.9025  0.9801  0.9838  1.0716  1.4479 \n\n\n\n\nClick to expand/collapse code\nggplot() + \n  aes(geary_mc_2018_res$res[1:999]) + \n  geom_histogram(colour=\"black\", fill=\"grey\") +\n  geom_vline(xintercept = 1, color = \"red\", linetype = \"solid\", size = 1) +\n  labs(title = \"Histogram of Simulated  Geary C Values For Thailand Drug Use Cases 2018\",\n       x = \"Simulated  Geary C\",\n       y = \"Frequency\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nTo performs Geary’s C test for spatial autocorrelation using global_c_test() from sfdep package.\n\n\nClick to expand/collapse code\nglobal_c_test(wm_q_2019$no_cases,\n              wm_q_2019$nb,\n              wm_q_2019$wt,\n              randomization = TRUE,\n              allow_zero=TRUE)\n\n\n\n    Geary C test under randomisation\n\ndata:  x \nweights: listw  \nn reduced by no-neighbour observations \n\nGeary C statistic standard deviate = 0.78768, p-value = 0.2154\nalternative hypothesis: Expectation greater than statistic\nsample estimates:\nGeary C statistic       Expectation          Variance \n       0.91637784        1.00000000        0.01127045 \n\n\n\n\nClick to expand/collapse code\nset.seed(1234)\ngeary_mc_2019_res = global_c_perm(wm_q_2019$no_cases,\n              wm_q_2019$nb,\n              wm_q_2019$wt,\n              nsim = 999,\n              allow_zero = TRUE,)\ngeary_mc_2019_res\n\n\n\n    Monte-Carlo simulation of Geary C\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.91638, observed rank = 244, p-value = 0.244\nalternative hypothesis: greater\n\n\nBased on the above actual and simulation statistic, with C = 0.91638, the drug use cases exhibits a slight tendency toward spatial clustered, but the result is close to 1, indicating near-randomness. The simulated p-value(0.224) which is larger than the alpha-value 0.05, suggests that the spatial distribution is likely random, and there is not enough significant evidence of spatial autocorrelation in 2019.\nTo visvualising the Monte Carlo Geary’s C.\n\n\nClick to expand/collapse code\nsummary(geary_mc_2019_res$res)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.5495  0.9180  0.9862  0.9847  1.0591  1.3493 \n\n\n\n\nClick to expand/collapse code\nggplot() + \n  aes(geary_mc_2019_res$res[1:999]) + \n  geom_histogram(colour=\"black\", fill=\"grey\") +\n  geom_vline(xintercept = 1, color = \"red\", linetype = \"solid\", size = 1) +\n  labs(title = \"Histogram of Simulated  Geary C Values For Thailand Drug Use Cases 2019\",\n       x = \"Simulated  Geary C\",\n       y = \"Frequency\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nTo performs Geary’s C test for spatial autocorrelation using global_c_test() from sfdep package.\n\n\nClick to expand/collapse code\nglobal_c_test(wm_q_2020$no_cases,\n              wm_q_2020$nb,\n              wm_q_2020$wt,\n              randomization = TRUE,\n              allow_zero=TRUE)\n\n\n\n    Geary C test under randomisation\n\ndata:  x \nweights: listw  \nn reduced by no-neighbour observations \n\nGeary C statistic standard deviate = 0.47955, p-value = 0.3158\nalternative hypothesis: Expectation greater than statistic\nsample estimates:\nGeary C statistic       Expectation          Variance \n      0.952367243       1.000000000       0.009866203 \n\n\nTo performs permutation test for Geary’s C statistic by using global_c_perm() from sfdep packages.\n\n\nClick to expand/collapse code\nset.seed(1234)\ngeary_mc_2020_res = global_c_perm(wm_q_2020$no_cases,\n              wm_q_2020$nb,\n              wm_q_2020$wt,\n              nsim = 999,\n              allow_zero = TRUE,)\ngeary_mc_2020_res\n\n\n\n    Monte-Carlo simulation of Geary C\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.95237, observed rank = 324, p-value = 0.324\nalternative hypothesis: greater\n\n\nBased on the above actual and simulation statistic, with C = 0.95237 and the simulated p-value(0.324) similar to previous year (2020) suggests the drug use cases exhibits a slight tendency toward spatial clustered and the spatial distribution is likely random, and there is not enough significant evidence of spatial autocorrelation in 2020.\nTo visvualising the Monte Carlo Geary’s C.\n\n\nClick to expand/collapse code\nsummary(geary_mc_2020_res$res)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.5720  0.9290  0.9944  0.9896  1.0558  1.3507 \n\n\n\n\nClick to expand/collapse code\nggplot() + \n  aes(geary_mc_2020_res$res[1:999]) + \n  geom_histogram(colour=\"black\", fill=\"grey\") +\n  geom_vline(xintercept = 1, color = \"red\", linetype = \"solid\", size = 1) +\n  labs(title = \"Histogram of Simulated  Geary C Values For Thailand Drug Use Cases 2020\",\n       x = \"Simulated  Geary C\",\n       y = \"Frequency\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nTo performs Geary’s C test for spatial autocorrelation using global_c_test() from sfdep package.\n\n\nClick to expand/collapse code\nglobal_c_test(wm_q_2021$no_cases,\n              wm_q_2021$nb,\n              wm_q_2021$wt,\n              randomization = TRUE,\n              allow_zero=TRUE)\n\n\n\n    Geary C test under randomisation\n\ndata:  x \nweights: listw  \nn reduced by no-neighbour observations \n\nGeary C statistic standard deviate = 1.8151, p-value = 0.03476\nalternative hypothesis: Expectation greater than statistic\nsample estimates:\nGeary C statistic       Expectation          Variance \n      0.843832653       1.000000000       0.007402704 \n\n\nTo performs permutation test for Geary’s C statistic by using global_c_perm() from sfdep packages.\n\n\nClick to expand/collapse code\nset.seed(1234)\ngeary_mc_2021_res = global_c_perm(wm_q_2021$no_cases,\n              wm_q_2021$nb,\n              wm_q_2021$wt,\n              nsim = 999,\n              allow_zero = TRUE,)\ngeary_mc_2021_res\n\n\n\n    Monte-Carlo simulation of Geary C\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.84383, observed rank = 61, p-value = 0.061\nalternative hypothesis: greater\n\n\nBased on the above actual and simulation statistic, with C = 0.84383 and actual p-value of 0.03476, the drug use cases exhibits a slight tendency toward spatial clustered and we can reject the null hypothesis that the autocorrelation is unlikely to have occurred by random. However,the simulated p-value(0.061) which is larger than the alpha-value 0.05.\nThe difference between the actual p-value (0.034) and the simulated p-value (0.061) indicates that while the traditional test suggests significant spatial clustering, the Monte Carlo simulation shows weaker evidence for rejecting the null hypothesis of spatial randomness.\nTo visvualising the Monte Carlo Geary’s C.\n\n\nClick to expand/collapse code\nsummary(geary_mc_2021_res$res)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.6844  0.9350  0.9918  0.9886  1.0443  1.2153 \n\n\n\n\nClick to expand/collapse code\nggplot() + \n  aes(geary_mc_2021_res$res[1:999]) + \n  geom_histogram(colour=\"black\", fill=\"grey\") +\n  geom_vline(xintercept = 1, color = \"red\", linetype = \"solid\", size = 1) +\n  labs(title = \"Histogram of Simulated  Geary C Values For Thailand Drug Use Cases 2021\",\n       x = \"Simulated  Geary C\",\n       y = \"Frequency\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nTo performs Geary’s C test for spatial autocorrelation using global_c_test() from sfdep package.\n\n\nClick to expand/collapse code\nglobal_c_test(wm_q_2022$no_cases,\n              wm_q_2022$nb,\n              wm_q_2022$wt,\n              randomization = TRUE,\n              allow_zero=TRUE)\n\n\n\n    Geary C test under randomisation\n\ndata:  x \nweights: listw  \nn reduced by no-neighbour observations \n\nGeary C statistic standard deviate = 1.9389, p-value = 0.02626\nalternative hypothesis: Expectation greater than statistic\nsample estimates:\nGeary C statistic       Expectation          Variance \n      0.837233817       1.000000000       0.007047089 \n\n\nTo performs permutation test for Geary’s C statistic by using global_c_perm() from sfdep packages.\n\n\nClick to expand/collapse code\nset.seed(1234)\ngeary_mc_2022_res = global_c_perm(wm_q_2022$no_cases,\n              wm_q_2022$nb,\n              wm_q_2022$wt,\n              nsim = 999,\n              allow_zero = TRUE,)\ngeary_mc_2022_res\n\n\n\n    Monte-Carlo simulation of Geary C\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.83723, observed rank = 46, p-value = 0.046\nalternative hypothesis: greater\n\n\nBased on the above actual and simulation statistic, unlike previous year(2021), with C = 0.83723 suggesting the drug use cases exhibits a slight tendency toward spatial clustered. With actual p-value of 0.2626 and simulated p-value of 0.046, both is smaller than the alpha-value 0.05, hence, we can reject the null hypothesis that the autocorrelation is unlikely to have occurred by random and infer there is spatial autocorrelation of clustering in 2022.\nTo visvualising the Monte Carlo Geary’s C.\n\n\nClick to expand/collapse code\nsummary(geary_mc_2022_res$res)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.6673  0.9333  0.9851  0.9845  1.0400  1.2253 \n\n\n\n\nClick to expand/collapse code\nggplot() + \n  aes(geary_mc_2022_res$res[1:999]) + \n  geom_histogram(colour=\"black\", fill=\"grey\") +\n  geom_vline(xintercept = 1, color = \"red\", linetype = \"solid\", size = 1) +\n  labs(title = \"Histogram of Simulated  Geary C Values For Thailand Drug Use Cases 2022\",\n       x = \"Simulated  Geary C\",\n       y = \"Frequency\") +\n  theme_minimal()"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#global-gearys-overall-insight",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#global-gearys-overall-insight",
    "title": "Take Home Exercise 02",
    "section": "4.4 Global Geary’s Overall Insight",
    "text": "4.4 Global Geary’s Overall Insight\n\n\n\n\n\n\nOverall Observation\n\n\n\n\nOverall, the trend of what we have observed from Global Geary’s test"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#global-gearys-c-test",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#global-gearys-c-test",
    "title": "Take Home Exercise 02",
    "section": "4.3 Global Geary’s C Test",
    "text": "4.3 Global Geary’s C Test\nIn order to gain different insight into the spatial pattern of the drug use cases in Thailand. I will perform Global Geary’s C as it focus more on local differences and more sensitive to local variability than Moran’s I. Hence, it will be able to help to detect local variability and help in understand how strongly individual provinces deviate from the overall pattern in neighboring regions.\n\n201720182019202020212022\n\n\nTo performs Geary’s C test for spatial autocorrelation using global_c_test() from sfdep package.\n\n\nClick to expand/collapse code\nglobal_c_test(wm_q_2017$no_cases,\n              wm_q_2017$nb,\n              wm_q_2017$wt,\n              randomization = TRUE,\n              allow_zero=TRUE)\n\n\n\n    Geary C test under randomisation\n\ndata:  x \nweights: listw  \nn reduced by no-neighbour observations \n\nGeary C statistic standard deviate = -0.18872, p-value = 0.5748\nalternative hypothesis: Expectation greater than statistic\nsample estimates:\nGeary C statistic       Expectation          Variance \n        1.0256614         1.0000000         0.0184905 \n\n\nTo performs permutation test for Geary’s C statistic by using global_c_perm() from sfdep packages.\n\n\nClick to expand/collapse code\nset.seed(1234)\ngeary_mc_2017_res = global_c_perm(wm_q_2017$no_cases,\n              wm_q_2017$nb,\n              wm_q_2017$wt,\n              nsim = 999,\n              allow_zero = TRUE,)\ngeary_mc_2017_res\n\n\n\n    Monte-Carlo simulation of Geary C\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 1000 \n\nstatistic = 1.0257, observed rank = 629, p-value = 0.629\nalternative hypothesis: greater\n\n\nBased on the above actual and simulation statistic, with C = 1.0257, the drug use cases exhibits a slight tendency toward spatial dispersion, but the result is very close to 1, indicating near-randomness. The large simulated p-value(0.629) suggests that the spatial distribution is likely random, and there is no significant evidence of spatial autocorrelation in 2017.\nTo visvualising the Monte Carlo Geary’s C.\n\n\nClick to expand/collapse code\nsummary(geary_mc_2017_res$res)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.3162  0.8991  0.9790  0.9845  1.0755  1.4784 \n\n\n\n\nClick to expand/collapse code\nggplot() + \n  aes(geary_mc_2017_res$res[1:999]) + \n  geom_histogram(colour=\"black\", fill=\"grey\") +\n  geom_vline(xintercept = 1, color = \"red\", linetype = \"solid\", size = 1) +\n  labs(title = \"Histogram of Simulated  Geary C Values For Thailand Drug Use Cases 2017\",\n       x = \"Simulated  Geary C\",\n       y = \"Frequency\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nTo performs Geary’s C test for spatial autocorrelation using global_c_test() from sfdep package.\n\n\nClick to expand/collapse code\nglobal_c_test(wm_q_2018$no_cases,\n              wm_q_2018$nb,\n              wm_q_2018$wt,\n              randomization = TRUE,\n              allow_zero=TRUE)\n\n\n\n    Geary C test under randomisation\n\ndata:  x \nweights: listw  \nn reduced by no-neighbour observations \n\nGeary C statistic standard deviate = -0.026269, p-value = 0.5105\nalternative hypothesis: Expectation greater than statistic\nsample estimates:\nGeary C statistic       Expectation          Variance \n       1.00342511        1.00000000        0.01700099 \n\n\nTo performs permutation test for Geary’s C statistic by using global_c_perm() from sfdep packages.\n\n\nClick to expand/collapse code\nset.seed(1234)\ngeary_mc_2018_res = global_c_perm(wm_q_2018$no_cases,\n              wm_q_2018$nb,\n              wm_q_2018$wt,\n              nsim = 999,\n              allow_zero = TRUE,)\ngeary_mc_2018_res\n\n\n\n    Monte-Carlo simulation of Geary C\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 1000 \n\nstatistic = 1.0034, observed rank = 582, p-value = 0.582\nalternative hypothesis: greater\n\n\nBased on the above actual and simulation statistic, with C = 1.0034 and simulated p-value are very similar to Year 2017. Hence, we ca infer there is no significant evidence of spatial autocorrelation in 2018 as well.\nTo visvualising the Monte Carlo Geary’s C.\n\n\nClick to expand/collapse code\nsummary(geary_mc_2018_res$res)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.3658  0.9025  0.9801  0.9838  1.0716  1.4479 \n\n\n\n\nClick to expand/collapse code\nggplot() + \n  aes(geary_mc_2018_res$res[1:999]) + \n  geom_histogram(colour=\"black\", fill=\"grey\") +\n  geom_vline(xintercept = 1, color = \"red\", linetype = \"solid\", size = 1) +\n  labs(title = \"Histogram of Simulated  Geary C Values For Thailand Drug Use Cases 2018\",\n       x = \"Simulated  Geary C\",\n       y = \"Frequency\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nTo performs Geary’s C test for spatial autocorrelation using global_c_test() from sfdep package.\n\n\nClick to expand/collapse code\nglobal_c_test(wm_q_2019$no_cases,\n              wm_q_2019$nb,\n              wm_q_2019$wt,\n              randomization = TRUE,\n              allow_zero=TRUE)\n\n\n\n    Geary C test under randomisation\n\ndata:  x \nweights: listw  \nn reduced by no-neighbour observations \n\nGeary C statistic standard deviate = 0.78768, p-value = 0.2154\nalternative hypothesis: Expectation greater than statistic\nsample estimates:\nGeary C statistic       Expectation          Variance \n       0.91637784        1.00000000        0.01127045 \n\n\n\n\nClick to expand/collapse code\nset.seed(1234)\ngeary_mc_2019_res = global_c_perm(wm_q_2019$no_cases,\n              wm_q_2019$nb,\n              wm_q_2019$wt,\n              nsim = 999,\n              allow_zero = TRUE,)\ngeary_mc_2019_res\n\n\n\n    Monte-Carlo simulation of Geary C\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.91638, observed rank = 244, p-value = 0.244\nalternative hypothesis: greater\n\n\nBased on the above actual and simulation statistic, with C = 0.91638, the drug use cases exhibits a slight tendency toward spatial clustered, but the result is close to 1, indicating near-randomness. The simulated p-value(0.224) which is larger than the alpha-value 0.05, suggests that the spatial distribution is likely random, and there is not enough significant evidence of spatial autocorrelation in 2019.\nTo visvualising the Monte Carlo Geary’s C.\n\n\nClick to expand/collapse code\nsummary(geary_mc_2019_res$res)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.5495  0.9180  0.9862  0.9847  1.0591  1.3493 \n\n\n\n\nClick to expand/collapse code\nggplot() + \n  aes(geary_mc_2019_res$res[1:999]) + \n  geom_histogram(colour=\"black\", fill=\"grey\") +\n  geom_vline(xintercept = 1, color = \"red\", linetype = \"solid\", size = 1) +\n  labs(title = \"Histogram of Simulated  Geary C Values For Thailand Drug Use Cases 2019\",\n       x = \"Simulated  Geary C\",\n       y = \"Frequency\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nTo performs Geary’s C test for spatial autocorrelation using global_c_test() from sfdep package.\n\n\nClick to expand/collapse code\nglobal_c_test(wm_q_2020$no_cases,\n              wm_q_2020$nb,\n              wm_q_2020$wt,\n              randomization = TRUE,\n              allow_zero=TRUE)\n\n\n\n    Geary C test under randomisation\n\ndata:  x \nweights: listw  \nn reduced by no-neighbour observations \n\nGeary C statistic standard deviate = 0.47955, p-value = 0.3158\nalternative hypothesis: Expectation greater than statistic\nsample estimates:\nGeary C statistic       Expectation          Variance \n      0.952367243       1.000000000       0.009866203 \n\n\nTo performs permutation test for Geary’s C statistic by using global_c_perm() from sfdep packages.\n\n\nClick to expand/collapse code\nset.seed(1234)\ngeary_mc_2020_res = global_c_perm(wm_q_2020$no_cases,\n              wm_q_2020$nb,\n              wm_q_2020$wt,\n              nsim = 999,\n              allow_zero = TRUE,)\ngeary_mc_2020_res\n\n\n\n    Monte-Carlo simulation of Geary C\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.95237, observed rank = 324, p-value = 0.324\nalternative hypothesis: greater\n\n\nBased on the above actual and simulation statistic, with C = 0.95237 and the simulated p-value(0.324) similar to previous year (2020) suggests the drug use cases exhibits a slight tendency toward spatial clustered and the spatial distribution is likely random, and there is not enough significant evidence of spatial autocorrelation in 2020.\nTo visvualising the Monte Carlo Geary’s C.\n\n\nClick to expand/collapse code\nsummary(geary_mc_2020_res$res)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.5720  0.9290  0.9944  0.9896  1.0558  1.3507 \n\n\n\n\nClick to expand/collapse code\nggplot() + \n  aes(geary_mc_2020_res$res[1:999]) + \n  geom_histogram(colour=\"black\", fill=\"grey\") +\n  geom_vline(xintercept = 1, color = \"red\", linetype = \"solid\", size = 1) +\n  labs(title = \"Histogram of Simulated  Geary C Values For Thailand Drug Use Cases 2020\",\n       x = \"Simulated  Geary C\",\n       y = \"Frequency\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nTo performs Geary’s C test for spatial autocorrelation using global_c_test() from sfdep package.\n\n\nClick to expand/collapse code\nglobal_c_test(wm_q_2021$no_cases,\n              wm_q_2021$nb,\n              wm_q_2021$wt,\n              randomization = TRUE,\n              allow_zero=TRUE)\n\n\n\n    Geary C test under randomisation\n\ndata:  x \nweights: listw  \nn reduced by no-neighbour observations \n\nGeary C statistic standard deviate = 1.8151, p-value = 0.03476\nalternative hypothesis: Expectation greater than statistic\nsample estimates:\nGeary C statistic       Expectation          Variance \n      0.843832653       1.000000000       0.007402704 \n\n\nTo performs permutation test for Geary’s C statistic by using global_c_perm() from sfdep packages.\n\n\nClick to expand/collapse code\nset.seed(1234)\ngeary_mc_2021_res = global_c_perm(wm_q_2021$no_cases,\n              wm_q_2021$nb,\n              wm_q_2021$wt,\n              nsim = 999,\n              allow_zero = TRUE,)\ngeary_mc_2021_res\n\n\n\n    Monte-Carlo simulation of Geary C\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.84383, observed rank = 61, p-value = 0.061\nalternative hypothesis: greater\n\n\nBased on the above actual and simulation statistic, with C = 0.84383 and actual p-value of 0.03476, the drug use cases exhibits a slight tendency toward spatial clustered and we can reject the null hypothesis that the autocorrelation is unlikely to have occurred by random. However,the simulated p-value(0.061) which is larger than the alpha-value 0.05.\nThe difference between the actual p-value (0.034) and the simulated p-value (0.061) indicates that while the traditional test suggests significant spatial clustering, the Monte Carlo simulation shows weaker evidence for rejecting the null hypothesis of spatial randomness.\nTo visvualising the Monte Carlo Geary’s C.\n\n\nClick to expand/collapse code\nsummary(geary_mc_2021_res$res)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.6844  0.9350  0.9918  0.9886  1.0443  1.2153 \n\n\n\n\nClick to expand/collapse code\nggplot() + \n  aes(geary_mc_2021_res$res[1:999]) + \n  geom_histogram(colour=\"black\", fill=\"grey\") +\n  geom_vline(xintercept = 1, color = \"red\", linetype = \"solid\", size = 1) +\n  labs(title = \"Histogram of Simulated  Geary C Values For Thailand Drug Use Cases 2021\",\n       x = \"Simulated  Geary C\",\n       y = \"Frequency\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nTo performs Geary’s C test for spatial autocorrelation using global_c_test() from sfdep package.\n\n\nClick to expand/collapse code\nglobal_c_test(wm_q_2022$no_cases,\n              wm_q_2022$nb,\n              wm_q_2022$wt,\n              randomization = TRUE,\n              allow_zero=TRUE)\n\n\n\n    Geary C test under randomisation\n\ndata:  x \nweights: listw  \nn reduced by no-neighbour observations \n\nGeary C statistic standard deviate = 1.9389, p-value = 0.02626\nalternative hypothesis: Expectation greater than statistic\nsample estimates:\nGeary C statistic       Expectation          Variance \n      0.837233817       1.000000000       0.007047089 \n\n\nTo performs permutation test for Geary’s C statistic by using global_c_perm() from sfdep packages.\n\n\nClick to expand/collapse code\nset.seed(1234)\ngeary_mc_2022_res = global_c_perm(wm_q_2022$no_cases,\n              wm_q_2022$nb,\n              wm_q_2022$wt,\n              nsim = 999,\n              allow_zero = TRUE,)\ngeary_mc_2022_res\n\n\n\n    Monte-Carlo simulation of Geary C\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.83723, observed rank = 46, p-value = 0.046\nalternative hypothesis: greater\n\n\nBased on the above actual and simulation statistic, unlike previous year(2021), with C = 0.83723 suggesting the drug use cases exhibits a slight tendency toward spatial clustered. With actual p-value of 0.2626 and simulated p-value of 0.046, both is smaller than the alpha-value 0.05, hence, we can reject the null hypothesis that the autocorrelation is unlikely to have occurred by random and infer there is spatial autocorrelation of clustering in 2022.\nTo visvualising the Monte Carlo Geary’s C.\n\n\nClick to expand/collapse code\nsummary(geary_mc_2022_res$res)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.6673  0.9333  0.9851  0.9845  1.0400  1.2253 \n\n\n\n\nClick to expand/collapse code\nggplot() + \n  aes(geary_mc_2022_res$res[1:999]) + \n  geom_histogram(colour=\"black\", fill=\"grey\") +\n  geom_vline(xintercept = 1, color = \"red\", linetype = \"solid\", size = 1) +\n  labs(title = \"Histogram of Simulated  Geary C Values For Thailand Drug Use Cases 2022\",\n       x = \"Simulated  Geary C\",\n       y = \"Frequency\") +\n  theme_minimal()"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#global-gearys-test-overall-insight",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#global-gearys-test-overall-insight",
    "title": "Take Home Exercise 02",
    "section": "4.4 Global Geary’s Test Overall Insight",
    "text": "4.4 Global Geary’s Test Overall Insight\n\n\n\n\n\n\nOverall Observation\n\n\n\n\nOverall, the trend of what we have observed from Global Geary’s test is similar to Global Moran’s I Test. Except for Year 2021.\nYear 2017 to 2019, decreasing trend in C value and p-value.\nYear 2020, instead of continuing decreasing, both C value and p-value increased.\nHowever, for Year 2021, the C value and actual p-value suggest spatial clustered and we can reject the null hypothesis, but the simulated p-value become larger than alpha-value of 0.05, which shows weak evidence for us to reject the null hypothesis.\nYear 2022, There is sufficient statistical evidence for us to reject the null hypothesis and conclude that there is spatial autocorrelation indicating clustering."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#local-morans-i-test",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#local-morans-i-test",
    "title": "Take Home Exercise 02",
    "section": "5.1 Local Moran’s I Test",
    "text": "5.1 Local Moran’s I Test\nAs there are 3 p-value fields, for consistency, I will use p_ii_sim field, since I will simulate 1000 times and it will help in ensure robustness in my finding\n\n201720182019202020212022\n\n\n\n\nClick to expand/collapse code\nset.seed(1234)\nlisa_2017 &lt;- wm_q_2017 %&gt;% \n  mutate(local_moran = local_moran(\n    no_cases, nb, wt, nsim = 999,zero.policy = TRUE),\n         .before = 1) %&gt;%\n  unnest(local_moran)\n\n\nTo visualise local Moran’s I and p-value.\n\n\nClick to expand/collapse code\ntmap_mode(\"plot\")\nmap1 &lt;- tm_shape(lisa_2017) +\n  tm_fill(\"ii\",\n          style = \"pretty\",\n          palette = \"BrBG\",\n          title = \"local moran statistics\") + \n  tm_borders(alpha = 0.5) +\n  tm_view(set.zoom.limits = c(6,8)) +\n  tm_layout(main.title = \"local Moran's I of Drug Use Cases in 2017\",\n            main.title.size = 0.8)\n\nmap2 &lt;- tm_shape(lisa_2017) +\n  tm_fill(\"p_ii_sim\",\n          breaks = c(0, 0.001, 0.01, 0.05, 1),\n              labels = c(\"0.001\", \"0.01\", \"0.05\", \"Not sig\")) + \n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"p-value of local Moran's I in 2017\",\n            main.title.size = 0.8)\ntmap_arrange(map1, map2, ncol = 2)\n\n\n\n\n\n\n\n\n\nWe can observe that the area around Bangkok contains local outliers, as the number of cases in Bangkok is significantly higher than in the surrounding areas, which have relatively low case numbers. Additionally, there are three provinces on the northern side of Bangkok that exhibit local clustering.\n\n\n\n\nClick to expand/collapse code\nset.seed(1234)\nlisa_2018 &lt;- wm_q_2018 %&gt;% \n  mutate(local_moran = local_moran(\n    no_cases, nb, wt, nsim = 999,zero.policy = TRUE),\n         .before = 1) %&gt;%\n  unnest(local_moran)\n\n\nTo visualise local Moran’s I and p-value.\n\n\nClick to expand/collapse code\ntmap_mode(\"plot\")\nmap1 &lt;- tm_shape(lisa_2018) +\n  tm_fill(\"ii\",\n          style = \"pretty\",\n          palette = \"BrBG\",\n          title = \"local moran statistics\") + \n  tm_borders(alpha = 0.5) +\n  tm_view(set.zoom.limits = c(6,8)) +\n  tm_layout(main.title = \"local Moran's I of Drug Use Cases in 2018\",\n            main.title.size = 0.8)\n\nmap2 &lt;- tm_shape(lisa_2018) +\n  tm_fill(\"p_ii_sim\",\n          breaks = c(0, 0.001, 0.01, 0.05, 1),\n              labels = c(\"0.001\", \"0.01\", \"0.05\", \"Not sig\")) + \n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"p-value of local Moran's I in 2018\",\n            main.title.size = 0.8)\n\ntmap_arrange(map1, map2, ncol = 2)\n\n\n\n\n\n\n\n\n\nWe can see that the some provinces( Samut Prakan,Chachoengsao) near Bangkok have p_ii_sim values are below the alpha level of 0.05, with nearly being positive for the I value, except for Samut Sakhon(left side of Bangkok) and Nonthaburi(top side of Bangkok), which has a negative I value, indicating it being the outlier.\n\n\n\n\nClick to expand/collapse code\nset.seed(1234)\nlisa_2019 &lt;- wm_q_2019 %&gt;% \n  mutate(local_moran = local_moran(\n    no_cases, nb, wt, nsim = 999,zero.policy = TRUE),\n         .before = 1) %&gt;%\n  unnest(local_moran)\n\n\nTo visualise local Moran’s I and p-value.\n\n\nClick to expand/collapse code\ntmap_mode(\"plot\")\nmap1 &lt;- tm_shape(lisa_2019) +\n  tm_fill(\"ii\",\n          style = \"pretty\",\n          palette = \"BrBG\",\n          title = \"local moran statistics\") + \n  tm_borders(alpha = 0.5) +\n  tm_view(set.zoom.limits = c(6,8)) +\n  tm_layout(main.title = \"local Moran's I of Drug Use Cases in 2019\",\n            main.title.size = 0.8)\n\nmap2 &lt;- tm_shape(lisa_2019) +\n  tm_fill(\"p_ii_sim\",\n          breaks = c(0, 0.001, 0.01, 0.05, 1),\n              labels = c(\"0.001\", \"0.01\", \"0.05\", \"Not sig\")) + \n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"p-value of local Moran's I in 2019\",\n            main.title.size = 0.8)\n\ntmap_arrange(map1, map2, ncol = 2)\n\n\n\n\n\n\n\n\n\nWe can observed that the drug use cases in 2019 , is quite similar to previous year (2018), except that Samut Sakhon is no longer an outlier. And we have two province on top of the northern side.\n\n\n\n\nClick to expand/collapse code\nset.seed(1234)\nlisa_2020 &lt;- wm_q_2020 %&gt;% \n  mutate(local_moran = local_moran(\n    no_cases, nb, wt, nsim = 999,zero.policy = TRUE),\n         .before = 1) %&gt;%\n  unnest(local_moran)\n\n\nTo visualise local Moran’s I and p-value.\n\n\nClick to expand/collapse code\ntmap_mode(\"plot\")\nmap1 &lt;- tm_shape(lisa_2020) +\n  tm_fill(\"ii\",\n          style = \"pretty\",\n          palette = \"BrBG\",\n          title = \"local moran statistics\") + \n  tm_borders(alpha = 0.5) +\n  tm_view(set.zoom.limits = c(6,8)) +\n  tm_layout(main.title = \"local Moran's I of Drug Use Cases in 2020\",\n            main.title.size = 0.8)\n\nmap2 &lt;- tm_shape(lisa_2020) +\n  tm_fill(\"p_ii_sim\",\n          breaks = c(0, 0.001, 0.01, 0.05, 1),\n              labels = c(\"0.001\", \"0.01\", \"0.05\", \"Not sig\")) + \n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"p-value of local Moran's I in 2020\",\n            main.title.size = 0.8)\n\ntmap_arrange(map1, map2, ncol = 2)\n\n\n\n\n\n\n\n\n\nFrom the map, we can observe that there are not many significant changes compared to previous years, except for one province in the central become a weak cluster(I value = 0 to 1).\n\n\n\n\nClick to expand/collapse code\nset.seed(1234)\nlisa_2021 &lt;- wm_q_2021 %&gt;% \n  mutate(local_moran = local_moran(\n    no_cases, nb, wt, nsim = 999,zero.policy = TRUE),\n         .before = 1) %&gt;%\n  unnest(local_moran)\n\n\nTo visualise local Moran’s I and p-value.\n\n\nClick to expand/collapse code\ntmap_mode(\"plot\")\nmap1 &lt;- tm_shape(lisa_2021) +\n  tm_fill(\"ii\",\n          style = \"pretty\",\n          palette = \"BrBG\",\n          title = \"local moran statistics\") + \n  tm_borders(alpha = 0.5) +\n  tm_view(set.zoom.limits = c(6,8)) +\n  tm_layout(main.title = \"local Moran's I of Drug Use Cases in 2021\",\n            main.title.size = 0.8)\n\nmap2 &lt;- tm_shape(lisa_2021) +\n  tm_fill(\"p_ii_sim\",\n          breaks = c(0, 0.001, 0.01, 0.05, 1),\n              labels = c(\"0.001\", \"0.01\", \"0.05\", \"Not sig\")) + \n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"p-value of local Moran's I in 2021\",\n            main.title.size = 0.8)\n\ntmap_arrange(map1, map2, ncol = 2)\n\n\n\n\n\n\n\n\n\nBased on the above plots,we can see a cluster at the western region.\n\n\n\n\nClick to expand/collapse code\nset.seed(1234)\nlisa_2022 &lt;- wm_q_2022 %&gt;% \n  mutate(local_moran = local_moran(\n    no_cases, nb, wt, nsim = 999,zero.policy = TRUE),\n         .before = 1) %&gt;%\n  unnest(local_moran)\n\n\nTo visualise local Moran’s I and p-value.\n\n\nClick to expand/collapse code\ntmap_mode(\"plot\")\nmap1 &lt;- tm_shape(lisa_2022) +\n  tm_fill(\"ii\",\n          style = \"pretty\",\n          palette = \"BrBG\",\n          title = \"local moran statistics\") + \n  tm_borders(alpha = 0.5) +\n  tm_view(set.zoom.limits = c(6,8)) +\n  tm_layout(main.title = \"local Moran's I of Drug Use Cases in 2022\",\n            main.title.size = 0.8)\n\nmap2 &lt;- tm_shape(lisa_2022) +\n  tm_fill(\"p_ii_sim\",\n          breaks = c(0, 0.001, 0.01, 0.05, 1),\n              labels = c(\"0.001\", \"0.01\", \"0.05\", \"Not sig\")) + \n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"p-value of local Moran's I in 2022\",\n            main.title.size = 0.8)\n\ntmap_arrange(map1, map2, ncol = 2)\n\n\n\n\n\n\n\n\n\nThe plots for 2022 reveal distinct clustering areas in both the Northeast and Central regions, characterized by predominantly positive I values and p-values below the alpha threshold of 0.05. Additionally, there are two notable outliers: one located in the southern region and another in the northeastern region adjacent to the cluster.\n\n\n\n\n\n\n\n\n\nOverall Insight\n\n\n\n\nFrom 2017 to 2020, the p-values below 0.05 are primarily concentrated near the Bangkok area and central region, indicating that outliers (with negative I values) and clusters (with positive I values) are likely located in this region.\nIn 2021 and 2022, there are clear sign of clustering and in 2022, there are two outliers as well."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#section-12",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#section-12",
    "title": "Take Home Exercise 02",
    "section": "2017",
    "text": "2017"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#section-13",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#section-13",
    "title": "Take Home Exercise 02",
    "section": "2018",
    "text": "2018"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#section-14",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#section-14",
    "title": "Take Home Exercise 02",
    "section": "2019",
    "text": "2019"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#section-15",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#section-15",
    "title": "Take Home Exercise 02",
    "section": "2020",
    "text": "2020"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#section-16",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#section-16",
    "title": "Take Home Exercise 02",
    "section": "2021",
    "text": "2021"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#section-17",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#section-17",
    "title": "Take Home Exercise 02",
    "section": "2022",
    "text": "2022"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#lisa-quadrant-map",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#lisa-quadrant-map",
    "title": "Take Home Exercise 02",
    "section": "5.2 LISA Quadrant Map",
    "text": "5.2 LISA Quadrant Map\nWe can utilize the LISA Cluster Map to highlight statistically significant areas, making it easier to identify spatial autocorrelation patterns in the data.\nAfter examining the skewness of the LISA for each year, the distribution of skewness appears to be negatively skewed (left side has more values than the positive side). Therefore, I will use the median for the LISA map analysis and filter by p_ii value.\n\n\nClick to expand/collapse code\nlibrary(gridExtra)\n\nplot_2017 &lt;- ggplot(lisa_2017, aes(x = skewness)) + \n  geom_histogram(colour = \"black\", fill = \"grey\", bins = 30) +\n  labs(x = \"Skewness\", y = \"Frequency\", title = \"Skewness in 2017\") +\n  theme_minimal()\n\nplot_2018 &lt;- ggplot(lisa_2018, aes(x = skewness)) + \n  geom_histogram(colour = \"black\", fill = \"grey\", bins = 30) +\n  labs(x = \"Skewness\", y = \"Frequency\", title = \"Skewness in 2018\") +\n  theme_minimal()\n\nplot_2019 &lt;- ggplot(lisa_2019, aes(x = skewness)) + \n  geom_histogram(colour = \"black\", fill = \"grey\", bins = 30) +\n  labs(x = \"Skewness\", y = \"Frequency\", title = \"Skewness in 2019\") +\n  theme_minimal()\n\nplot_2020 &lt;- ggplot(lisa_2020, aes(x = skewness)) + \n  geom_histogram(colour = \"black\", fill = \"grey\", bins = 30) +\n  labs(x = \"Skewness\", y = \"Frequency\", title = \"Skewness in 2020\") +\n  theme_minimal()\n\nplot_2021 &lt;- ggplot(lisa_2021, aes(x = skewness)) + \n  geom_histogram(colour = \"black\", fill = \"grey\", bins = 30) +\n  labs(x = \"Skewness\", y = \"Frequency\", title = \"Skewness in 2021\") +\n  theme_minimal()\n\nplot_2022 &lt;- ggplot(lisa_2022, aes(x = skewness)) + \n  geom_histogram(colour = \"black\", fill = \"grey\", bins = 30) +\n  labs(x = \"Skewness\", y = \"Frequency\", title = \"Skewness in 2022\") +\n  theme_minimal()\n\n# Arrange all six plots in a 2-column, 3-row layout\ngrid.arrange(plot_2017, plot_2018, plot_2019, plot_2020, plot_2021, plot_2022, ncol = 2, nrow = 3)\n\n\n\n\n\n\n\n\n\nLocal Indicators of Spatial Association(LISA) interpretation:\n\nClusters\n\nHigh-High: High value area, surrounded by high value neighbours.\nLow-Low: Low value area, surrounded by low value neightbours.\n\nOutliers\n\nHigh-Low: High value area, surrounded by low value neighbours.\nLow-High: Low value area, surrounded by high value neighbours.\n\n\n\n201720182019202020212022\n\n\n\n\nClick to expand/collapse code\nlisa_sig_2017 &lt;- lisa_2017  %&gt;%\n  filter(p_ii &lt; 0.05)\nmap1 &lt;- tm_shape(lisa_2017) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\ntm_shape(lisa_sig_2017) +\n  tm_fill(\"median\") + \n  tm_borders(alpha = 0.4)\n\nmap2 &lt;- tm_shape(lisa_2017) +\n  tm_polygons(\"no_cases\",\n          palette = \"Blues\",\n          style=\"quantile\", n=10)\n\ntmap_arrange(map1, map2,asp=1,  ncol=2)\n\n\n\n\n\n\n\n\n\n\n\n\n\nClick to expand/collapse code\nlisa_sig_2018 &lt;- lisa_2018  %&gt;%\n  filter(p_ii &lt; 0.05)\n\nmap1 &lt;- tm_shape(lisa_2018) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\ntm_shape(lisa_sig_2018) +\n  tm_fill(\"median\") + \n  tm_borders(alpha = 0.4)\n\nmap2 &lt;- tm_shape(lisa_2018) +\n  tm_polygons(\"no_cases\",\n          palette = \"Blues\",\n          style=\"quantile\", n=10)\n\ntmap_arrange(map1, map2,asp=1,  ncol=2)\n\n\n\n\n\n\n\n\n\n\n\n\n\nClick to expand/collapse code\nlisa_sig_2019 &lt;- lisa_2019  %&gt;%\n  filter(p_ii &lt; 0.05)\n\nmap1 &lt;- tm_shape(lisa_2019) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\ntm_shape(lisa_sig_2019) +\n  tm_fill(\"median\") + \n  tm_borders(alpha = 0.4)\n\nmap2 &lt;- tm_shape(lisa_2019) +\n  tm_polygons(\"no_cases\",\n          palette = \"Blues\",\n          style=\"quantile\", n=10)\n\ntmap_arrange(map1, map2,asp=1,  ncol=2)\n\n\n\n\n\n\n\n\n\n\n\n\n\nClick to expand/collapse code\nlisa_sig_2020 &lt;- lisa_2020  %&gt;%\n  filter(p_ii &lt; 0.05)\n\nmap1 &lt;- tm_shape(lisa_2020) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\ntm_shape(lisa_sig_2020) +\n  tm_fill(\"median\") + \n  tm_borders(alpha = 0.4)\n\nmap2 &lt;- tm_shape(lisa_2020) +\n  tm_polygons(\"no_cases\",\n          palette = \"Blues\",\n          style=\"quantile\", n=10)\n\ntmap_arrange(map1, map2,asp=1,  ncol=2)\n\n\n\n\n\n\n\n\n\n\n\n\n\nClick to expand/collapse code\nlisa_sig_2021 &lt;- lisa_2021  %&gt;%\n  filter(p_ii &lt; 0.05)\n\nmap1 &lt;- tm_shape(lisa_2021) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\ntm_shape(lisa_sig_2021) +\n  tm_fill(\"median\") + \n  tm_borders(alpha = 0.4)\n\nmap2 &lt;- tm_shape(lisa_2021) +\n  tm_polygons(\"no_cases\",\n          palette = \"Blues\",\n          style=\"quantile\", n=10)\n\ntmap_arrange(map1, map2,asp=1,  ncol=2)\n\n\n\n\n\n\n\n\n\n\n\n\n\nClick to expand/collapse code\nlisa_sig_2022 &lt;- lisa_2022  %&gt;%\n  filter(p_ii &lt; 0.05)\n\nmap1 &lt;- tm_shape(lisa_2022) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\ntm_shape(lisa_sig_2022) +\n  tm_fill(\"median\") + \n  tm_borders(alpha = 0.4)\n\nmap2 &lt;- tm_shape(lisa_2022) +\n  tm_polygons(\"no_cases\",\n          palette = \"Blues\",\n          style=\"quantile\", n=10)\n\ntmap_arrange(map1, map2,asp=1,  ncol=2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nWith LISA map, it is so much easier for use to identify cluster"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#local-indicators-of-spatial-associationlisa",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#local-indicators-of-spatial-associationlisa",
    "title": "Take Home Exercise 02",
    "section": "5.2 Local Indicators of Spatial Association(LISA)",
    "text": "5.2 Local Indicators of Spatial Association(LISA)\nWe can utilize the LISA Cluster Map to highlight statistically significant areas, making it easier to identify spatial autocorrelation patterns in the data.\nAfter examining the skewness of the LISA for each year, the distribution of skewness appears to be negatively skewed (left side has more values than the positive side). Therefore, I will use the median for the LISA map analysis and filter by p_ii_sim value.\n\n\nClick to expand/collapse code\nlibrary(gridExtra)\n\nplot_2017 &lt;- ggplot(lisa_2017, aes(x = skewness)) + \n  geom_histogram(colour = \"black\", fill = \"grey\", bins = 30) +\n  labs(x = \"Skewness\", y = \"Frequency\", title = \"Skewness in 2017\") +\n  theme_minimal()\n\nplot_2018 &lt;- ggplot(lisa_2018, aes(x = skewness)) + \n  geom_histogram(colour = \"black\", fill = \"grey\", bins = 30) +\n  labs(x = \"Skewness\", y = \"Frequency\", title = \"Skewness in 2018\") +\n  theme_minimal()\n\nplot_2019 &lt;- ggplot(lisa_2019, aes(x = skewness)) + \n  geom_histogram(colour = \"black\", fill = \"grey\", bins = 30) +\n  labs(x = \"Skewness\", y = \"Frequency\", title = \"Skewness in 2019\") +\n  theme_minimal()\n\nplot_2020 &lt;- ggplot(lisa_2020, aes(x = skewness)) + \n  geom_histogram(colour = \"black\", fill = \"grey\", bins = 30) +\n  labs(x = \"Skewness\", y = \"Frequency\", title = \"Skewness in 2020\") +\n  theme_minimal()\n\nplot_2021 &lt;- ggplot(lisa_2021, aes(x = skewness)) + \n  geom_histogram(colour = \"black\", fill = \"grey\", bins = 30) +\n  labs(x = \"Skewness\", y = \"Frequency\", title = \"Skewness in 2021\") +\n  theme_minimal()\n\nplot_2022 &lt;- ggplot(lisa_2022, aes(x = skewness)) + \n  geom_histogram(colour = \"black\", fill = \"grey\", bins = 30) +\n  labs(x = \"Skewness\", y = \"Frequency\", title = \"Skewness in 2022\") +\n  theme_minimal()\n\n# Arrange all six plots in a 2-column, 3-row layout\ngrid.arrange(plot_2017, plot_2018, plot_2019, plot_2020, plot_2021, plot_2022, ncol = 2, nrow = 3)\n\n\n\n\n\n\n\n\n\nLocal Indicators of Spatial Association(LISA) interpretation:\n\nClusters\n\nHigh-High: High value area, surrounded by high value neighbours.\nLow-Low: Low value area, surrounded by low value neightbours.\n\nOutliers\n\nHigh-Low: High value area, surrounded by low value neighbours.\nLow-High: Low value area, surrounded by high value neighbours.\n\n\nBelow is the code chunk that visualizes the LISA map alongside the drug use case map. I found this side-by-side comparison particularly useful for providing additional context, helping to better understand why specific locations are classified as clusters or outliers.\n\n201720182019202020212022\n\n\n\n\nClick to expand/collapse code\nlisa_sig_2017 &lt;- lisa_2017  %&gt;%\n  filter(p_ii_sim &lt; 0.05)\nmap1 &lt;- tm_shape(lisa_2017) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\ntm_shape(lisa_sig_2017) +\n  tm_fill(\"median\") + \n  tm_borders(alpha = 0.4)\n\nmap2 &lt;- tm_shape(lisa_2017) +\n  tm_polygons(\"no_cases\",\n          palette = \"Blues\",\n          style=\"quantile\", n=10)\n\ntmap_arrange(map1, map2,ncol=2)\n\n\n\n\n\n\n\n\n\n\n\n\n\nClick to expand/collapse code\nlisa_sig_2018 &lt;- lisa_2018  %&gt;%\n  filter(p_ii_sim &lt; 0.05)\n\nmap1 &lt;- tm_shape(lisa_2018) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\ntm_shape(lisa_sig_2018) +\n  tm_fill(\"median\") + \n  tm_borders(alpha = 0.4)\n\nmap2 &lt;- tm_shape(lisa_2018) +\n  tm_polygons(\"no_cases\",\n          palette = \"Blues\",\n          style=\"quantile\", n=10)\n\ntmap_arrange(map1, map2,ncol=2)\n\n\n\n\n\n\n\n\n\n\n\n\n\nClick to expand/collapse code\nlisa_sig_2019 &lt;- lisa_2019  %&gt;%\n  filter(p_ii_sim &lt; 0.05)\n\nmap1 &lt;- tm_shape(lisa_2019) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\ntm_shape(lisa_sig_2019) +\n  tm_fill(\"median\") + \n  tm_borders(alpha = 0.4)\n\nmap2 &lt;- tm_shape(lisa_2019) +\n  tm_polygons(\"no_cases\",\n          palette = \"Blues\",\n          style=\"quantile\", n=10)\n\ntmap_arrange(map1, map2,ncol=2)\n\n\n\n\n\n\n\n\n\n\n\n\n\nClick to expand/collapse code\nlisa_sig_2020 &lt;- lisa_2020  %&gt;%\n  filter(p_ii_sim &lt; 0.05)\n\nmap1 &lt;- tm_shape(lisa_2020) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\ntm_shape(lisa_sig_2020) +\n  tm_fill(\"median\") + \n  tm_borders(alpha = 0.4)\n\nmap2 &lt;- tm_shape(lisa_2020) +\n  tm_polygons(\"no_cases\",\n          palette = \"Blues\",\n          style=\"quantile\", n=10)\n\ntmap_arrange(map1, map2,ncol=2)\n\n\n\n\n\n\n\n\n\n\n\n\n\nClick to expand/collapse code\nlisa_sig_2021 &lt;- lisa_2021  %&gt;%\n  filter(p_ii_sim &lt; 0.05)\n\nmap1 &lt;- tm_shape(lisa_2021) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\ntm_shape(lisa_sig_2021) +\n  tm_fill(\"median\") + \n  tm_borders(alpha = 0.4)\n\nmap2 &lt;- tm_shape(lisa_2021) +\n  tm_polygons(\"no_cases\",\n          palette = \"Blues\",\n          style=\"quantile\", n=10)\n\ntmap_arrange(map1, map2,ncol=2)\n\n\n\n\n\n\n\n\n\n\n\n\n\nClick to expand/collapse code\nlisa_sig_2022 &lt;- lisa_2022  %&gt;%\n  filter(p_ii_sim &lt; 0.05)\n\nmap1 &lt;- tm_shape(lisa_2022) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\ntm_shape(lisa_sig_2022) +\n  tm_fill(\"median\") + \n  tm_borders(alpha = 0.4)\n\nmap2 &lt;- tm_shape(lisa_2022) +\n  tm_polygons(\"no_cases\",\n          palette = \"Blues\",\n          style=\"quantile\", n=10)\n\ntmap_arrange(map1, map2, ncol=2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLISA Insight\n\n\n\n\nThe LISA map provides a much clearer way to identify clusters, particularly distinguishing between high-value and low-value clusters. This approach simplifies the process compared to the Local Moran’s I Test maps we plotted earlier, which required multiple maps and side-by-side comparisons to interpret the spatial patterns. With LISA, clusters are more visually intuitive, making it easier to draw insights from spatial autocorrelation.\nNote that since I am using the median instead of the mean, some provinces in the LISA map appear slightly different from the Local Moran’s I test p-value map. I think the difference arises because the median is more robust to skewed data, which may influence the spatial patterns displayed."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#hot-spot-and-code-spot-area-analysishcsa-with-local-gi",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#hot-spot-and-code-spot-area-analysishcsa-with-local-gi",
    "title": "Take Home Exercise 02",
    "section": "5.3 Hot Spot and Code Spot Area Analysis(HCSA) with local Gi*",
    "text": "5.3 Hot Spot and Code Spot Area Analysis(HCSA) with local Gi*\nBeside LISA, we can use local Gi* statistic to detect hot spots (areas of high values surrounded by high values) and cold spots (areas of low values surrounded by low values). In this analysis, spatial clustering patterns are measured, and locations are classified as hot spots, cold spots, or neutral areas. I will be use local Gi*, which will consider itself.\n\n\n\n\n\n\nWarning\n\n\n\nThis analysis is not to identify outliers and need to use distance matrix with all positive value.\n\n\nIn the previous sections, I used queen contiguity to define neighbors. Since there are multiple methods for calculating neighbors, I will explore different approaches using the 2017 data to identify the most suitable one for our analysis using HCSA and to gain diverse insights. For the weights, I will continue utilizing st_inverse_distance.\n\nQueen Contiguity =&gt; st_contiguity\nNeighbors from a distance =&gt; st_dist_band\nK-Nearest Neighbors =&gt; st_knn\n\nFirst, we need to derive a spatial weight matrix before computing the local Gi* statistics. As the local Gi* statistic requires a distance matrix, we use the distance matrix for this purpose.\nSince the geometry field in drug_use_cases contains multipolygons instead of polygons, I will apply st_centroid to extract the central points for each multipolygon. This ensures we can calculate the distances between these central points accurately for the analysis.\n\n2017 - Queen Contiguity2017 - Neighbors from a distance2017 - K-Nearest Neighbors\n\n\nBelow code chunk will compute spatial weight matrix.\n\n\nClick to expand/collapse code\nwm_idw_2017 &lt;- drug_use_cases %&gt;%\n  filter(fiscal_year == 2017) %&gt;%\n  mutate(nb = include_self(st_contiguity(geometry)),\n         geometry_centroid = st_centroid(geometry), \n         wts = st_inverse_distance(nb, \n                              geometry_centroid, \n                              scale = 1,\n                              alpha = 1),\n         .before = 1)\n\n\nTo compute the local G* and using set.seed() to ensure the computation is reproducible.\n\n\nClick to expand/collapse code\nset.seed(1234)\nHCSA_2017 &lt;- wm_idw_2017 %&gt;% \n  mutate(local_Gi = local_gstar_perm(\n    no_cases, nb, wts, nsim = 999),\n         .before = 1) %&gt;%\n  unnest(local_Gi)\n\n\nVisualising Gi* and p_sim value using tmap at province level.\n\n\nClick to expand/collapse code\ntmap_mode(\"plot\")\nmap1 &lt;- tm_shape(HCSA_2017) +\n  tm_fill(\"gi_star\") + \n  tm_borders(alpha = 0.5) +\n  tm_view(set.zoom.limits = c(6,8)) +\n  tm_layout(main.title = \"Gi* of Drug Use Cases in 2017\",\n            main.title.size = 0.8)\n\nmap2 &lt;- tm_shape(HCSA_2017) +\n  tm_fill(\"p_sim\",\n          breaks = c(0, 0.001, 0.01, 0.05, 1),\n              labels = c(\"0.001\", \"0.01\", \"0.05\", \"Not sig\")) + \n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"p-value of Gi*\",\n            main.title.size = 0.8)\n\ntmap_arrange(map1, map2, ncol = 2)\n\n\n\n\n\n\n\n\n\nVisualising the significant(p_sim &lt; 0.05) HCSA map.\n\n\nClick to expand/collapse code\nHCSA_sig_2017 &lt;- HCSA_2017  %&gt;%\n  filter(p_sim &lt; 0.05)\n\nmap1 &lt;- tm_shape(HCSA_2017) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\ntm_shape(HCSA_sig_2017) +\n  tm_fill(\"cluster\") + \n  tm_borders(alpha = 0.4)\n\nmap2 &lt;- tm_shape(HCSA_2017) +\n  tm_polygons(\"no_cases\",\n          palette = \"Blues\",\n          style=\"quantile\", n=10)\n\ntmap_arrange(map1, map2,ncol=2)\n\n\n\n\n\n\n\n\n\n\n\nBelow code chunk will compute spatial weight matrix.\n\n\nClick to expand/collapse code\nwm_idw_2017_db &lt;- drug_use_cases %&gt;%\n  filter(fiscal_year == 2017) %&gt;%\n  mutate(nb = include_self(st_dist_band(geometry, lower=0)),\n         geometry_centroid = st_centroid(geometry), \n         wts = st_inverse_distance(nb, \n                              geometry_centroid, \n                              scale = 1,\n                              alpha = 1),\n         .before = 1)\n\n\nTo compute the local G* and using set.seed() to ensure the computation is reproducible.\n\n\nClick to expand/collapse code\nset.seed(1234)\nHCSA_2017_db &lt;- wm_idw_2017_db %&gt;% \n  mutate(local_Gi = local_gstar_perm(\n    no_cases, nb, wts, nsim = 999),\n         .before = 1) %&gt;%\n  unnest(local_Gi)\n\n\nVisualising Gi* and p_sim value using tmap at province level.\n\n\nClick to expand/collapse code\ntmap_mode(\"plot\")\nmap1 &lt;- tm_shape(HCSA_2017_db) +\n  tm_fill(\"gi_star\") + \n  tm_borders(alpha = 0.5) +\n  tm_view(set.zoom.limits = c(6,8)) +\n  tm_layout(main.title = \"Gi* of Drug Use Cases in 2017\",\n            main.title.size = 0.8)\n\nmap2 &lt;- tm_shape(HCSA_2017_db) +\n  tm_fill(\"p_sim\",\n          breaks = c(0, 0.001, 0.01, 0.05, 1),\n              labels = c(\"0.001\", \"0.01\", \"0.05\", \"Not sig\")) + \n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"p-value of Gi*\",\n            main.title.size = 0.8)\n\ntmap_arrange(map1, map2, ncol = 2)\n\n\n\n\n\n\n\n\n\nVisualising the significant(p_sim &lt; 0.05) HCSA map.\n\n\nClick to expand/collapse code\nHCSA_sig_2017_db &lt;- HCSA_2017_db  %&gt;%\n  filter(p_sim &lt; 0.05)\n\nmap1 &lt;- tm_shape(HCSA_2017_db) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\ntm_shape(HCSA_sig_2017_db) +\n  tm_fill(\"cluster\") + \n  tm_borders(alpha = 0.4)\n\nmap2 &lt;- tm_shape(HCSA_2017_db) +\n  tm_polygons(\"no_cases\",\n          palette = \"Blues\",\n          style=\"quantile\", n=10)\n\ntmap_arrange(map1, map2,ncol=2)\n\n\n\n\n\n\n\n\n\n\n\nBelow code chunk will compute spatial weight matrix.\n\n\nClick to expand/collapse code\nwm_idw_2017_k &lt;- drug_use_cases %&gt;%\n  filter(fiscal_year == 2017) %&gt;%\n  mutate(nb = include_self(st_knn(geometry, k = 8)),\n         geometry_centroid = st_centroid(geometry), \n         wts = st_inverse_distance(nb, \n                              geometry_centroid,\n                              scale = 1,\n                              alpha = 1),\n         .before = 1)\n\n\nTo compute the local G* and using set.seed() to ensure the computation is reproducible.\n\n\nClick to expand/collapse code\nset.seed(1234)\nHCSA_2017_k &lt;- wm_idw_2017_k %&gt;% \n  mutate(local_Gi = local_gstar_perm(\n    no_cases, nb, wts, nsim = 999),\n         .before = 1) %&gt;%\n  unnest(local_Gi)\n\n\nVisualising Gi* and p_sim value using tmap at province level.\n\n\nClick to expand/collapse code\ntmap_mode(\"plot\")\nmap1 &lt;- tm_shape(HCSA_2017_k) +\n  tm_fill(\"gi_star\") + \n  tm_borders(alpha = 0.5) +\n  tm_view(set.zoom.limits = c(6,8)) +\n  tm_layout(main.title = \"Gi* of Drug Use Cases in 2017\",\n            main.title.size = 0.8)\n\nmap2 &lt;- tm_shape(HCSA_2017_k) +\n  tm_fill(\"p_sim\",\n          breaks = c(0, 0.001, 0.01, 0.05, 1),\n              labels = c(\"0.001\", \"0.01\", \"0.05\", \"Not sig\")) + \n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"p-value of Gi*\",\n            main.title.size = 0.8)\n\ntmap_arrange(map1, map2, ncol = 2)\n\n\n\n\n\n\n\n\n\nVisualising the significant(p_sim &lt; 0.05) HCSA map.\n\n\nClick to expand/collapse code\nHCSA_sig_2017_k &lt;- HCSA_2017_k  %&gt;%\n  filter(p_sim &lt; 0.05)\n\nmap1 &lt;- tm_shape(HCSA_2017_k) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\ntm_shape(HCSA_sig_2017_k) +\n  tm_fill(\"cluster\") + \n  tm_borders(alpha = 0.4)\n\nmap2 &lt;- tm_shape(HCSA_2017_k) +\n  tm_polygons(\"no_cases\",\n          palette = \"Blues\",\n          style=\"quantile\", n=10)\n\ntmap_arrange(map1, map2,ncol=2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\nNotice that for the HCSA map for Queen Contiguity, is actually every similar with what we have in LISA map, because of the same method on how we define neighbors.\nFor distance-based neighbors, I am using the default upper threshold. From the plot, we can observe that some areas lose local context. For example, the three provinces at the right side of Bangkok all have relatively high no_cases, yet one of them is not identified as part of the high cluster.\nFor K-Nearest Neighbors (KNN), the plot accurately represents the central region. However, a potential issue with KNN is that it disregards local context, as it doesn’t account for whether neighboring provinces are adjacent. For instance, in the southern region, the provinces are arranged in a line, meaning that using KNN may select neighbors that are quite distant from the province itself. This can result in weaker or misleading spatial autocorrelation patterns.\nHence, for the years 2018 to 2022, I will continue using Queen Contiguity to identify neighbors, as it considers all adjacent neighbors, providing a more precise representation of local spatial relationships.\n\n\n\n\n20182019202020212022\n\n\nBelow code chunk will compute spatial weight matrix.\n\n\nClick to expand/collapse code\nwm_idw_2018 &lt;- drug_use_cases %&gt;%\n  filter(fiscal_year == 2018) %&gt;%\n  mutate(nb = include_self(st_contiguity(geometry)),\n         geometry_centroid = st_centroid(geometry), \n         wts = st_inverse_distance(nb, \n                              geometry_centroid, \n                              scale = 1,\n                              alpha = 1),\n         .before = 1)\n\n\nTo compute the local G* and using set.seed() to ensure the computation is reproducible.\n\n\nClick to expand/collapse code\nset.seed(1234)\nHCSA_2018 &lt;- wm_idw_2018 %&gt;% \n  mutate(local_Gi = local_gstar_perm(\n    no_cases, nb, wts, nsim = 999),\n         .before = 1) %&gt;%\n  unnest(local_Gi)\n\n\nVisualising Gi* and p_sim value using tmap at province level.\n\n\nClick to expand/collapse code\ntmap_mode(\"plot\")\nmap1 &lt;- tm_shape(HCSA_2018) +\n  tm_fill(\"gi_star\") + \n  tm_borders(alpha = 0.5) +\n  tm_view(set.zoom.limits = c(6,8)) +\n  tm_layout(main.title = \"Gi* of Drug Use Cases in 2018\",\n            main.title.size = 0.8)\n\nmap2 &lt;- tm_shape(HCSA_2018) +\n  tm_fill(\"p_sim\",\n          breaks = c(0, 0.001, 0.01, 0.05, 1),\n              labels = c(\"0.001\", \"0.01\", \"0.05\", \"Not sig\")) + \n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"p-value of Gi*\",\n            main.title.size = 0.8)\n\ntmap_arrange(map1, map2, ncol = 2)\n\n\n\n\n\n\n\n\n\nVisualising the significant(p_sim &lt; 0.05) HCSA map.\n\n\nClick to expand/collapse code\nHCSA_sig_2018 &lt;- HCSA_2018  %&gt;%\n  filter(p_sim &lt; 0.05)\n\nmap1 &lt;- tm_shape(HCSA_2018) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\ntm_shape(HCSA_sig_2018) +\n  tm_fill(\"cluster\") + \n  tm_borders(alpha = 0.4)\n\nmap2 &lt;- tm_shape(HCSA_2018) +\n  tm_polygons(\"no_cases\",\n          palette = \"Blues\",\n          style=\"quantile\", n=10)\n\ntmap_arrange(map1, map2,ncol=2)\n\n\n\n\n\n\n\n\n\n\n\nBelow code chunk will compute spatial weight matrix.\n\n\nClick to expand/collapse code\nwm_idw_2019 &lt;- drug_use_cases %&gt;%\n  filter(fiscal_year == 2019) %&gt;%\n  mutate(nb = include_self(st_contiguity(geometry)),\n         geometry_centroid = st_centroid(geometry), \n         wts = st_inverse_distance(nb, \n                              geometry_centroid, \n                              scale = 1,\n                              alpha = 1),\n         .before = 1)\n\n\nTo compute the local G* and using set.seed() to ensure the computation is reproducible.\n\n\nClick to expand/collapse code\nset.seed(1234)\nHCSA_2019 &lt;- wm_idw_2019 %&gt;% \n  mutate(local_Gi = local_gstar_perm(\n    no_cases, nb, wts, nsim = 999),\n         .before = 1) %&gt;%\n  unnest(local_Gi)\n\n\nVisualising Gi* and p_sim value using tmap at province level.\n\n\nClick to expand/collapse code\ntmap_mode(\"plot\")\nmap1 &lt;- tm_shape(HCSA_2019) +\n  tm_fill(\"gi_star\") + \n  tm_borders(alpha = 0.5) +\n  tm_view(set.zoom.limits = c(6,8)) +\n  tm_layout(main.title = \"Gi* of Drug Use Cases in 2019\",\n            main.title.size = 0.8)\n\nmap2 &lt;- tm_shape(HCSA_2019) +\n  tm_fill(\"p_sim\",\n          breaks = c(0, 0.001, 0.01, 0.05, 1),\n              labels = c(\"0.001\", \"0.01\", \"0.05\", \"Not sig\")) + \n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"p-value of Gi*\",\n            main.title.size = 0.8)\n\ntmap_arrange(map1, map2, ncol = 2)\n\n\n\n\n\n\n\n\n\nVisualising the significant(p_sim &lt; 0.05) HCSA map.\n\n\nClick to expand/collapse code\nHCSA_sig_2019 &lt;- HCSA_2019  %&gt;%\n  filter(p_sim &lt; 0.05)\n\nmap1 &lt;- tm_shape(HCSA_2019) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\ntm_shape(HCSA_sig_2019) +\n  tm_fill(\"cluster\") + \n  tm_borders(alpha = 0.4)\n\nmap2 &lt;- tm_shape(HCSA_2019) +\n  tm_polygons(\"no_cases\",\n          palette = \"Blues\",\n          style=\"quantile\", n=10)\n\ntmap_arrange(map1, map2,ncol=2)\n\n\n\n\n\n\n\n\n\n\n\nBelow code chunk will compute spatial weight matrix.\n\n\nClick to expand/collapse code\nwm_idw_2020 &lt;- drug_use_cases %&gt;%\n  filter(fiscal_year == 2020) %&gt;%\n  mutate(nb = include_self(st_contiguity(geometry)),\n         geometry_centroid = st_centroid(geometry), \n         wts = st_inverse_distance(nb, \n                              geometry_centroid, \n                              scale = 1,\n                              alpha = 1),\n         .before = 1)\n\n\nTo compute the local G* and using set.seed() to ensure the computation is reproducible.\n\n\nClick to expand/collapse code\nset.seed(1234)\nHCSA_2020 &lt;- wm_idw_2020 %&gt;% \n  mutate(local_Gi = local_gstar_perm(\n    no_cases, nb, wts, nsim = 999),\n         .before = 1) %&gt;%\n  unnest(local_Gi)\n\n\nVisualising Gi* and p_sim value using tmap at province level.\n\n\nClick to expand/collapse code\ntmap_mode(\"plot\")\nmap1 &lt;- tm_shape(HCSA_2020) +\n  tm_fill(\"gi_star\") + \n  tm_borders(alpha = 0.5) +\n  tm_view(set.zoom.limits = c(6,8)) +\n  tm_layout(main.title = \"Gi* of Drug Use Cases in 2020\",\n            main.title.size = 0.8)\n\nmap2 &lt;- tm_shape(HCSA_2020) +\n  tm_fill(\"p_sim\",\n          breaks = c(0, 0.001, 0.01, 0.05, 1),\n              labels = c(\"0.001\", \"0.01\", \"0.05\", \"Not sig\")) + \n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"p-value of Gi*\",\n            main.title.size = 0.8)\n\ntmap_arrange(map1, map2, ncol = 2)\n\n\n\n\n\n\n\n\n\nVisualising the significant(p_sim &lt; 0.05) HCSA map.\n\n\nClick to expand/collapse code\nHCSA_sig_2020 &lt;- HCSA_2020  %&gt;%\n  filter(p_sim &lt; 0.05)\n\nmap1 &lt;- tm_shape(HCSA_2020) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\ntm_shape(HCSA_sig_2020) +\n  tm_fill(\"cluster\") + \n  tm_borders(alpha = 0.4)\n\nmap2 &lt;- tm_shape(HCSA_2020) +\n  tm_polygons(\"no_cases\",\n          palette = \"Blues\",\n          style=\"quantile\", n=10)\n\ntmap_arrange(map1, map2,ncol=2)\n\n\n\n\n\n\n\n\n\n\n\nBelow code chunk will compute spatial weight matrix.\n\n\nClick to expand/collapse code\nwm_idw_2021 &lt;- drug_use_cases %&gt;%\n  filter(fiscal_year == 2021) %&gt;%\n  mutate(nb = include_self(st_contiguity(geometry)),\n         geometry_centroid = st_centroid(geometry), \n         wts = st_inverse_distance(nb, \n                              geometry_centroid, \n                              scale = 1,\n                              alpha = 1),\n         .before = 1)\n\n\nTo compute the local G* and using set.seed() to ensure the computation is reproducible.\n\n\nClick to expand/collapse code\nset.seed(1234)\nHCSA_2021 &lt;- wm_idw_2021 %&gt;% \n  mutate(local_Gi = local_gstar_perm(\n    no_cases, nb, wts, nsim = 999),\n         .before = 1) %&gt;%\n  unnest(local_Gi)\n\n\nVisualising Gi* and p_sim value using tmap at province level.\n\n\nClick to expand/collapse code\ntmap_mode(\"plot\")\nmap1 &lt;- tm_shape(HCSA_2021) +\n  tm_fill(\"gi_star\") + \n  tm_borders(alpha = 0.5) +\n  tm_view(set.zoom.limits = c(6,8)) +\n  tm_layout(main.title = \"Gi* of Drug Use Cases in 2021\",\n            main.title.size = 0.8)\n\nmap2 &lt;- tm_shape(HCSA_2021) +\n  tm_fill(\"p_sim\",\n          breaks = c(0, 0.001, 0.01, 0.05, 1),\n              labels = c(\"0.001\", \"0.01\", \"0.05\", \"Not sig\")) + \n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"p-value of Gi*\",\n            main.title.size = 0.8)\n\ntmap_arrange(map1, map2, ncol = 2)\n\n\n\n\n\n\n\n\n\nVisualising the significant(p_sim &lt; 0.05) HCSA map.\n\n\nClick to expand/collapse code\nHCSA_sig_2021 &lt;- HCSA_2021  %&gt;%\n  filter(p_sim &lt; 0.05)\n\nmap1 &lt;- tm_shape(HCSA_2021) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\ntm_shape(HCSA_sig_2021) +\n  tm_fill(\"cluster\") + \n  tm_borders(alpha = 0.4)\n\nmap2 &lt;- tm_shape(HCSA_2021) +\n  tm_polygons(\"no_cases\",\n          palette = \"Blues\",\n          style=\"quantile\", n=10)\n\ntmap_arrange(map1, map2,ncol=2)\n\n\n\n\n\n\n\n\n\n\n\nBelow code chunk will compute spatial weight matrix.\n\n\nClick to expand/collapse code\nwm_idw_2022 &lt;- drug_use_cases %&gt;%\n  filter(fiscal_year == 2022) %&gt;%\n  mutate(nb = include_self(st_contiguity(geometry)),\n         geometry_centroid = st_centroid(geometry), \n         wts = st_inverse_distance(nb, \n                              geometry_centroid, \n                              scale = 1,\n                              alpha = 1),\n         .before = 1)\n\n\nTo compute the local G* and using set.seed() to ensure the computation is reproducible.\n\n\nClick to expand/collapse code\nset.seed(1234)\nHCSA_2022 &lt;- wm_idw_2022 %&gt;% \n  mutate(local_Gi = local_gstar_perm(\n    no_cases, nb, wts, nsim = 999),\n         .before = 1) %&gt;%\n  unnest(local_Gi)\n\n\nVisualising Gi* and p_sim value using tmap at province level.\n\n\nClick to expand/collapse code\ntmap_mode(\"plot\")\nmap1 &lt;- tm_shape(HCSA_2022) +\n  tm_fill(\"gi_star\") + \n  tm_borders(alpha = 0.5) +\n  tm_view(set.zoom.limits = c(6,8)) +\n  tm_layout(main.title = \"Gi* of Drug Use Cases in 2022\",\n            main.title.size = 0.8)\n\nmap2 &lt;- tm_shape(HCSA_2022) +\n  tm_fill(\"p_sim\",\n          breaks = c(0, 0.001, 0.01, 0.05, 1),\n              labels = c(\"0.001\", \"0.01\", \"0.05\", \"Not sig\")) + \n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"p-value of Gi*\",\n            main.title.size = 0.8)\n\ntmap_arrange(map1, map2, ncol = 2)\n\n\n\n\n\n\n\n\n\nVisualising the significant(p_sim &lt; 0.05) HCSA map.\n\n\nClick to expand/collapse code\nHCSA_sig_2022 &lt;- HCSA_2022  %&gt;%\n  filter(p_sim &lt; 0.05)\n\nmap1 &lt;- tm_shape(HCSA_2022) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\ntm_shape(HCSA_sig_2022) +\n  tm_fill(\"cluster\") + \n  tm_borders(alpha = 0.4)\n\nmap2 &lt;- tm_shape(HCSA_2022) +\n  tm_polygons(\"no_cases\",\n          palette = \"Blues\",\n          style=\"quantile\", n=10)\n\ntmap_arrange(map1, map2,ncol=2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHCSA Insight\n\n\n\n\nIn 2017, there were two cold spot areas—one located above Bangkok and another surrounding the city. Notably, the cold spot area above Bangkok coincided with the Low-Low cluster identified in the LISA map.\nBy 2018, the cold spot above Bangkok had disappeared, while the area around Bangkok persisted. However, one province transitioned from a cold spot to a hot spot during this period.\nIn 2019, only one province remained as a cold spot around Bangkok, down from three provinces in 2018, while a new cold spot emerged in a province to the north. Additionally, the number of hot spots surrounding Bangkok increased.\nIn 2020, a new cold spot area appeared slightly above Bangkok, leaving only one province as a hot spot in the region.\nBy 2021, the hot spots had vanished, and a large cold spot area emerged to the left of Bangkok.\nIn 2022, the cold spot areas were reduced to two provinces, while a new hot spot emerged in the northern region."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#emerging-hot-spot-analysis-ehsa",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#emerging-hot-spot-analysis-ehsa",
    "title": "Take Home Exercise 02",
    "section": "5.4 Emerging Hot Spot Analysis (EHSA)",
    "text": "5.4 Emerging Hot Spot Analysis (EHSA)\nIn this section, I will explore how to perform Emerging Hot Spot Analysis (EHSA) on the current dataset.\nEHSA is an exploratory spatial data analysis (ESDA) technique that integrates traditional hot spot analysis, using the Getis-Ord Gi* statistic, with the time-series Mann-Kendall test for detecting monotonic trends.\nBy applying EHSA, I can assess how hot and cold spots evolve over time. I will begin by conducting the Mann-Kendall test to identify trends from 2017 to 2022, followed by the EHSA to analyze these spatial-temporal patterns.\nFirst, I need to create a space-time cube to move forward with the analysis. The code snippet below will accomplish this task.\n\nspt &lt;- as_spacetime(drug_use_cases, \"province_en\", \"fiscal_year\")\nis_spacetime_cube(spt)\n\n[1] TRUE\n\n\n\n5.4.1 Mann-Kendall Test\nThe Mann-Kendall test for trend is a statistical method used to evaluate whether a dataset exhibits a significant upward or downward trend over time. This non-parametric test is versatile, as it does not assume the data follows a normal distribution, making it applicable to a wide range of data types. However, it is important that the data is free of serial correlation(AKA autocorrelation) for the test to be valid. If the data does follow a normal distribution, simple linear regression can be used as an alternative to assess trends.\nThe Hypothesis:\n\nNull hypothesis: There is no monotonic trend in the series.\nAlternate hypothesis: A trend exists. This trend can be positive, negative, or non-null.\n\nBased on the HCSA section plot, I observed that from 2017 to 2020, the primary hot and cold spots were concentrated around Bangkok. However, in 2021 and 2022, a major hotspot emerged in the central-west region. As a result, I will conduct the Mann-Kendall test to examine the trends in the five provinces surrounding Bangkok: Chachoengsao (1), Pathum Thani (2), Nonthaburi (3), Samut Sakhon (4), and Samut Prakan (5).\n\n\n\n\n\nTo compute spatial weight matrix of all years.\n\nwm_idw &lt;- spt %&gt;%\n  activate(\"geometry\") %&gt;%\n  mutate(nb = include_self(st_contiguity(geometry)),\n         geometry_centroid = st_centroid(geometry), \n         wt = st_inverse_distance(nb, \n                              geometry_centroid, \n                              scale = 1,\n                              alpha = 1),\n         .before = 1) %&gt;%\n  set_nbs(\"nb\") %&gt;%\n  set_wts(\"wt\")\n\n\nhead(wm_idw)\n\n                province_en fiscal_year types_of_drug_offenses no_cases\n1                   Bangkok        2017         drug_use_cases    11871\n7              Samut Prakan        2017         drug_use_cases      820\n13               Nonthaburi        2017         drug_use_cases      553\n19             Pathum Thani        2017         drug_use_cases      450\n25 Phra Nakhon Si Ayutthaya        2017         drug_use_cases      378\n31                Ang Thong        2017         drug_use_cases      208\n                          nb\n1     1, 2, 3, 4, 15, 59, 60\n7                   1, 2, 15\n13            1, 3, 4, 5, 59\n19    1, 3, 4, 5, 10, 15, 17\n25 3, 4, 5, 6, 7, 10, 58, 59\n31            5, 6, 7, 8, 58\n                                                                                               wt\n1              0.00000000, 0.04623058, 0.03320981, 0.03009409, 0.01128737, 0.01703632, 0.02007676\n7                                                              0.04623058, 0.00000000, 0.01293165\n13                                     0.03320981, 0.00000000, 0.02860959, 0.02040690, 0.03198346\n19             0.03009409, 0.02860959, 0.00000000, 0.02825561, 0.01387401, 0.01051713, 0.01806330\n25 0.02040690, 0.02825561, 0.00000000, 0.02747784, 0.01062597, 0.01629943, 0.01341095, 0.01530443\n31                                     0.02747784, 0.00000000, 0.01240505, 0.03112161, 0.02033245\n\n\n\nclass(wm_idw$nb)\n\n[1] \"list\"\n\n\nNotices that the class of the nb field has changed to a list instead of a neighbors list due to the set_nbs function in the spacetime. The code snippet below will convert this field back into a neighbors list.\n\nclass(wm_idw$nb) &lt;- c(\"nb\", \"list\")\nclass(wm_idw$nb)\n\n[1] \"nb\"   \"list\"\n\n\nTo compute the local G* and using set.seed() to ensure it is reproducible.\n\nset.seed(1234)\nlocal_g_star_years &lt;- wm_idw %&gt;%\n  group_by(fiscal_year) %&gt;%\n  mutate(local_Gi = local_gstar_perm(\n    no_cases, nb, wt,nsim = 999),\n    .before = 1) %&gt;%\n  unnest(local_Gi)\n\nFilter the 5 province data.\n\ncha_gs &lt;- local_g_star_years%&gt;%\n  filter(province_en == \"Chachoengsao\") |&gt;\n  select(province_en, fiscal_year, gi_star)\n\npt_gs &lt;- local_g_star_years%&gt;%\n  filter(province_en == \"Pathum Thani\") |&gt;\n  select(province_en, fiscal_year, gi_star)\n\nnon_gs &lt;- local_g_star_years%&gt;%\n  filter(province_en == \"Nonthaburi\") |&gt;\n  select(province_en, fiscal_year, gi_star)\n\nss_gs &lt;- local_g_star_years%&gt;%\n  filter(province_en == \"Samut Sakhon\") |&gt;\n  select(province_en, fiscal_year, gi_star)\n\nsp_gs &lt;- local_g_star_years%&gt;%\n  filter(province_en == \"Samut Prakan\") |&gt;\n  select(province_en, fiscal_year, gi_star)\n\nLets visualize the trend of the G* value of the 5 province surrounding Bangkok\n\nlibrary(plotly)\np_mkt &lt;- ggplot() +\n  geom_line(data = cha_gs, mapping = aes(x = fiscal_year, y = gi_star, color = \"Chachoengsao\")) +\n  geom_line(data = pt_gs, mapping = aes(x = fiscal_year, y = gi_star, color = \"Pathum Thani\")) + \n  geom_line(data = non_gs, mapping = aes(x = fiscal_year, y = gi_star, color = \"Nonthaburi\")) + \n  geom_line(data = ss_gs, mapping = aes(x = fiscal_year, y = gi_star, color = \"Samut Sakhon\")) + \n  geom_line(data = sp_gs, mapping = aes(x = fiscal_year, y = gi_star, color = \"Samut Prakan\")) + \n  labs(x = \"Year\", y = \"Gi* Value\", \n       title = \"Gi* value of the five province surrounding Bangkok\",\n       color = \"Province\")\n\nplotly::ggplotly(p_mkt)\n\n\n\n\n\n\n\n\n\n\n\nInsight\n\n\n\n\nWe can observed that the Gi* value of the five province are having a negative trend(from hot spot to cold spot) suggests that there is a tendency for the drug use cases around Bangkok decrease over time.\nThe plot match with what we have in the HCSA for the"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#mann-kendall-test",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#mann-kendall-test",
    "title": "Take Home Exercise 02",
    "section": "6.1 Mann-Kendall Test",
    "text": "6.1 Mann-Kendall Test\nThe Mann-Kendall test for trend is a statistical method used to evaluate whether a dataset exhibits a significant upward or downward trend over time. This non-parametric test is versatile, as it does not assume the data follows a normal distribution, making it applicable to a wide range of data types. However, it is important that the data is free of serial correlation(AKA autocorrelation) for the test to be valid. If the data does follow a normal distribution, simple linear regression can be used as an alternative to assess trends.\nThe Hypothesis:\n\nNull hypothesis: There is no monotonic trend in the series.\nAlternate hypothesis: A trend exists. This trend can be positive, negative, or non-null.\n\nBased on the HCSA section plot, I observed that from 2017 to 2020, the primary hot and cold spots were concentrated around Bangkok. However, in 2021 and 2022, a major hotspot emerged in the central-west region. As a result, I will conduct the Mann-Kendall test to examine the trends in the five provinces surrounding Bangkok: Chachoengsao (1), Pathum Thani (2), Nonthaburi (3), Samut Sakhon (4), and Samut Prakan (5).\n\n\n\n\n\nTo compute spatial weight matrix of all years.\n\nwm_idw &lt;- spt %&gt;%\n  activate(\"geometry\") %&gt;%\n  mutate(nb = include_self(st_contiguity(geometry)),\n         geometry_centroid = st_centroid(geometry), \n         wt = st_inverse_distance(nb, \n                              geometry_centroid, \n                              scale = 1,\n                              alpha = 1),\n         .before = 1) %&gt;%\n  set_nbs(\"nb\") %&gt;%\n  set_wts(\"wt\")\n\n\nhead(wm_idw)\n\n                province_en fiscal_year types_of_drug_offenses no_cases\n1                   Bangkok        2017         drug_use_cases    11871\n7              Samut Prakan        2017         drug_use_cases      820\n13               Nonthaburi        2017         drug_use_cases      553\n19             Pathum Thani        2017         drug_use_cases      450\n25 Phra Nakhon Si Ayutthaya        2017         drug_use_cases      378\n31                Ang Thong        2017         drug_use_cases      208\n                          nb\n1     1, 2, 3, 4, 15, 59, 60\n7                   1, 2, 15\n13            1, 3, 4, 5, 59\n19    1, 3, 4, 5, 10, 15, 17\n25 3, 4, 5, 6, 7, 10, 58, 59\n31            5, 6, 7, 8, 58\n                                                                                               wt\n1              0.00000000, 0.04623058, 0.03320981, 0.03009409, 0.01128737, 0.01703632, 0.02007676\n7                                                              0.04623058, 0.00000000, 0.01293165\n13                                     0.03320981, 0.00000000, 0.02860959, 0.02040690, 0.03198346\n19             0.03009409, 0.02860959, 0.00000000, 0.02825561, 0.01387401, 0.01051713, 0.01806330\n25 0.02040690, 0.02825561, 0.00000000, 0.02747784, 0.01062597, 0.01629943, 0.01341095, 0.01530443\n31                                     0.02747784, 0.00000000, 0.01240505, 0.03112161, 0.02033245\n\n\n\nclass(wm_idw$nb)\n\n[1] \"list\"\n\n\nNotices that the class of the nb field has changed to a list instead of a neighbors list due to the set_nbs function in the spacetime. The code snippet below will convert this field back into a neighbors list.\n\nclass(wm_idw$nb) &lt;- c(\"nb\", \"list\")\nclass(wm_idw$nb)\n\n[1] \"nb\"   \"list\"\n\n\nTo compute the local G* and using set.seed() to ensure it is reproducible.\n\nset.seed(1234)\nlocal_g_star_years &lt;- wm_idw %&gt;%\n  group_by(fiscal_year) %&gt;%\n  mutate(local_Gi = local_gstar_perm(\n    no_cases, nb, wt,nsim = 999),\n    .before = 1) %&gt;%\n  unnest(local_Gi)\n\nFilter the 5 province data.\n\ncha_gs &lt;- local_g_star_years%&gt;%\n  filter(province_en == \"Chachoengsao\") |&gt;\n  select(province_en, fiscal_year, gi_star)\n\npt_gs &lt;- local_g_star_years%&gt;%\n  filter(province_en == \"Pathum Thani\") |&gt;\n  select(province_en, fiscal_year, gi_star)\n\nnon_gs &lt;- local_g_star_years%&gt;%\n  filter(province_en == \"Nonthaburi\") |&gt;\n  select(province_en, fiscal_year, gi_star)\n\nss_gs &lt;- local_g_star_years%&gt;%\n  filter(province_en == \"Samut Sakhon\") |&gt;\n  select(province_en, fiscal_year, gi_star)\n\nsp_gs &lt;- local_g_star_years%&gt;%\n  filter(province_en == \"Samut Prakan\") |&gt;\n  select(province_en, fiscal_year, gi_star)\n\nLets visualize the trend of the G* value of the 5 province surrounding Bangkok\n\np_mkt &lt;- ggplot() +\n  geom_line(data = cha_gs, mapping = aes(x = fiscal_year, y = gi_star, color = \"Chachoengsao\")) +\n  geom_line(data = pt_gs, mapping = aes(x = fiscal_year, y = gi_star, color = \"Pathum Thani\")) + \n  geom_line(data = non_gs, mapping = aes(x = fiscal_year, y = gi_star, color = \"Nonthaburi\")) + \n  geom_line(data = ss_gs, mapping = aes(x = fiscal_year, y = gi_star, color = \"Samut Sakhon\")) + \n  geom_line(data = sp_gs, mapping = aes(x = fiscal_year, y = gi_star, color = \"Samut Prakan\")) + \n  labs(x = \"Year\", y = \"Gi* Value\", \n       title = \"Gi* value of the five province surrounding Bangkok\",\n       color = \"Province\")\n\nplotly::ggplotly(p_mkt)\n\n\n\n\n\n\n\n\n\n\n\nInsight\n\n\n\n\nWe can observed that the Gi* value of the five province are having a negative trend(from hot spot to cold spot) suggests that there is a tendency for the drug use cases around Bangkok decrease over time."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#plotting-ehsa",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#plotting-ehsa",
    "title": "Take Home Exercise 02",
    "section": "6.2 Plotting EHSA",
    "text": "6.2 Plotting EHSA\nWe can perform EHSA analysis by using emerging_hotspot_analysis() of sfdep package. It will takes a spacetime object named spt which I created at the start of this section. .var is the numeric vector in the spactime cube. Arguments k is to define the number of time lags to include in the neighborhood for calculating the local Gi*. nsim is the number of simulations to run in calculating the simulated p-value for the local Gi*.\n\nset.seed(1234)\nEHSA &lt;- emerging_hotspot_analysis(\n  x = spt, \n  .var = \"no_cases\", \n  k = 1, \n  nsim = 999\n)\n\n\n6.2.1 Visualizing the EHSA\nBefore, we plot the EHSA map, lets plot a bar graph to see the distribution of the EHSA using ggplot2.\n\nggplot(data = EHSA,\n       aes(y = classification,fill = classification)) +\n  geom_bar(show.legend = FALSE)\n\n\n\n\n\n\n\n\nBefore, we can plot the EHSA map, we need to perform left join with the Thailand sf object, as there is no geometry field in the EHSA .\n\nhead(EHSA)\n\n                  location        tau   p_value      classification\n1                  Bangkok  0.3333333 0.4523704 no pattern detected\n2             Samut Prakan -0.3333333 0.4523704   sporadic coldspot\n3               Nonthaburi  0.2000000 0.7071142    sporadic hotspot\n4             Pathum Thani  0.3333333 0.4523704 no pattern detected\n5 Phra Nakhon Si Ayutthaya  0.3333333 0.4523704 no pattern detected\n6                Ang Thong  0.3333333 0.4523704    sporadic hotspot\n\n\nThe following code chunk uses the left_join() function from the dplyr package. Since the two variables do not have a common field name, we specify the corresponding fields manually for the join.\n\nthailand_ehsa &lt;- left_join(thailand_sf, EHSA,  by = join_by(province_en == location))\n\n\nhead(thailand_ehsa)\n\nSimple feature collection with 6 features and 4 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 100.1913 ymin: 13.47842 xmax: 100.9639 ymax: 14.80246\nGeodetic CRS:  WGS 84\n               province_en        tau   p_value      classification\n1                  Bangkok  0.3333333 0.4523704 no pattern detected\n2             Samut Prakan -0.3333333 0.4523704   sporadic coldspot\n3               Nonthaburi  0.2000000 0.7071142    sporadic hotspot\n4             Pathum Thani  0.3333333 0.4523704 no pattern detected\n5 Phra Nakhon Si Ayutthaya  0.3333333 0.4523704 no pattern detected\n6                Ang Thong  0.3333333 0.4523704    sporadic hotspot\n                        geometry\n1 MULTIPOLYGON (((100.6139 13...\n2 MULTIPOLYGON (((100.7306 13...\n3 MULTIPOLYGON (((100.3415 14...\n4 MULTIPOLYGON (((100.8916 14...\n5 MULTIPOLYGON (((100.5131 14...\n6 MULTIPOLYGON (((100.3332 14...\n\n\nBefore plotting the map, we should typically only display provinces that are statistically significant (p-value &lt; 0.05). However, since it appears that all the provinces have p-values greater than 0.05, I will proceed with plotting the entire EHSA data with filtering of p-value &lt; 0.1 instead.\n\nthailand_ehsa_sig_0.05 &lt;- thailand_ehsa  %&gt;%\n  filter(p_value &lt; 0.05)\ncount(thailand_ehsa_sig_0.05)\n\nSimple feature collection with 1 feature and 1 field (with 1 geometry empty)\nGeometry type: GEOMETRYCOLLECTION\nDimension:     XY\nBounding box:  xmin: NA ymin: NA xmax: NA ymax: NA\nGeodetic CRS:  WGS 84\n  n                 geometry\n1 0 GEOMETRYCOLLECTION EMPTY\n\n\n\nthailand_ehsa_sig_0.1 &lt;- thailand_ehsa  %&gt;%\n  filter(p_value &lt; 0.1)\ncount(thailand_ehsa_sig_0.1)\n\nSimple feature collection with 1 feature and 1 field\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 98.67082 ymin: 12.50787 xmax: 101.7197 ymax: 18.70837\nGeodetic CRS:  WGS 84\n  n                       geometry\n1 5 MULTIPOLYGON (((100.4275 15...\n\n\nNow lets, plot the EHSA map using tmap functions.\n\ntmap_mode(\"plot\")  \ntm_shape(thailand_ehsa)+\n  tm_polygons()+\n  tm_borders(col = \"black\", alpha = 0.6)+\ntm_shape(thailand_ehsa_sig_0.1)+\n  tm_fill(\"classification\", \n          palette = \"Set1\",\n          title = \"classification\",\n          midpoint = 0) +\n  tm_borders(col = \"black\", alpha = 0.6)+\n  tm_layout(main.title = \"EHSA of Thailand Drug Use Cases\",\n            main.title.position = \"center\",\n            main.title.size = 1.7,\n            main.title.fontface = \"bold\",\n            legend.title.size = 1.8,\n            legend.text.size = 1.3,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 1.5, size = 3, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=1.2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEHSA Insights\n\n\n\n\nNotice that, even when I raise the p-value threshold to 0.1, only five provinces are identified.\nI believe the potential reason for this may be that the distribution of drug use cases does not exhibit strong spatial clustering. This means there are no distinct areas where cases are significantly higher or lower than expected, which could be why the EHSA results show high p-values."
  }
]